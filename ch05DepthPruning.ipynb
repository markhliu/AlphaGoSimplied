{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 5: Depth Pruning in Minimax\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "*“Genius sometimes consists of knowing when to stop.”*\n",
    "\n",
    "-- Charles De Gaulle\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "What you'll learn in this chapter:\n",
    "\n",
    "* Creating a MiniMax agent in Tic Tac Toe\n",
    "* Understanding the idea behind depth pruning\n",
    "* Creating a generic MiniMax agent with depth pruning\n",
    "* Testing MiniMax with depth pruning in Tic Tac Toe and Connect Four\n",
    "\n",
    "\n",
    "Your learned how MiniMax tree search works in the previous chapter and applied it to the coin game by searching for the best move in the next\n",
    "step recursively until the game ends. The MiniMax agent solves the coin\n",
    "game: it wins 100% of the time when it plays second.\n",
    "\n",
    "In this chapter, you'll first create a MiniMax agent in Tic Tac Toe by using\n",
    "recursion, as you did in Chapter 4. MiniMax tree search exhausts all possible future game\n",
    "paths and solves the Tic Tac Toe game. However, at the beginning\n",
    "of the game, it takes more than 34 seconds for the MiniMax agent to\n",
    "make a move. Later moves take much less time, though.\n",
    "\n",
    "In more complicated games such as Connect Four, Chess, or Go, the\n",
    "MiniMax algorithm cannot exhaust all possible future game paths in a\n",
    "short amount of time. However, this doesn't mean that MiniMax tree\n",
    "search is useless in these games. One of the answers lies in depth pruning: Instead of searching all the way to the terminal state of the game, you\n",
    "search a certain number of moves ahead and stop searching. You can then evaluate the game outcome by using a position evaluation function. In this chapter, we assume that the game is tied after searching a fixed number of steps ahead and the game is still not terminal. We'll discuss how to apply position evaluation functions in Chapter 7. \n",
    "\n",
    "In this chapter, you'll learn to create a generic MiniMax agent with depth pruning that can be applied to both Tic Tac Toe and Connect Four. Depth pruning allows the MiniMax agent to come up with intelligent (though not perfect) game strategies in a short amount of time. In fact, the algorithm used by Deep Blue to beat World Chess Champion Gary Kasparov in 1997 was\n",
    "based on MiniMax tree search with depth pruning (along with other\n",
    "strategies).\n",
    "\n",
    "After that, you'll test the effectiveness of your MiniMax agents against the rule-based AI that you developed in Chapters 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. MiniMax Tree Search in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. The MiniMax Algorithm in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "We'll use a simplified version of the self-made Tic Tac Toe game environment from Chapter 2 to speed up MiniMax tree search. Specifically, the module is saved as *ttt_simple_env.py* in the folder *utils* in the book's GitHub repository https://github.com/markhliu/AlphaGoSimplified. Download the file and save it in the folder /Desktop/ags/utils/ on your computer. The file *ttt_simple_env.py* is the same as *ttt_env.py* that we used in Chapter 2, except that we have deleted the graphical game window functionality. As a result, you cannot use the *render()* method in the simplified Tic Tac Toe game environment. We use the simplified coin game environment to make the MiniMax agent make moves faster: to search ahead, the algorithm makes hypothetical moves by creating a deep copy of the current game environment. Without the *render()* method, the algorithm makes deep copies (hence decisions) faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddc025",
   "metadata": {},
   "source": [
    "We'll define a *MiniMax_X()* function for Player X a different function *MiniMax_O()* function for Player O. Potentially we can define one function for both players, but it's easier to explain the functions when we have one for each player. There is a tradeoff between code efficiency and code readability and here we choose the latter. \n",
    "\n",
    "In the local module *ch05util*, we first define a *MiniMax_X()* function for Player X who is about to make a move. Download the file *ch05util.py* from the book's GitHub repository and save it in /Desktop/ai/utils/ on your computer. The file acts as a local module with a few functions in it. The *MiniMax_X()* function is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8807d",
   "metadata": {},
   "source": [
    "```python\n",
    "def MiniMax_X(env):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If wins right away with move m, take it.\n",
    "        if done and reward==1:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=maximized_payoff(env_copy,reward,done)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return env.sample()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d2b3d9",
   "metadata": {},
   "source": [
    "```python\n",
    "def maximized_payoff(env,reward,done):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # Otherwise, search for action to maximize payoff\n",
    "    best_payoff=-2\n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=maximized_payoff(env_copy,reward,done)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        # update your best payoff \n",
    "        if my_payoff>best_payoff:        \n",
    "            best_payoff=my_payoff\n",
    "    return best_payoff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "## 1.3. Test the MiniMax Algorithm in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=5\n",
      "It took the agent 34.98157286643982 seconds\n",
      "Current state is \n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Player O, what's your move?\n",
      "1\n",
      "Player O has chosen action=1\n",
      "Current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  0]]\n",
      "Player X has chosen action=6\n",
      "It took the agent 0.4136021137237549 seconds\n",
      "Current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  1  1]\n",
      " [-1  0  0]]\n",
      "Player O, what's your move?\n",
      "4\n",
      "Player O has chosen action=4\n",
      "Current state is \n",
      "[[ 0  0  0]\n",
      " [-1  1  1]\n",
      " [-1  0  0]]\n",
      "Player X has chosen action=7\n",
      "It took the agent 0.014605283737182617 seconds\n",
      "Current state is \n",
      "[[ 1  0  0]\n",
      " [-1  1  1]\n",
      " [-1  0  0]]\n",
      "Player O, what's your move?\n",
      "3\n",
      "Player O has chosen action=3\n",
      "Current state is \n",
      "[[ 1  0  0]\n",
      " [-1  1  1]\n",
      " [-1  0 -1]]\n",
      "Player X has chosen action=2\n",
      "It took the agent 0.0018301010131835938 seconds\n",
      "Current state is \n",
      "[[ 1  0  0]\n",
      " [-1  1  1]\n",
      " [-1  1 -1]]\n",
      "Player O, what's your move?\n",
      "8\n",
      "Player O has chosen action=8\n",
      "Current state is \n",
      "[[ 1 -1  0]\n",
      " [-1  1  1]\n",
      " [-1  1 -1]]\n",
      "Player X has chosen action=9\n",
      "It took the agent 0.0 seconds\n",
      "Current state is \n",
      "[[ 1 -1  1]\n",
      " [-1  1  1]\n",
      " [-1  1 -1]]\n",
      "Game over, it's a tie!\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "from utils.ch05util import MiniMax_X, maximized_payoff \n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action = MiniMax_X(env)\n",
    "    end=time.time()\n",
    "    print(f\"Player X has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(\"Player X has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        print(\"Player O has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The game is tied. It took the MiniMax algorithm about 35 seconds to make the very first move. It took the agent a fraction of a second to make later moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece520",
   "metadata": {},
   "source": [
    "## 1.4. Efficacy of the MiniMax Algorithm in Tic Tac Toe\n",
    "Next, we’ll test how often the MiniMax Algorithm wins against the think-three-steps-ahead game strategy that we developed in Chapter 2. \n",
    "\n",
    "To do that, we first define a *MiniMax_O()* function in the local *ch05util* module, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e5732",
   "metadata": {},
   "source": [
    "```python\n",
    "def MiniMax_O(env):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward==-1:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=maximized_payoff(env_copy,reward,done)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return env.sample()      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The function is very similar to the *MiniMax_X()* function we defined before. The only difference between is that we changed\n",
    "\n",
    "        if done and reward==1:\n",
    "            return m\n",
    "\n",
    "to \n",
    "\n",
    "        if done and reward==-1:\n",
    "            return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8367f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import MiniMax_O\n",
    "from utils.ch02util import ttt_think3, one_ttt_game \n",
    "\n",
    "# Play 20 games\n",
    "results=[]\n",
    "for i in range(20):\n",
    "    # MiniMax moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=one_ttt_game(MiniMax_X,ttt_think3)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # MiniMax moves second if i is an odd number\n",
    "    else:\n",
    "        result=one_ttt_game(ttt_think3,MiniMax_O)\n",
    "        # record negative game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a7084ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MiniMax agent has won 9 games\n",
      "the MiniMax agent has lost 0 games\n",
      "the game was tied 11 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MiniMax agent has won\n",
    "wins=results.count(1)\n",
    "print(f\"the MiniMax agent has won {wins} games\")\n",
    "# count how many times the MiniMax agent has lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MiniMax agent has lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game was tied {ties} times\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85a350",
   "metadata": {},
   "source": [
    "The results show that the MiniMax agent has won 9 games, while the rest 11 games are tied. The MiniMax agent has never lost to the think-three-steps ahead AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe6000",
   "metadata": {},
   "source": [
    "# 2. Depth Pruning in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c0ce",
   "metadata": {},
   "source": [
    "## 2.1. The max_payoff() Function\n",
    "We'll define a *max_payoff()* function in the local module *ch05util*. The function is similar to the *maximized_payoff()* function we defined in the last section. However, there are two important differences. First, there is a depth argument in the function to control how many steps the MiniMax agent searches. Second, we'll make the function general so that it can be applied to Tic Tac Toe as well as the Connect Four game later in this chapter. \n",
    "\n",
    "Go to the file *ch05util.py* you just downloaded and take a look at the *max_payoff()* function, which is defined as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce145749",
   "metadata": {},
   "source": [
    "```python\n",
    "def max_payoff(env,reward,done,depth):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # If the maximum depth is reached, assume tie game\n",
    "    if depth==0:\n",
    "        return 0        \n",
    "    # Otherwise, search for action to maximize payoff\n",
    "    best_payoff=-2\n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=max_payoff(env_copy,reward,done,depth-1)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        # update your best payoff \n",
    "        if my_payoff>best_payoff:        \n",
    "            best_payoff=my_payoff\n",
    "    return best_payoff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cead4",
   "metadata": {},
   "source": [
    "## 2.2. The MiniMax_depth() Function\n",
    "We also define a *MiniMax_depth()* function to produce the best move for the MiniMax agent. There is a *depth* argument in the function to control how many steps the MiniMax agent searches. The default *depth* value is set to 3. We'll make the function general so that it can be applied to Tic Tac Toe as well as the Connect Four game later in this chapter. \n",
    "\n",
    "The *MiniMax_depth()* function is defined as follows. It's saved in the file *ch05util.py* that you just downloaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123399b",
   "metadata": {},
   "source": [
    "```python\n",
    "def MiniMax_depth(env,depth=3):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=max_payoff(env_copy,reward,done,depth)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return env.sample()     \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d4e3",
   "metadata": {},
   "source": [
    "## 2.3. Speed of the Depth-Pruned MiniMax Agent\n",
    "Next, we test how fast the depth-pruned MiniMax agent is. We use the default depth of 3, and play a game with the agent. We let the agent play first again and measure how long it takes for the agent to make a move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20703e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=5\n",
      "It took the agent 0.19032764434814453 seconds\n",
      "Current state is \n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Player O, what's your move?\n",
      "1\n",
      "Player O has chosen action=1\n",
      "Current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  0]]\n",
      "Player X has chosen action=7\n",
      "It took the agent 0.0625150203704834 seconds\n",
      "Current state is \n",
      "[[ 1  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  0]]\n",
      "Player O, what's your move?\n",
      "2\n",
      "Player O has chosen action=2\n",
      "Current state is \n",
      "[[ 1  0  0]\n",
      " [ 0  1  0]\n",
      " [-1 -1  0]]\n",
      "Player X has chosen action=3\n",
      "It took the agent 0.0010073184967041016 seconds\n",
      "Current state is \n",
      "[[ 1  0  0]\n",
      " [ 0  1  0]\n",
      " [-1 -1  1]]\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch05util import MiniMax_depth\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action = MiniMax_depth(env,depth=3)\n",
    "    end=time.time()\n",
    "    print(f\"Player X has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(\"Player X has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        print(\"Player O has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df6cd7",
   "metadata": {},
   "source": [
    "# 3. Depth Pruning in Connect Four\n",
    "Next, we’ll create a MiniMax agent for the connect four game. The agent searches for a maximum of three steps. \n",
    "\n",
    "We first manually play a game against the MiniMax agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36b6b6",
   "metadata": {},
   "source": [
    "## 3.1. The MiniMax Agent in Connect Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb92577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The red player has chosen action=5\n",
      "It took the agent 0.14301228523254395 seconds\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]]\n",
      "Player yellow, what's your move?\n",
      "1\n",
      "Player yellow has chosen action=1\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  1  0  0]]\n",
      "The red player has chosen action=6\n",
      "It took the agent 0.14829635620117188 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  1  1  0]]\n",
      "Player yellow, what's your move?\n",
      "1\n",
      "Player yellow has chosen action=1\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0  0  0  1  1  0]]\n",
      "The red player has chosen action=4\n",
      "It took the agent 0.13970661163330078 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0  0  1  1  1  0]]\n",
      "Player yellow, what's your move?\n",
      "3\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0 -1  1  1  1  0]]\n",
      "The red player has chosen action=7\n",
      "It took the agent 0.1260826587677002 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0 -1  1  1  1  1]]\n",
      "The red player has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action=MiniMax(env,depth=3)\n",
    "    end=time.time()\n",
    "    print(f\"The red player has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"The red player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action=input(\"Player yellow, what's your move?\\n\")\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==-1:\n",
    "            print(f\"The yellow player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fa1bb",
   "metadata": {},
   "source": [
    "The MiniMax agent is able to plan three steps ahead and create a double attack and win the game. It takes less than 0.15 seconds for the agent to come up with a move in each step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "## 3.2. MiniMax verus Rule-Based AI in Connect Four\n",
    "We'll test if the MiniMax algorithm that searches for three steps ahead can beat the rule-based think-three-steps-ahead AI that we developed in Chapter 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be50ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch03util import one_conn_game, conn_think3\n",
    " \n",
    "results=[]\n",
    "for i in range(100):\n",
    "    # MiniMax moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=one_conn_game(MiniMax_depth,conn_think3)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # MiniMax moves second if i is an odd number\n",
    "    else:\n",
    "        result=one_conn_game(conn_think3,MiniMax_depth)\n",
    "        # record negative game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eecbe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MiniMax agent won 68 games\n",
      "the MiniMax agent lost 26 games\n",
      "the game was tied 6 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times MiniMax won\n",
    "wins=results.count(1)\n",
    "print(f\"the MiniMax agent won {wins} games\")\n",
    "# count how many times MiniMax lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MiniMax agent lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game was tied {ties} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
