{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 15: Introduction to the Actor-Critic Method\n",
    "\n",
    "In the last three chapters, we have learned two different methods of reinforcement learning: value-based approaches such as tabular Q learning and deep Q learning, and a policy-based approach (namely the policy gradients approach). In this chapter, we'll introduce a method that combines the two methods: the actor-critic method. \n",
    "\n",
    "In the actor-critic method, we simultaneously learn a policy function and a value function. Specifically, we create a neural network with one input network and two parallel output networks. The two output networks share the same input network. One of the output network, the actor, tells the agent how to make decisions: it returns the probability distribution of each possible next move. The other output network, the critic, estimate the expected game outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 15}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 15 in a subfolder /files/ch15. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch15\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. The Idea behind the Actor-Critic Approach\n",
    "This section introduces the logic hehind the actor-critic approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "## 1.1. Pros and Cons of Policy-Based and Value-Based Approaches\n",
    "We have discussed in Chapter 14 the idea behind policy gradients. In a nutshell, to maximize cumulative rewards, the agent adjusts the model parameters proportional to the product of the graidietns and rewards. The product of the gradients and reward is as follows: \n",
    "$$\n",
    "E[\\sum\\limits_{t=0}^{T-1} \\nabla _{\\theta }log\\pi _{\\theta }(a_{t}|s_{t})r_t(s_{0},a_{0},\\ldots ,a_{T-1},s_T)|\\pi _{\\theta }]\n",
    "$$\n",
    "The discounted reward agent receives at time t, $r_t(s_{0},a_{0},\\ldots ,a_{T-1},s_T)$, is calculated ex post, after observing the final payoff R and the trajectory of states and actions $(s_{0},a_{0},\\ldots ,a_{T-1},s_T)$. That is, the agent adjusts the parameters proportional to the product of the gradients and the rewards. The advantages of policy based approach is that it's very direct and works fast in many situations. The concern is that the gradients have high variance and the training process may become unstable. In some scenarios, the parameters doen't converge. \n",
    "\n",
    "The value-based approach, on the otehr hand, tries to find the value functions for each state-action pair. The agent then chooses teh nbest action in each state by picking the action with the highest value. The approach is stable but training is more time-consuming. \n",
    "\n",
    "The actor-critic approach combines the value-based approach and hte policy based appraoch and provides a more powerful reinforcement learning method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa92c7",
   "metadata": {},
   "source": [
    "## 1.2. The Concept of Advantage\n",
    "The actor-critic reinforcement learning models have two networks: a value-based network that we call the critic, and a policy-based network that we call the actor. The value network estimates the expected game outcome (that is, the value of the action produced by the actor), while the poilcy network tells the agent which actions to take. \n",
    "\n",
    "Instead of adjusting the parameters by the product of the gradients and the rewards, the Actor-Critic approach adjusts parmaters by the preoduct of the gradients and teh advanrtage. The advantage is defined as the difference between the reward and the expected outcome:\n",
    "$$A(s_t,a_t)=r_t(s_{0},a_{0},\\ldots ,a_{T-1},s_T)-V(s_t)$$\n",
    "\n",
    "The logic is as follows: if the agent expects to win the game (i.e., the value of $V(s_t)$ is high), and the agent eventually wins the game, then the move made by the agent at time t, $a_t$, may not be that valuable. Therefore, teh agent shouldn't adjust the model parameters that much. On the other hand, if the agent expects to lose the game (i.e., the value of $V(s_t)$ is low), and the agent surprisingly wins the game, the move made by the agent at time t, $a_t$, must be a really good move.  Therefore, teh agent should adjust the model parameters so that such valuable moves are rewarded. This is a more efficient way of adjusting the model parameters to produce intelligent agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "# 2. Use Actor-Critic to Play the Coin Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "You have learned how to play the coin game using the actor critic approach. Specifically, we'll adjust the parameters proportional to the product of the gradients and teh advanrtage.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "### 2.1. Create the Model with Two Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_inputs = 22\n",
    "num_actions = 2\n",
    "# The input layer\n",
    "inputs = layers.Input(shape=(22,))\n",
    "# The common layer\n",
    "common = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "common = layers.Dense(32, activation=\"relu\")(common)\n",
    "# The policy network\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "# The value network\n",
    "critic = layers.Dense(1, activation=\"tanh\")(common)\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1def0",
   "metadata": {},
   "source": [
    "The input shape is 22 since we'll convert the game state into a one-hot variable with a depth of 22 (since the number of coins left in the pile can range from 0 to 21). There are two possible actions for the agent: the agent can take either 1 or 2 coins from the pile in each turn. \n",
    "\n",
    "The value network and the policy network share a common network, but the model has two output networks: the action network has two neurons in it since there are two possible actions that the agent can take. We use the softmax activation function so the two values in the action network add up to one. We can interpret the two values as the probabilities of takeing actions 1 and 2, respectively. \n",
    "\n",
    "The value network has one neuron in it. The output is the expected outcome of the game for the current player. We use tanh activation function in the value network so the output is a value between -1 and 1. We can interpret a value of -1 as an expectation that the current palyer will lose the game, hwile a vlaue of 1 indicating that the current player will win the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_func = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 2.2. Calculate Gradients and Discounted Rewards\n",
    "We'll let the agent play against the rule-based AI that we developed in Chapter 1. For that purpose, we first define a rule_based_AI() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b08faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rule_based_AI(state):\n",
    "    if state%3 != 0:\n",
    "        move = state%3\n",
    "    else:\n",
    "        move = random.choice([1,2])\n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca631e4",
   "metadata": {},
   "source": [
    "We also define a onehot_encoder() function to convert the state to a one-hot variable with a depth of 22, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b557b2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encoder(state):\n",
    "    onehot=np.zeros((1,22))\n",
    "    onehot[0,state]=1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfd813",
   "metadata": {},
   "source": [
    "The playing() function simulates a full game, with the rule-based AI as the first player, and the actor-critic (AC) agent as the second player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "import tensorflow as tf\n",
    "\n",
    "env=coin_game()\n",
    "def playing():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    # Rule-based AI moves first\n",
    "    state, reward, done, _ = env.step(rule_based_AI(state))\n",
    "    while True:\n",
    "        # convert state to onehot to feed to model\n",
    "        onehot_state = onehot_encoder(state)\n",
    "        # estimate action probabilities and future rewards\n",
    "        action_probs, critic_value = model(onehot_state)\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "        # Apply the sampled action in our environment\n",
    "        # Remember to add 1 to action since the actual actions are 1 and 2\n",
    "        state, reward, done, _ = env.step(action+1)\n",
    "        if done:\n",
    "            # Since AC player is player 2, -1 means AC player wins\n",
    "            reward = -reward\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward \n",
    "            break\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(rule_based_AI(state))\n",
    "            reward = -reward\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward                 \n",
    "            if done:\n",
    "                break\n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387db1da",
   "metadata": {},
   "source": [
    "The playing() function simulates a full game, and it records all the intermediate steps made by the AC agent. The function returns four values: \n",
    "* a list action_probs_history with the natural logorithm of the recommended probability of the action taken by the agent from the policy network; \n",
    "* a list critic_value_history with the estimated future rewards from the value network; \n",
    "* rewards_history with the rewards to each action taken by the agent in the game;\n",
    "* a number episode_reward showing the total rewards to the agent during the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1121b",
   "metadata": {},
   "source": [
    "The playing() function plays a full game and calculates the gradients and rewards.\n",
    "\n",
    "In reinforcement learning, actions affect not only current period rewards, but also future rewards. We therefore use discounted rewards to assign credits properly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def discount_rs(r):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = gamma*running_add + r[i]\n",
    "        discounted_rs[i] = running_add  \n",
    "    return discounted_rs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 2.3. Update Parameters\n",
    "Instead of updating model parameters after one episode, we update after a certain number of episodes to make the model stable. Here we update parameters every ten games, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "def create_batch(batch_size):\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,cvs,rs,er = playing()\n",
    "        returns = discount_rs(rs)\n",
    "        action_probs_history += aps\n",
    "        critic_value_history += cvs\n",
    "        rewards_history += returns\n",
    "        episode_rewards.append(er)\n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cd09207",
   "metadata": {},
   "source": [
    "We'll train the model and update the parameters until the average episode reward to the AC agent in the last 100 games reaches 0.999. Since in each episode, the total reward to the AC agent is either -1 or 1, this amounts to that the AC agent wins all 100 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6e8ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -1.000000 at episode 100\n",
      "running reward: -0.980000 at episode 200\n",
      "running reward: -0.920000 at episode 300\n",
      "running reward: -0.960000 at episode 400\n",
      "running reward: -0.900000 at episode 500\n",
      "running reward: -0.880000 at episode 600\n",
      "running reward: -0.260000 at episode 700\n",
      "running reward: 0.800000 at episode 800\n",
      "running reward: 0.900000 at episode 900\n",
      "running reward: 0.960000 at episode 1000\n",
      "running reward: 1.000000 at episode 1100\n",
      "Solved at episode 1100!\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "running_rewards=deque(maxlen=100)\n",
    "gamma = 0.95  \n",
    "episode_count = 0\n",
    "# Train the model\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs_history,critic_value_history,\\\n",
    "            rewards_history,episode_rewards=create_batch(batch_size)\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, rewards_history)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # Calculate actor loss\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "            # Calculate critic loss\n",
    "            critic_losses.append(\n",
    "                loss_func(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "        # Adjust model parameters\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    for r in episode_rewards:\n",
    "        running_rewards.append(r)\n",
    "    running_reward=np.mean(np.array(running_rewards)) \n",
    "    # print out progress\n",
    "    if episode_count % 100 == 0:\n",
    "        template = \"running reward: {:.6f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))   \n",
    "    # Stop if the game is solved\n",
    "    if running_reward > 0.999 and episode_count>100:  \n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "model.save(\"files/ch15/AC_coin.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c598877",
   "metadata": {},
   "source": [
    "Note here we adjust the parameters by the preduct of teh gradients and the discounted rewards. This is related to the solution to the rewards maximizatip probelm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa7b9a",
   "metadata": {},
   "source": [
    "# 3. Play Games with the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "We'll use the trained model to play against the rule-based AI. We'll use two strategies: deterministic policy and stochastic policy. If you choose a deterministic policy, you select teh move with the highest probabiltity. On the other hand, if you choose stochastic policy, you select the moves according to the probability produced by the policy network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf2ef2",
   "metadata": {},
   "source": [
    "## 3.1. Deterministic Policy from the Trained Model\n",
    "You play 1000 games against the rule-based AI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c3f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env = coin_game()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = rule_based_AI(state)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record -1 if rule-based AI player won\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # estimate action probabilities and future rewards\n",
    "        onehot_state = onehot_encoder(state)\n",
    "        action_probs, _ = model(onehot_state)\n",
    "        # select action with the highest probability\n",
    "        action=np.argmax(action_probs[0])+1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record 1 if AC agent won\n",
    "            results.append(1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea611d",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdcf55ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 100 games\n",
      "The AC player has lost 0 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3485a",
   "metadata": {},
   "source": [
    "The AC agent plays perfectly and wins all 100 games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306efab3",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic Policy from the Trained Model\n",
    "What if the AC player uses a stochastic policy? That is, the agent will randomly choose actions, with probabilities recommended by the policy network from the trained model. We'll test such a strategy next. The AC player uses a stochastic policy to play 100 games against the rule-based AI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1939968",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env = coin_game()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = rule_based_AI(state)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record -1 if rule-based AI player won\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # estimate action probabilities and future rewards\n",
    "        onehot_state = onehot_encoder(state)\n",
    "        action_probs, _ = model(onehot_state)\n",
    "        # select action with the highest probability\n",
    "        action=np.random.choice(num_actions,p=np.squeeze(action_probs))+1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record 1 if AC agent won\n",
    "            results.append(1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248266b",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9444762f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 100 games\n",
      "The AC player has lost 0 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53260f1d",
   "metadata": {},
   "source": [
    "The AC agent has won all 100 games as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974dc206",
   "metadata": {},
   "source": [
    "# 4. What If the AC Agent Moves First?\n",
    "We train the AC agent against a perfect rule-based AI player. Since if both sides play perfectly, the second player always wins, so we let the AC agent move second and see if it can learn to play perfectly. The answer is yes based on results from the last section.\n",
    "\n",
    "Now, what if teh AC agent moves first? The opponent cannot be a perfect player, so we select a random player to play against the AC agent. We need to change a few things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc94da0",
   "metadata": {},
   "source": [
    "## 4.1. Define A playing_first() Function\n",
    "The playing_first() function simulates a full game, with the AC agent as the first player, and the random player as the second player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "498ba568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_first():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        # convert state to onehot to feed to model\n",
    "        onehot_state = onehot_encoder(state)\n",
    "        # estimate action probabilities and future rewards\n",
    "        action_probs, critic_value = model(onehot_state)\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "        # Apply the sampled action in our environment\n",
    "        # Remember to add 1 to action since the actual actions are 1 and 2\n",
    "        state, reward, done, _ = env.step(action+1)\n",
    "        if done:\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward \n",
    "            break\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(random.choice([1,2]))\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward                 \n",
    "            if done:\n",
    "                break\n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481efe41",
   "metadata": {},
   "source": [
    "The playing() function simulates a full game, and it records all the intermediate steps made by the AC agent. The function returns four values: \n",
    "* a list action_probs_history with the natural logorithm of the recommended probability of the action taken by the agent from the policy network; \n",
    "* a list critic_value_history with the estimated future rewards from the value network; \n",
    "* rewards_history with the rewards to each action taken by the agent in the game;\n",
    "* a number episode_reward showing the total rewards to the agent during the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76303c41",
   "metadata": {},
   "source": [
    "The playing() function plays a full game and calculates the gradients and rewards.\n",
    "\n",
    "In reinforcement learning, actions affect not only current period rewards, but also future rewards. We therefore use discounted rewards to assign credits properly as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf08762",
   "metadata": {},
   "source": [
    "## 4.2. Update Parameters\n",
    "Instead of updating model parameters after one episode, we update after a certain number of episodes to make the model stable. Here we update parameters every ten games, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0474599",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "def create_batch_first(batch_size):\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,cvs,rs,er = playing_first()\n",
    "        returns = discount_rs(rs)\n",
    "        action_probs_history += aps\n",
    "        critic_value_history += cvs\n",
    "        rewards_history += returns\n",
    "        episode_rewards.append(er)\n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "raw",
   "id": "760a15e1",
   "metadata": {},
   "source": [
    "We'll train the model and update the parameters until the average episode reward to the AC agent in the last 100 games reaches 0.999. Since in each episode, the total reward to the AC agent is either -1 or 1, this amounts to that the AC agent wins all 100 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53819832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 1.000000 at episode 100\n",
      "Solved at episode 110!\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "running_rewards=deque(maxlen=100)\n",
    "gamma = 0.95  \n",
    "episode_count = 0\n",
    "# Train the model\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs_history,critic_value_history,\\\n",
    "            rewards_history,episode_rewards=create_batch_first(batch_size)\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, rewards_history)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # Calculate actor loss\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)\n",
    "            # Calculate critic loss\n",
    "            critic_losses.append(\n",
    "                loss_func(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "        # Adjust model parameters\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    for r in episode_rewards:\n",
    "        running_rewards.append(r)\n",
    "    running_reward=np.mean(np.array(running_rewards)) \n",
    "    # print out progress\n",
    "    if episode_count % 100 == 0:\n",
    "        template = \"running reward: {:.6f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))   \n",
    "    # Stop if the game is solved\n",
    "    if running_reward > 0.999 and episode_count>100:  \n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "model.save(\"files/ch15/AC_coin_first.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e06b79",
   "metadata": {},
   "source": [
    "Note here we adjust the parameters by the preduct of teh gradients and the discounted rewards. This is related to the solution to the rewards maximizatip probelm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85972546",
   "metadata": {},
   "source": [
    "# 5. Play Games with the New Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bbde66",
   "metadata": {},
   "source": [
    "We'll use the trained model to play against the rule-based AI. We'll use two strategies: deterministic policy and stochastic policy. If you choose a deterministic policy, you select teh move with the highest probabiltity. On the other hand, if you choose stochastic policy, you select the moves according to the probability produced by the policy network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba90b549",
   "metadata": {},
   "source": [
    "## 5.1. Deterministic Policy from the Trained Model\n",
    "You play 1000 games against the rule-based AI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45deab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env = coin_game()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = random.choice([1,2])  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # estimate action probabilities and future rewards\n",
    "        onehot_state = onehot_encoder(state)\n",
    "        action_probs, _ = model(onehot_state)\n",
    "        # select action with the highest probability\n",
    "        action=np.argmax(action_probs[0])+1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record 1 if AC agent won\n",
    "            results.append(1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bf2ce",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72f9614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 100 games\n",
      "The AC player has lost 0 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f44a200",
   "metadata": {},
   "source": [
    "The AC player still plays perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f295d83",
   "metadata": {},
   "source": [
    "## 5.2. Stochastic Policy from the Trained Model\n",
    "You play 100 games against the rule-based AI: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ded01d",
   "metadata": {},
   "source": [
    "We test ten games and see on averge how many consecutive steps the cart pole can stay upright. We define a test_a_game() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e966f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env = coin_game()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = random.choice([1,2])  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record -1 if rule-based AI player won\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # estimate action probabilities and future rewards\n",
    "        onehot_state = onehot_encoder(state)\n",
    "        action_probs, _ = model(onehot_state)\n",
    "        # select action with the highest probability\n",
    "        action=np.random.choice(num_actions,p=np.squeeze(action_probs))+1\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record 1 if AC agent won\n",
    "            results.append(1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ac0af",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0a7a5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 99 games\n",
      "The AC player has lost 1 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aac8ccc",
   "metadata": {},
   "source": [
    "The AC player has won 99 out of 100 games. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
