{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 5: Depth Pruning in Minimax\n",
    "\n",
    "You have learned how the minimax algorithm works in Chapter 4. You'll use it to play two other games in this chapter: Tic Tac Toe and Connect Four. The algorithm exhausts all possibilities in the Tic Tac Toe game. The minimax agent plays perfectly: no strategy can beat it. However, it takes about half a minute for the agent to make its first move.\n",
    "\n",
    "You'll then apply the minimax algorithm to the Connect Four game. While there is nothing wrong with the algorithm, it takes forever for the agent to make a move. That is, the minimax algorithm doesn't work unless you have a super computer. \n",
    "\n",
    "You'll learn how to cut down on the amount of time the minimax agent can come up with a move. The obvious answer is depth pruning: the agent stops searching after a fixed number of stages. For example, you can limit the program to look ahead at most four steps so that the program can recommend a solution in just a few seconds.\n",
    "\n",
    "After that, you'll test your minimax agents with the rule-based AI that you developed in Chapters 2 and 3 and see which agent is more powerful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 5}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 5 in a subfolder /files/ch05. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch05\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. Minimax Tree Search in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "You have already learned how the minimax algorithm works in Chapter 4. Basically, the algorithm assumes each player in the game makes the best possible decisions at each step. Players also know that their opponents make fully rational decisions as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "The minimax agent in Tic Tac Toe come up with best moves through backward induction. It starts with the terminal state of the game (in Tic Tac Toe, when the game is tied or when one player has won) and finds out the payoffs to each player in that state. In the second to last stage of the game, the player looks one step ahead and makes the best decision for himself/herself, anticipating that the opponent makes the best decision in the previous stage, and so on.\n",
    "\n",
    "In Tic Tac Toe, Each game has a maximum of 9 stages. In stage 9 (assuming the game is not over by then), player X has only one choice so no decision is needed. In stage 8, player O looks at the two choices and picks the best one for himself/herself. In stage 7, player X picks the best decision, knowing that player O will pick a choice that minimizes player X's payoff in stage 8, and so on. The reasoning goes all the way back to the very first step when player X makes a decision. \n",
    "\n",
    "Since the total number of possible scenarios in a Tic Tac Toe game is small (less than $3^9=19,683$), the computer program can exhaust all scenarios in a reasonable amount of time and find the best solution for each player in every stage of the game. We'll discuss how to reduce the amount of time that the agent needs to make a decision through depth pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. The Minimax Algorithm in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "We'll use the self-made Tic Tac Toe game environment we created in Chapter 2. Specifically, the module is saved as *ttt_env.py* in the folder *utils* in this GitHub repository. Download the file and save it under /Desktop/ai/utils/ on your computer. \n",
    "\n",
    "First, let's define a couple of functions that the algorithm uses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddc025",
   "metadata": {},
   "source": [
    "We'll define a minimax_X() function for the player X. Potentially we can define one function for both players but it's more difficult to explain. There is a tradeoff between coding efficiency and readabiliyt of the code. So here we choose the latter. \n",
    "\n",
    "The function tells the player X what's the best next move, anticipating that player O will make the best decision in the next stage as well, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8839e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_X(env):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward==1:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=maximized_payoff(env_copy,reward,done)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return choice(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110e83",
   "metadata": {},
   "source": [
    "At each step, player X iterates through all possible next moves. If a move allows player X to win the game right away, player X will stop searching and take the move. Otherwise, player X will see what's the best outcome for player O in the next stage, knowing full well that player O will make the best decision to maximize player O's payoff. Since it's a zero-sum game, payer X's payoff is the opposite of player O's payoff. Player X will then pick winning moves if there is one; otherwise, he/she will pick a typing move; otherwise, player X has no choice but to pick whatever move is left. \n",
    "\n",
    "Here, we use the *maximized_payoff()* function to find the best payoff for player O in the next stage. Let's define that function next.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "Next, we'll define the *maximized_payoff(env,reward,done)* function. This function produces the best possible outcome for the next player in the next stage of the game. Note this function applies to any stage of the game so we don't need to define one for player X and one for player O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a847d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximized_payoff(env,reward,done):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # Otherwise, search for action to maximize payoff\n",
    "    best_payoff=-2\n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=maximized_payoff(env_copy,reward,done)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        # update your best payoff \n",
    "        if my_payoff>best_payoff:        \n",
    "            best_payoff=my_payoff\n",
    "    return best_payoff"
   ]
  },
  {
   "cell_type": "raw",
   "id": "556288e2",
   "metadata": {},
   "source": [
    "If the game has ended after the previous player's move, the function calculates the payoff to the player based on the game outcome. Otherwise, the next player searches for the best action by iterating throug all possible next moves, knowing full well that the opponent takes a fully rational action in the next stage as well.\n",
    "\n",
    "Note here that we have used the *maximized_payoff()* function in the *maximized_payoff()* function itself. This creates an infinite loop. The function keeps on searching to the next stage until the game ends. The process exhausts all game scenarios in Tic Tac Toe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "## 1.3. Test the Minimax Algorithm in Tic Tac Toe\n",
    "Next, you'll play a game against the minimax algorithm. We'll let the minimax agent move first and see if it can win the game. We also time how long it takes for the minimax agent to come up with each move.\n",
    "\n",
    "Warning: it takes about 20 seconds on my computer for the minimax agent to make the first move. It may take longer depending on your computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=5\n",
      "It took the agent 29.12164831161499 seconds\n",
      "the current state is \n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Player O, what's your move?\n",
      "4\n",
      "Player O has chosen action=4\n",
      "the current state is \n",
      "[[ 0  0  0]\n",
      " [-1  1  0]\n",
      " [ 0  0  0]]\n",
      "Player X has chosen action=7\n",
      "It took the agent 0.3951141834259033 seconds\n",
      "the current state is \n",
      "[[ 0  0  0]\n",
      " [-1  1  0]\n",
      " [ 1  0  0]]\n",
      "Player O, what's your move?\n",
      "3\n",
      "Player O has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0 -1]\n",
      " [-1  1  0]\n",
      " [ 1  0  0]]\n",
      "Player X has chosen action=8\n",
      "It took the agent 0.01595616340637207 seconds\n",
      "the current state is \n",
      "[[ 0  0 -1]\n",
      " [-1  1  0]\n",
      " [ 1  1  0]]\n",
      "Player O, what's your move?\n",
      "9\n",
      "Player O has chosen action=9\n",
      "the current state is \n",
      "[[ 0  0 -1]\n",
      " [-1  1  0]\n",
      " [ 1  1 -1]]\n",
      "Player X has chosen action=2\n",
      "It took the agent 0.0 seconds\n",
      "the current state is \n",
      "[[ 0  1 -1]\n",
      " [-1  1  0]\n",
      " [ 1  1 -1]]\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_env import ttt\n",
    "from utils.ch05util import minimax_X,maximized_payoff \n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action = minimax_X(env)\n",
    "    end=time.time()\n",
    "    print(f\"Player X has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The minimax algorithm first occupies Cell 5. I occupied Cell 4. The minimax algorithm then occupies Cells 7 and 8, creating a double attack: it can win in either Cell 2 or Cell 9 in the next move. Since I can only stop one of the two attacks, the minimax algorithm has generated a move to guarantee a win. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece520",
   "metadata": {},
   "source": [
    "## 1.4. Efficacy of the Minimax Algorithm in Tic Tac Toe\n",
    "Next, we’ll test how often the Minimax Algorithm wins against the think-three-steps-ahead game strategy that we developed in Chapter 2. \n",
    "\n",
    "To do that, we first define a minimax_O() function in the local ch05util module, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d392cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_O(env):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward==-1:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=maximized_payoff(env_copy,reward,done)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return choice(losses)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "The only difference is that we changed\n",
    "\n",
    "        if done and reward==1:\n",
    "\n",
    "to \n",
    "\n",
    "        if done and reward==-1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a917e1e7",
   "metadata": {},
   "source": [
    "We have also defined a few other functions in the local ch05util module: AI_think1(), AI_think2(), AI_think3(), and test_ttt_game(). They are defined similar to what we have done in Chapter 2. You can open the file ch05util.py to have a look.\n",
    "\n",
    "To test how the minimax agent fairs against the think-three-steps-ahead AI player, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8367f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import minimax_X,minimax_O,AI_think3,test_ttt_game \n",
    "\n",
    "\n",
    "# Play a full game manually\n",
    "results=[]\n",
    "for i in range(10):\n",
    "    # AI moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=test_ttt_game(minimax_X,AI_think3)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # AI moves second if i is an odd number\n",
    "    else:\n",
    "        result=test_ttt_game(AI_think3,minimax_O)\n",
    "        # record negative of game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b311286",
   "metadata": {},
   "source": [
    "We test 10 games. The minimax agent goes first in five games and the think-three-steps-ahead AI player goes first in the other half so no player has a first-mover's advantage. The game outcome is 1 when the first player wins and -1 when the second player wins. The game is tied when the outcome is 0. When the minimax agent is playing second, we multiply the outcome by -1 so that in all 10 games, a value 1 indicates that the minimax agent has won in the list *results*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a7084ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimax agent has won 5 games\n",
      "the minimax agent has lost 0 games\n",
      "the game has tied 5 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the minimax agent has won\n",
    "wins=results.count(1)\n",
    "print(f\"the minimax agent has won {wins} games\")\n",
    "# count how many times minimax agent has lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the minimax agent has lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game has tied {ties} times\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85a350",
   "metadata": {},
   "source": [
    "The results show that the minimax agent has won 5 games, while the rest 5 games are tied. The minimax agent has never lost to the think-three-steps ahead AI player. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe6000",
   "metadata": {},
   "source": [
    "# 2. Depth Pruning in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1417875",
   "metadata": {},
   "source": [
    "In the last section, you have seen that it took the minimax agent 29 seconds to make the first move. While this is tolerable, in more complicated games such as Connect Four or Chess, it takes forever for the agent to make a move. Therefore, something has to be done. \n",
    "\n",
    "Depth pruning is one solution: instead of searching all the way to the terminal state of the game, the algorithm stops searching after a fixed number of steps. In this section, you'll learn how to implement depth pruning in the game of Tic Tac Toe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c0ce",
   "metadata": {},
   "source": [
    "## 2.1. The max_payoff() Function\n",
    "We'll define a max_payoff() function. The function is similar to the maximized_payoff() function we defined in the last section. However, there are two important differences. First, there is a depth argument in the function to control how many steps the minimax agent searches. Second, we'll make the function general so that it can be applied to Tic Tac Toe as well as the Connect Four game later in this chapter. \n",
    "\n",
    "The max_payoff() function is defined as follows. It's saved in the file ch05util.py that you just downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b94f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_payoff(env,reward,done,depth):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # If the maximum depth is reached, assume tie game\n",
    "    if depth==0:\n",
    "        return 0        \n",
    "    # Otherwise, search for action to maximize payoff\n",
    "    best_payoff=-2\n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=max_payoff(env_copy,reward,done,depth-1)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        # update your best payoff \n",
    "        if my_payoff>best_payoff:        \n",
    "            best_payoff=my_payoff\n",
    "    return best_payoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b208ef",
   "metadata": {},
   "source": [
    "In the function, if the variable depth reaches 0, we assume the game is tied and the payoff is 0. Later in this book, we'll use position evaluation functions to generate a value that's more realisitc. But a value of 0 will do for the moment. \n",
    "\n",
    "When the palyer makes a hypothetical move and anticipate the best response from the opponent, it uses the function max_payoff(env_copy,reward,done,depth-1). This means that each time the player searches to the next level, the depth variable decreases by 1. Once the variable depth reaches 0, the agent stops searching. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cead4",
   "metadata": {},
   "source": [
    "## 2.2. The minimax() Function\n",
    "We also define a minimax() function to produce the best move for the minimax agent. However, we make two changes: first, there is a depth argument in the function to control how many steps the minimax agent searches. Second, we'll make the function general so that it can be applied to Tic Tac Toe as well as the Connect Four game later in this chapter. \n",
    "\n",
    "The minimax() function is defined as follows. It's saved in the file ch05util.py that you just downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ef98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax(env,depth=3):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=max_payoff(env_copy,reward,done,depth)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return choice(losses)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bfefc",
   "metadata": {},
   "source": [
    "The minimax() function has two arguments: env, which is the game environment. It can be either the Tic Tac Toe for the Connect Four mae environment. The second argument, depth, is how many steps the minimax agent searches before making a move. The default value of depth is set to 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d4e3",
   "metadata": {},
   "source": [
    "## 2.3. Speed of the Depth-Pruned Minimax Agent\n",
    "Next, we test how fast is the depth-pruned minimax agent. We use the default depth of 3, and play a game with the agent. We let the agent play first again and measure how long it takes for the agent to make a move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20703e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=2\n",
      "It took the agent 0.1972355842590332 seconds\n",
      "the current state is \n",
      "[[0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Player O, what's your move?\n",
      "5\n",
      "Player O has chosen action=5\n",
      "the current state is \n",
      "[[ 0  1  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "Player X has chosen action=7\n",
      "It took the agent 0.055222272872924805 seconds\n",
      "the current state is \n",
      "[[ 0  1  0]\n",
      " [ 0 -1  0]\n",
      " [ 1  0  0]]\n",
      "Player O, what's your move?\n",
      "1\n",
      "Player O has chosen action=1\n",
      "the current state is \n",
      "[[-1  1  0]\n",
      " [ 0 -1  0]\n",
      " [ 1  0  0]]\n",
      "Player X has chosen action=9\n",
      "It took the agent 0.0 seconds\n",
      "the current state is \n",
      "[[-1  1  0]\n",
      " [ 0 -1  0]\n",
      " [ 1  0  1]]\n",
      "Player O, what's your move?\n",
      "8\n",
      "Player O has chosen action=8\n",
      "the current state is \n",
      "[[-1  1  0]\n",
      " [ 0 -1  0]\n",
      " [ 1 -1  1]]\n",
      "Player X has chosen action=4\n",
      "It took the agent 0.000995635986328125 seconds\n",
      "the current state is \n",
      "[[-1  1  0]\n",
      " [ 1 -1  0]\n",
      " [ 1 -1  1]]\n",
      "Player O, what's your move?\n",
      "3\n",
      "Player O has chosen action=3\n",
      "the current state is \n",
      "[[-1  1 -1]\n",
      " [ 1 -1  0]\n",
      " [ 1 -1  1]]\n",
      "Player X has chosen action=6\n",
      "It took the agent 0.0 seconds\n",
      "the current state is \n",
      "[[-1  1 -1]\n",
      " [ 1 -1  1]\n",
      " [ 1 -1  1]]\n",
      "Game over, it's a tie!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch05util import minimax\n",
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action = minimax(env,depth=3)\n",
    "    end=time.time()\n",
    "    print(f\"Player X has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a7d1a",
   "metadata": {},
   "source": [
    "It took only 0.2 seconds for the minimax agent to make the first move, instead of 29 seconds. That's a huge cutdown on the amount of time it takes to come up with a move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df6cd7",
   "metadata": {},
   "source": [
    "# 3. Depth Pruning in Connect Four\n",
    "Next, we’ll create a minimax ageent for the connect four game. The agent searches for a maximum of three steps. \n",
    "\n",
    "We first manually play a game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e36b6b6",
   "metadata": {},
   "source": [
    "## 3.1. Manually Play A Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb92577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The red player has chosen action=5\n",
      "It took the agent 0.14301228523254395 seconds\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]]\n",
      "Player yellow, what's your move?\n",
      "1\n",
      "Player yellow has chosen action=1\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  1  0  0]]\n",
      "The red player has chosen action=6\n",
      "It took the agent 0.14829635620117188 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  1  1  0]]\n",
      "Player yellow, what's your move?\n",
      "1\n",
      "Player yellow has chosen action=1\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0  0  0  1  1  0]]\n",
      "The red player has chosen action=4\n",
      "It took the agent 0.13970661163330078 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0  0  1  1  1  0]]\n",
      "Player yellow, what's your move?\n",
      "3\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0 -1  1  1  1  0]]\n",
      "The red player has chosen action=7\n",
      "It took the agent 0.1260826587677002 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [-1  0 -1  1  1  1  1]]\n",
      "The red player has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "import time\n",
    "from utils.ch05util import minimax\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action=minimax(env,depth=3)\n",
    "    end=time.time()\n",
    "    print(f\"The red player has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"The red player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action=input(\"Player yellow, what's your move?\\n\")\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==-1:\n",
    "            print(f\"The yellow player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fa1bb",
   "metadata": {},
   "source": [
    "The minimax agent is able to plan three steps ahead and create a double attack and win the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "## 3.2. The Minimax Algorithm verus Rule-Based AI\n",
    "We'll test if the minimax algorithm that searches for three steps ahead can beat the rule-based think-three-steps-ahead AI player that we created in Chapter 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3be50ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import test_conn_game,conn_think3\n",
    "\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    # minimax agent moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=test_conn_game(env,minimax,conn_think3)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # minimax agent moves second if i is an odd number\n",
    "    else:\n",
    "        result=test_conn_game(env,conn_think3,minimax)\n",
    "        # record negative of game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff7f0",
   "metadata": {},
   "source": [
    "We create a list *results* to store game outcomes. We simulate 100 games and half the time, the minimax agent moves first and the other half, the rule-based AI player moves first. This way, no player has a first-mover advantage and we have a fair assessment of the power of the minimax agent against the rule-based AI. Whenever the minimax moves second, we multiple the outcome by -1 so that a value 1 in the list *results* indicates that the minimax has won. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9eecbe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimax agent has won 68 games\n",
      "the minimax agent has lost 26 games\n",
      "the game has tied 6 games\n"
     ]
    }
   ],
   "source": [
    "# count how many times minimax agent has won\n",
    "wins=results.count(1)\n",
    "print(f\"the minimax agent has won {wins} games\")\n",
    "# count how many times minimax agent has lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the minimax agent has lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game has tied {ties} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7af50",
   "metadata": {},
   "source": [
    "The above output shows that the MiniMax agent has won 68 games, lost 26, and the rest 6 games are tied. The results show that the MiniMax agent is better than a think-three-step-ahead agent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
