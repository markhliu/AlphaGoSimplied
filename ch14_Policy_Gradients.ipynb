{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 14: Policy-Based Deep Reinforcement Learning\n",
    "\n",
    "AlphaGo use a deep reinforcement learning model called Actor-Critic method to create a value head and a policy head. It then combines the value head and policy head with MCTS to crearte powerful plays. To understand AC, we need to understand policy based deep reinformcent learning as well. So this chapter introducdes you to that.\n",
    "\n",
    "In the last two chapters, you have learned to use value-based reinforcement learning to solve the Frozen Lake and Cart Pole games. To train the model, you used trial and error to learn the value of each action in a certain state, $V(a|s)$. Once the reinforcement model is trained, you play the game by choosing the action with the highest value function $V(a|s)$ in the state $s$. \n",
    "\n",
    "In this chapter, you'll learn policy-based reinforcement learning. Instead of trying to make decisions based on the value functions, $V(a|s)$, the agent makes decisions based on a policy $\\pi(a|s)$ that explicitly tells you which action to take in each state in the game environment. While a deterministic policy tells tells the agent which action to take, a stochastic policy gives the probability distribution over all possible actions. \n",
    "\n",
    "In particular, you'll learn a method called policy gradients. To learn the best policy, the agent plays the game many times. When playing the game, the agent takes action based on the model's predictions. The agent then observes the rewards from the actions taken and adjusts the model weights accordingly. If the prediction is smaller than the desired outcome, the agent adjusts the model weights so that the prediction will increase. Similarly, if the prediction is greater than the desired outcome, the agent adjusts the model weights so that the prediction will decrease. Further, the magnitude of the adjustment is directly proportional to the rewards: the greater the reward, the greater the adjustment. You'll use the policy gradient method to play the Cart Pole game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 14}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 14 in a subfolder /files/ch14. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch14\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. Policy-Based Reinforcement Learning\n",
    "This section introducdes you to policy-based reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63f10c",
   "metadata": {},
   "source": [
    "## 1.1. What Is A Policy?\n",
    "A policy, $\\pi(a|s)$, can be any algorithm that tells the agent which action to take in a given state. Let's use the Cart Pole game we discussed in Chatper 13 as an example. We'll create a deep neural network that takes the current state as the input. We'll put one single neuron in the output layer so the output from the deep neural network is a signle number. We use sigmoid activation in the output layer so the output is a number between 0 and 1.  \n",
    "\n",
    "Our poicy could be: if the output from the neural network is greater than or equal to 0.5, we'll move the cartpole left (i.e., take action 0); otherwise, we'll move the Cart Pole right. This is called a deterministic policy in the sense that the action is determined once we know the output from teh DNN. A policy can be stochastic as well: since the output from the neural network, let's call it p, is a number between 0 and 1, we can have a stochastic policy as follows:\n",
    "* Move the cart pole to the left (i.e., take action 0) with probabiulity p;\n",
    "* Move the cart pole to the right (i.e., take action 1) with probabiltiy 1-p;\n",
    "\n",
    "The advantage of a stochastic policy is that it naturally allows for both exploitation and exploration. It allows for exploitation in the sense that the probabiltiy of action=0 is greater when the value of p increases. It also allows for exploration in the sense that the recommended policy is not taken with 100% certainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed19b2e4",
   "metadata": {},
   "source": [
    "## 1.2. What is the Policy Gradient Method?\n",
    "Policy gradients is an algorithm to adjust the model parameters to achieve the best outcome for an reinforcement learning agent. \n",
    "\n",
    "In RL, the agent is trying to learn the best strategy in order to maximize his or her expected payoff over time. A strategy (also called a policy) maps a certain state to a certain action. A strategy is basically a decision rule that tells the agent what to do in a certain situation.\n",
    "\n",
    "Let's say that the policy we are considering is $\\pi _{\\theta }(a_t|s_t,\\theta)$, where $\\theta$ are model parameters (e.g., the weights in a neural network). That is, the agent choose an action $a_t$ in time period $t$ based on the current state $s_t$, as a function of model parameters $\\theta$. Let's say that the agent needs to choose a sequence of actions $(a_{0},a_{1},\\ldots ,a_{T-1})$ to maximize her expected cumulated rewards as follows. In period $t$, after observing the state $s_t$ and taking action $a_t$, the agent receives a reward of $r(a_t,s_t)$. If the discount rate is $\\gamma$, the expected cumulative reward to the agent is \n",
    "$$\n",
    "R(s_{0},a_{0},\\ldots ,a_{T-1},s_T)=\\sum\\limits_{t=0}^{T-1}\\gamma ^{t}r(s_{t},a_{t})+\\gamma ^{T}r(s_{T})\n",
    "$$\n",
    "where $s_T$ is the terminal state. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c587ec45",
   "metadata": {},
   "source": [
    "The objective of the agent is to find the optimal parameter values for the model, $\\theta$, to maiximize the expected cumulative payoffs: \n",
    "\n",
    "$$\n",
    "\\max_{\\theta }E[R(s_{0},a_{0},\\ldots ,a_{T-1},s_T)|\\pi _{\\theta }]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a163a987",
   "metadata": {},
   "source": [
    "The above maximization prooblem can be solved by using a gradient ascent algorithm. That is, we can update the model parameters $\\theta$ by using the following formula until the parmaterters converge: \n",
    "$$\n",
    "\\theta \\leftarrow \\theta +Learning\\ Rate\\ast \\nabla _{\\theta }E[R|\\pi _{\\theta }]\n",
    "$$\n",
    "\n",
    "where $Learning\\ Rate$ is the learning rate hyperparameter that controls how fast we update the model pamameters. This boils down to train the model to prodict the probability of the correct action based on the state. The solution is \n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta +Learning\\ Rate \\times E[\\sum\\limits_{t=0}^{T-1} \\nabla _{\\theta }log\\pi _{\\theta }(a_{t}|s_{t})R|\\pi _{\\theta }]\n",
    "$$\n",
    "This is the formula we'll use in this chapter with the policy-gradient method. If you are interested, you can read the proof provided by OpenAI here \n",
    "\n",
    "https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121801ec",
   "metadata": {},
   "source": [
    "# 2. Policy Gradients in Cart Pole\n",
    "We'll implement use policy gradients to solve the Cart Pole game. \n",
    "\n",
    "\n",
    "## 2.1. Create A Policy Network\n",
    "First, we create a neural network to create a policy as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(5, activation=\"relu\", input_dim=4))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1def0",
   "metadata": {},
   "source": [
    "Next, we'll choose teh proper optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_function = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 2.2. Calculate Gradients and Discounted Rewards\n",
    "The policy gradients approach ajusts the parameters by the product the the reward and the gradients. We'll plan a game to obtain those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae7dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "def training():\n",
    "    rewards = []\n",
    "    grads = []\n",
    "    obs = env.reset()\n",
    "    for _ in range(200):\n",
    "        with tf.GradientTape() as tape:\n",
    "            aprob = model(np.array([obs]))[0,0]\n",
    "            action = 0 if np.random.uniform(0,1)<aprob else 1            \n",
    "            y=np.array([1-action]).reshape((1,1))\n",
    "            loss=tf.reduce_mean(loss_function(y,model(np.array([obs]))))\n",
    "        grad=tape.gradient(loss,model.trainable_variables)\n",
    "        obs, reward, done, _ =env.step(action)       \n",
    "        rewards.append(reward)\n",
    "        grads.append(np.array(grad))\n",
    "        if done:\n",
    "            break\n",
    "    return rewards, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "The training() function plays a full game and calculates the gradients and rewards.\n",
    "\n",
    "In reinforcement learning, actions affect not only current period rewards, but also future rewards. We therefore use discounted rewards to assign credits properly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rs(r):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = gamma*running_add + r[i]\n",
    "        discounted_rs[i] = running_add\n",
    "    discounted_rs -= np.mean(discounted_rs)\n",
    "    discounted_rs /= np.std(discounted_rs)    \n",
    "    \n",
    "    return discounted_rs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 2.3. Update Parameters\n",
    "Instead of updating model parameters after one episode, we update after a certain number of episodes to make the model stable. Here we update parameters every ten games, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "def create_batch(batch_size):\n",
    "    gs=[]\n",
    "    rs=[]\n",
    "    for i in range(batch_size):\n",
    "        rewards, grads = training()\n",
    "        returns = discount_rs(rewards)\n",
    "        gs += grads\n",
    "        rs += returns\n",
    "    return gs,rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26e535",
   "metadata": {},
   "source": [
    "We'll use 50 batches of data to update the parameters and train the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c6e8ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\AppData\\Local\\Temp\\ipykernel_3416\\1852500917.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  grads.append(np.array(grad))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "n_batches = 200\n",
    "gamma = 0.95  \n",
    "params=model.trainable_variables\n",
    "num_layers=len(params)\n",
    "\n",
    "for _ in range(n_batches):\n",
    "    gs,rs=create_batch(batch_size)\n",
    "    gradr=np.dot(np.array(gs).reshape(-1,num_layers).T,rs)/len(rs)\n",
    "    optimizer.apply_gradients(zip(gradr,params))\n",
    "\n",
    "model.save(\"files/ch14/cartpole_deep_pg.h5\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c598877",
   "metadata": {},
   "source": [
    "Note here we adjust the parameters by the preduct of teh gradients and the discounted rewards. This is related to the solution to the rewards maximizatip probelm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa7b9a",
   "metadata": {},
   "source": [
    "# 3. Play A Game with the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "We'll use the trained model to play a game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf2ef2",
   "metadata": {},
   "source": [
    "## 3.1. Play A Complete Cart Pole Game\n",
    "You can play a complete game by using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "629df3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Your score is 200!\n"
     ]
    }
   ],
   "source": [
    "#model=keras.models.load_model(\"cartpole_deep_pg.h5\")\n",
    "\n",
    "obs=env.reset()\n",
    "score=0\n",
    "for step in range(200):\n",
    "    score += 1\n",
    "    aprob=model(np.array(obs).reshape(-1,4))\n",
    "    action = 0 if np.random.uniform(0,1)<aprob else 1\n",
    "    obs,reward,done,info=env.step(action)\n",
    "    if done:\n",
    "        print(f\"Your score is {score}!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "## 3.2. Average Performance of the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f9c06",
   "metadata": {},
   "source": [
    "We test ten games and see on averge how many consecutive steps the cart pole can stay upright. We define a test_a_game() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa78099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_a_game():\n",
    "    obs=env.reset()\n",
    "    score=0\n",
    "    for step in range(200):\n",
    "        score += 1\n",
    "        aprob=model(np.array(obs).reshape(-1,4))\n",
    "        action = 0 if np.random.uniform(0,1)<aprob else 1\n",
    "        obs,reward,done,info=env.step(action)\n",
    "        if done:\n",
    "            return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace3e11",
   "metadata": {},
   "source": [
    "We then test ten games and print out the score in each game as well as the average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c3f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in game 1, the score is 200\n",
      "in game 2, the score is 165\n",
      "in game 3, the score is 200\n",
      "in game 4, the score is 200\n",
      "in game 5, the score is 200\n",
      "in game 6, the score is 156\n",
      "in game 7, the score is 200\n",
      "in game 8, the score is 140\n",
      "in game 9, the score is 200\n",
      "in game 10, the score is 200\n",
      "the average score is 186.1\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "for i in range(10):\n",
    "    score=test_a_game()\n",
    "    print(f\"in game {i+1}, the score is {score}\")\n",
    "    results.append(score)\n",
    "avg=sum(results)/len(results)   \n",
    "print(f\"the average score is {avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea611d",
   "metadata": {},
   "source": [
    "the average score is 200.0\n",
    "\n",
    "So the trained deep Q network managed to make the cart pole stay upright for 200 consecutive time steps in every sigle game. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
