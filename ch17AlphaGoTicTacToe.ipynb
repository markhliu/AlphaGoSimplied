{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3a7f85",
   "metadata": {},
   "source": [
    "# Chaper 17: AlphaGo in Tic Tac Toe and Connect Four\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***“We also introduce a new search algorithm that combines Monte Carlo simulation\n",
    "with value and policy networks. Using this search algorithm, our program AlphaGo\n",
    "achieved a 99.8% winning rate against other Go programs.”***\n",
    "\n",
    "-- Google DeepMind Team, Nature (2016) \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Chapter 16, you have learned the basic architecture of the AlphaGo algorithm, which combines Monte Carlo tree search with deep neural networks, as outlined in the open quote in an article published in the journal Nature in 2016. Specifically, you have implemented a basic version of the AlphaGo algorithm for the coin game, combining deep reinforcement learning with conventional rule-based AI.\n",
    "\n",
    "In this chapter, you’ll expand this approach, creating an AlphaGo agent adaptable to various games.\n",
    "The AlphaGo agent you create features two main enhancements. Firstly, it will be versatile, capable of handling two games, Tic Tac Toe and Connect Four. This flexibility reduces code redundancy and simplifies the application of the AlphaGo algorithm to a broader range of games. Secondly, the agent’s game simulation strategy includes a choice between random moves and those suggested by the fast policy network. This decision involves a trade-off: the network’s moves provide more insight but require\n",
    "more processing time due to the complex neural network. In contrast, random moves accelerate game simulations, enabling more game rollouts within a given time frame and potentially smarter move selection in actual gameplay.\n",
    "\n",
    "Monte Carlo Tree Search (MCTS) remains the core of the agent’s decision process, involving selection, expansion, simulation, and backpropagation, as outlined in Chapter 8. However, now three deep neural networks, introduced in previous chapters, will enhance the tree search. During real games, a large number of simulations will start from the current game state. Each simulation involves choosing a child node for expansion, not based on the upper confidence bounds applied to trees (UCT) formula\n",
    "from Chapter 8, but on a weighted average of each node’s rollout value and its prior probability from the trained policy-gradient network. Additionally, players can choose between random moves or those from the fast policy network for rollouts. Instead of playing to a terminal state, the game state will be evaluated at a certain depth using the trained value networks, allowing more simulations within the time limit.\n",
    "\n",
    "You’ll evaluate the AlphaGo algorithm’s effectiveness in Tic Tac Toe. Against the perfect rule-based AI from Chapter 6, the AlphaGo agent consistently draws, indicating its ability to solve the game.\n",
    "Given Tic Tac Toe’s simplicity compared to Chess or Go, we’ll also explore an AlphaGo version without the value network, rolling out games to their end. Another variant will use random moves instead of those from the fast policy network. Both versions will be shown to effectively solve Tic Tac Toe, setting the stage for Chapter 20, where we implement AlphaZero in Tic Tac Toe, omitting the value network and relying solely on one policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a933d3",
   "metadata": {},
   "source": [
    "# 1. An AlphaGo Algorithm for Multiple Games\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44908b0",
   "metadata": {},
   "source": [
    "## 1.1. Functions to Select and Expand\n",
    "In the local module *ch17util*, we first define a *select()* function to select a child node to expand the game tree, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063509d0",
   "metadata": {},
   "source": [
    "```python\n",
    "def select(priors,env,results,weight):    \n",
    "    # weighted average of priors and rollout_value\n",
    "    scores={}\n",
    "    for k,v in results.items():\n",
    "        # rollout_value for each next move\n",
    "        if len(v)==0:\n",
    "            vi=0\n",
    "        else:\n",
    "            vi=sum(v)/len(v)\n",
    "        # scale the prior by (1+N(L))\n",
    "        prior=priors[0][k-1]/(1+len(v))\n",
    "        # calculate weighted average\n",
    "        scores[k]=weight*prior+(1-weight)*vi\n",
    "    # select child node based on the weighted average     \n",
    "    return max(scores,key=scores.get) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74702e59",
   "metadata": {},
   "source": [
    "```python\n",
    "# expand the game tree by taking a hypothetical move\n",
    "def expand(env,move):\n",
    "    env_copy=deepcopy(env)\n",
    "    state,reward,done,info=env_copy.step(move)\n",
    "    return env_copy, done, reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fac83",
   "metadata": {},
   "source": [
    "## 1.2 Functions to simulate and backpropagate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b3ade",
   "metadata": {},
   "source": [
    "```python\n",
    "def best_move_fast_net(env, fast_net):\n",
    "    # priors from the policy network\n",
    "    if env.turn==\"X\":\n",
    "        state = env.state.reshape(-1,3,3,1)\n",
    "        action_probs=fast_net(state)\n",
    "    elif env.turn==\"O\":\n",
    "        state = env.state.reshape(-1,3,3,1)\n",
    "        action_probs=fast_net(-state)\n",
    "    elif env.turn==\"red\":\n",
    "        state = env.state.reshape(-1,7,6,1)\n",
    "        action_probs=fast_net(state)\n",
    "    elif env.turn==\"yellow\":\n",
    "        state = env.state.reshape(-1,7,6,1)\n",
    "        action_probs=fast_net(-state)             \n",
    "    ps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        ps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(ps)\n",
    "    return np.random.choice(sorted(env.validinputs),\n",
    "        p=ps/ps.sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32135136",
   "metadata": {},
   "source": [
    "```python\n",
    "# roll out a game till terminal state or depth reached\n",
    "def simulate(env_copy,done,reward,depth,value_net,\n",
    "             fast_net, policy_rollout=True):\n",
    "    # if the game has already ended\n",
    "    if done==True:\n",
    "        return reward\n",
    "    # select moves based on fast policy network\n",
    "    for _ in range(depth):\n",
    "        if policy_rollout:\n",
    "            move=best_move_fast_net(env_copy, fast_net)\n",
    "        else:\n",
    "            move=env_copy.sample()\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        # if terminal state is reached, returns outcome\n",
    "        if done==True:\n",
    "            return reward\n",
    "    # depth reached but game not over, evaluate\n",
    "    if env_copy.turn==\"X\":\n",
    "        state=state.reshape(-1,3,3,1)\n",
    "        ps=value_net.predict(state,verbose=0)\n",
    "        # reward is prob(X win) - prob(O win)\n",
    "        reward=ps[0][1]-ps[0][2]        \n",
    "    elif env_copy.turn==\"O\":\n",
    "        state=state.reshape(-1,3,3,1)\n",
    "        ps=value_net.predict(-state,verbose=0)\n",
    "        # reward is prob(X win) - prob(O win)\n",
    "        reward=ps[0][2]-ps[0][1]     \n",
    "    elif env_copy.turn==\"red\":\n",
    "        state=state.reshape(-1,7,6,1)\n",
    "        ps=value_net.predict(state,verbose=0)\n",
    "        # reward is prob(red win) - prob(yellow win)\n",
    "        reward=ps[0][1]-ps[0][2]        \n",
    "    elif env_copy.turn==\"yellow\":\n",
    "        state=state.reshape(-1,7,6,1)\n",
    "        ps=value_net.predict(-state,verbose=0)\n",
    "        # reward is prob(red win) - prob(yellow win)\n",
    "        reward=ps[0][2]-ps[0][1]           \n",
    "    return reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8ab61",
   "metadata": {},
   "source": [
    "```python\n",
    "def backpropagate(env,move,reward,results):\n",
    "    # update results\n",
    "    if env.turn==\"X\" or env.turn==\"red\":\n",
    "        results[move].append(reward)\n",
    "    # if current player is player 2, multiply outcome with -1\n",
    "    if env.turn==\"O\" or env.turn==\"yellow\":\n",
    "        results[move].append(-reward)                  \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f81d44",
   "metadata": {},
   "source": [
    "## 1.3 An AlphaGo Agent for Tic Tac Toe and Connect Four\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f027c",
   "metadata": {},
   "source": [
    "```python\n",
    "def alphago(env,weight,depth,PG_net,value_net,\n",
    "        fast_net, policy_rollout=True,num_rollouts=100):\n",
    "    # if there is only one valid move left, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # get the prior from the PG policy network\n",
    "    if env.turn==\"X\" or env.turn==\"O\":\n",
    "        state = env.state.reshape(-1,9)\n",
    "        conv_state = state.reshape(-1,3,3,1)\n",
    "        if env.turn==\"X\":\n",
    "            priors = PG_net([state,conv_state])\n",
    "        elif env.turn==\"O\":\n",
    "            priors = PG_net([-state,-conv_state])  \n",
    "    if env.turn==\"red\" or env.turn==\"yellow\":\n",
    "        state = env.state.reshape(-1,42)\n",
    "        conv_state = state.reshape(-1,7,6,1)\n",
    "        if env.turn==\"red\":\n",
    "            priors = PG_net([state,conv_state])\n",
    "        elif env.turn==\"yellow\":\n",
    "            priors = PG_net([-state,-conv_state])          \n",
    "    # create a dictionary results\n",
    "    results={}\n",
    "    for move in env.validinputs:\n",
    "        results[move]=[]\n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        # select\n",
    "        move=select(priors,env,results,weight)\n",
    "        # expand\n",
    "        env_copy, done, reward=expand(env,move)\n",
    "        # simulate\n",
    "        reward=simulate(env_copy,done,reward,depth,value_net,\n",
    "                     fast_net, policy_rollout)\n",
    "        # backpropagate\n",
    "        results=backpropagate(env,move,reward,results)\n",
    "    # select the most visited child node\n",
    "    visits={k:len(v) for k,v in results.items()}\n",
    "    return max(visits,key=visits.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d70a2",
   "metadata": {},
   "source": [
    "# 2. Test the AlphaGo Agent in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "## 2.1. The Opponent in Tic Tac Toe Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab56e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch06util import MiniMax_ab\n",
    "\n",
    "def rule_based_AI(env):\n",
    "    move = MiniMax_ab(env)\n",
    "    return move "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fab8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the trained fast policy network from Chapter 10\n",
    "fast_net=keras.models.load_model(\"files/fast_ttt.h5\")\n",
    "# Load the policy gradient network from Chapter 14\n",
    "PG_net=keras.models.load_model(\"files/pg_ttt.h5\")\n",
    "# Load the trained value network from Chapter 14\n",
    "value_net=keras.models.load_model(\"files/value_ttt.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef56cd1",
   "metadata": {},
   "source": [
    "## 2.2. AlphaGo vs Rule-Based AI in Tic Tac Toe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f806366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "from utils.ch17util import alphago\n",
    "\n",
    "weight=0.75\n",
    "depth=5\n",
    "# initiate game environment\n",
    "env=ttt()\n",
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # AlphaGo moves first\n",
    "        action=alphago(env,weight,depth,PG_net,value_net,\n",
    "                fast_net, policy_rollout=True,num_rollouts=100)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"AlphaGo wins!\")\n",
    "            break      \n",
    "        # move recommended by rule-based AI\n",
    "        action=rule_based_AI(env)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            print(\"Rule-based AI wins!\")\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cfa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n",
      "The game is tied!\n"
     ]
    }
   ],
   "source": [
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # Rule-based AI moves first\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"Rule-based AI wins!\")\n",
    "            break      \n",
    "        # AlphaGo moves second\n",
    "        action=alphago(env,weight,depth,PG_net,value_net,\n",
    "                fast_net, policy_rollout=True,num_rollouts=100) \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            print(\"AlphaGo wins!\")\n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53dc8fc",
   "metadata": {},
   "source": [
    "# 3. Redundancy in AlphaGo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91bdca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=0.8\n",
    "depth=10\n",
    "# create a list to record game outcome\n",
    "results=[]\n",
    "# test ten games\n",
    "for i in range(20):\n",
    "    state=env.reset()  \n",
    "    if i%2==0:\n",
    "        # Ruled-based AI moves\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)          \n",
    "    while True:     \n",
    "        # AlphaGo moves \n",
    "        action=alphago(env,weight,depth,PG_net,value_net,\n",
    "                fast_net, policy_rollout=True,num_rollouts=100) \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            results.append(abs(reward))\n",
    "            break  \n",
    "        # Rule-based AI moves\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            results.append(-abs(reward))\n",
    "            break             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8974da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo won 0 games\n",
      "AlphaGo lost 0 games\n",
      "the game was tied 20 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times AlphaGo won\n",
    "wins=results.count(1)\n",
    "print(f\"AlphaGo won {wins} games\")\n",
    "# count how many times AlphaGo lost\n",
    "losses=results.count(-1)\n",
    "print(f\"AlphaGo lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game was tied {ties} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d97e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=0.9\n",
    "depth=10\n",
    "# create a list to record game outcome\n",
    "results=[]\n",
    "# test ten games\n",
    "for i in range(20):\n",
    "    state=env.reset()  \n",
    "    if i%2==0:\n",
    "        # Ruled-based AI moves\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)          \n",
    "    while True:     \n",
    "        # AlphaGo moves, setting policy_rollout=False \n",
    "        action=alphago(env,weight,depth,PG_net,value_net,\n",
    "                fast_net, policy_rollout=False,num_rollouts=100) \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            results.append(abs(reward))\n",
    "            break  \n",
    "        # Rule-based AI moves\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            results.append(-abs(reward))\n",
    "            break             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397463d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo won 0 games\n",
      "AlphaGo lost 0 games\n",
      "the game was tied 20 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times AlphaGo won\n",
    "wins=results.count(1)\n",
    "print(f\"AlphaGo won {wins} games\")\n",
    "# count how many times AlphaGo lost\n",
    "losses=results.count(-1)\n",
    "print(f\"AlphaGo lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game was tied {ties} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
