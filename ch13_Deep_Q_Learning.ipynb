{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 13: Deep Q-Learning\n",
    "\n",
    "In Chapter 12, you used tabular Q-learning to train a Q table. You then use the trained Q-table to successfully play the OpenAI Gym Frozen Lake game. \n",
    "\n",
    "In many situations, the number of possible scenarios is too large. Examples include Chess or the Go game: the number of possible board positions is astronomical. It’s impractical to create a Q table for these types of games for two reasons: First, the computer will not have enough memory to save and update a Q table with so many different rows (each row represents a different scenario); Second, it's impossible to calculate and update the correct Q values because the number of size of the Q-table is too large. \n",
    "\n",
    "That's when deep neural networks can help. Neural networks are function approximators and we’ll use a deep neural network to approximate the Q values. That’s the idea behind deep Q learning. \n",
    "\n",
    "This chapter will apply deep Q learning to a game in OpenAI Gym: the Cart Pole game. You’ll learn how to calculate the Q tables by using deep Q networks, so that you can tackle much more complicated games later in this book (such as Tic Tac Toe and Connect Four)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 13}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 13 in a subfolder /files/ch13. The code in the cell below will create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch13\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. The Cart Pole Game in OpenAI Gym\n",
    "As we discussed in Chapter 12, you need to install the *gym* library first. If you haven't already, install the OpenAI Gym library as follows:\n",
    "\n",
    "`pip install gym==0.15.7`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 1.1. Features of the Cart Pole Game "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "If you go to the official Cartpole game website https://gym.openai.com/envs/CartPole-v0/. \n",
    "\n",
    "The problem is considered solved as getting an average reward of 195 or above in 200 consecutive trials. \n",
    "\n",
    "The code in the cell below will get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    " \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()                    \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "\n",
    "We can also print out all possible actions and states of the game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052e7981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The action space in the Cart Pole game is Discrete(2)\n",
      "The state space in the Cart Pole game is Box(4,)\n"
     ]
    }
   ],
   "source": [
    "# Print out all possible actions in this game\n",
    "actions = env.action_space\n",
    "print(f\"The action space in the Cart Pole game is {actions}\")\n",
    "\n",
    "# Print out all possible states in this game\n",
    "states = env.observation_space\n",
    "print(f\"The state space in the Cart Pole game is {states}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "The action space in the Cart Pole game has two values: 0 and 1, with the following meanings:\n",
    "* 0: moving left\n",
    "* 1: moving right\n",
    "\n",
    "The state in the Cart Pole game is a collection of four values, with the following meanings:\n",
    "\n",
    "* The position of the cart, with values between -4.8 and 4.8; \n",
    "* The velociyt of the cart, with values between -4 and 4; \n",
    "* The angle of the pole, with values between -0.42 and 0.42;\n",
    "* The angular velocity of the pole, with values between -4 and 4; \n",
    "\n",
    "\n",
    "The agent earns a reward of 1 for every time that the pole stays upright. If the pole is more than 15 degrees from \n",
    " vertical or the cart moves more than 2.4 units from the center, the agent loses the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf2ef2",
   "metadata": {},
   "source": [
    "## 1.2. A Complete Cart Pole Game\n",
    "You can play a complete game as follows by using random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "629df3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[-0.00077322 -0.1786936   0.00362296  0.28238639] 1.0 False {}\n",
      "0\n",
      "[-0.00434709 -0.37386704  0.00927069  0.57620978] 1.0 False {}\n",
      "1\n",
      "[-0.01182443 -0.17887626  0.02079488  0.28646172] 1.0 False {}\n",
      "1\n",
      "[-0.01540195  0.01594305  0.02652412  0.00040919] 1.0 False {}\n",
      "0\n",
      "[-0.01508309 -0.17954906  0.0265323   0.30134138] 1.0 False {}\n",
      "0\n",
      "[-0.01867407 -0.37503893  0.03255913  0.60227256] 1.0 False {}\n",
      "1\n",
      "[-0.02617485 -0.18038716  0.04460458  0.32002035] 1.0 False {}\n",
      "0\n",
      "[-0.0297826  -0.37611503  0.05100499  0.62642954] 1.0 False {}\n",
      "1\n",
      "[-0.0373049  -0.18174075  0.06353358  0.35023625] 1.0 False {}\n",
      "1\n",
      "[-0.04093971  0.01242282  0.0705383   0.07824482] 1.0 False {}\n",
      "0\n",
      "[-0.04069126 -0.18363572  0.0721032   0.39232236] 1.0 False {}\n",
      "0\n",
      "[-0.04436397 -0.37970288  0.07994965  0.70683892] 1.0 False {}\n",
      "1\n",
      "[-0.05195803 -0.18577423  0.09408642  0.44035529] 1.0 False {}\n",
      "0\n",
      "[-0.05567351 -0.38209301  0.10289353  0.76115164] 1.0 False {}\n",
      "0\n",
      "[-0.06331537 -0.57847056  0.11811656  1.0843574 ] 1.0 False {}\n",
      "0\n",
      "[-0.07488478 -0.77493596  0.13980371  1.41164771] 1.0 False {}\n",
      "1\n",
      "[-0.0903835  -0.58179648  0.16803667  1.16573361] 1.0 False {}\n",
      "0\n",
      "[-0.10201943 -0.77865856  0.19135134  1.50603815] 1.0 False {}\n",
      "0\n",
      "[-0.1175926  -0.97551641  0.2214721   1.85185   ] 1.0 True {}\n"
     ]
    }
   ],
   "source": [
    "env.reset()   \n",
    "\n",
    "while True:\n",
    "    action = actions.sample()\n",
    "    print(action)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(new_state, reward, done, info)    \n",
    "    if done == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791cedf2",
   "metadata": {},
   "source": [
    "The above cell used several methods in the game environment:\n",
    "* The sample() method randomly selects an action from the action space. That is, it will return one of the values among {0, 1}. \n",
    "* The step() method is where the agent is interacting with the environment, and it takes the agent’s action as input. The output are four values: the new state, the reward, a variable *done* indicating whether the game has ended. The *info* variable provides some information about the game. \n",
    "* The render() method shows a diagram of the resulting state. \n",
    "\n",
    "The game loop is an infinite *while* loop. If the *done* variable returns a value *True*, the game ends, and we stop the infinite while loop.\n",
    "\n",
    "Note that since the actions are chosen randomly, when you run the script, you’ll most likely get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dbb5c",
   "metadata": {},
   "source": [
    "# 2. Deep Q-Learning in Cart Pole\n",
    "\n",
    "The input of the deep Q-learning model is the state of the game, just as in deep learning models. However, the output layers are different. In deep learning models, the output is the probability of winning. In contrast, in deep Q-learning, the number of neurons in the output layer is the same as the number of actions. The output are the Q values of the state-action pair. More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b7aaab",
   "metadata": {},
   "source": [
    "## 2.1. Create the Deep Q Network\n",
    "The model we'll use is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb8b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "input_shape = [4] \n",
    "num_actions = 2\n",
    "\n",
    "def create_q_model():\n",
    "    model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", \n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(num_actions)\n",
    "])\n",
    "    return model\n",
    "model = create_q_model()\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.00025)\n",
    "loss_fn = keras.losses.mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59747014",
   "metadata": {},
   "source": [
    "The neural network has one input layer, two hidden layers, and one output layer. The network is an approximation of the Q-table: it takes in the state, and returns two values. The two values are the Q-values for Q(s, action=1) and Q(s, action=2), respectively. Therefore, the agent should take the action with the higher Q value, as we did in Chapter 12 with tabular Q-learning. \n",
    "\n",
    "Note here we are using the Exponential Linear Unit (ELU) activation function instead of the usual ReLU activation function. This is due to the fact that in the Cart Pole game, all rewards are positive numbers. In such situations, ELU activation functions returns negative values for small values of inputs, and this allows the function to push the mean values closer to zero. Alternatively, you can use batch normalization layers to achieve simimar effects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9ad6c",
   "metadata": {},
   "source": [
    "## 2.2. Train the Deep Q Network\n",
    "The following script trains the deep Q network for the Cart Pole game. It stops until the average score is at least 195. That is, the cart pole needs to stay upright for at leat 195 consecutive time steps. \n",
    "\n",
    "First, we define a update_Q() function to train the model by selecting a batch from the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fc8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay and update model parameters\n",
    "def update_Q():\n",
    "    # select a batch from the buffer memory\n",
    "    samples = random.sample(memory,batch_size)\n",
    "    dones = []\n",
    "    frames = []\n",
    "    new_frames = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    for sample in samples:\n",
    "        frame, new_frame, action, reward, done = sample\n",
    "        frames.append(frame)\n",
    "        new_frames.append(new_frame)\n",
    "        actions.append(action)\n",
    "        if done==1.0 or done==True or done==1:done=1.0          \n",
    "        else: done=0.0    \n",
    "        dones.append(done)\n",
    "        rewards.append(reward)\n",
    "    frames=np.array(frames)\n",
    "    new_frames=np.array(new_frames)\n",
    "    dones=tf.convert_to_tensor(dones)\n",
    "    # update the Q table\n",
    "    preds = model.predict(new_frames, verbose=0)\n",
    "    Qs = rewards + gamma * tf.reduce_max(preds, axis=1)\n",
    "    # if done=1  reset Q to  -1; important\n",
    "    new_Qs = Qs * (1 - dones) - dones\n",
    "\n",
    "    # update model parameters\n",
    "    onehot = tf.one_hot(actions, num_actions)\n",
    "    with tf.GradientTape() as t:\n",
    "        Q_preds = model(frames)\n",
    "        # Calculate old Qs for the action taken\n",
    "        old_Qs = tf.reduce_sum(tf.multiply(Q_preds,onehot),axis=1)\n",
    "        # Calculate loss between new Qs and old Qs\n",
    "        loss = loss_fn(new_Qs, old_Qs)\n",
    "    # Update using backpropagation\n",
    "    gs = t.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gs,model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8443e",
   "metadata": {},
   "source": [
    "The update_Q() function selects 32 observations from the replay buffer and update the Q values. We create the replay buffer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b623e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# Discount factor for past rewards\n",
    "gamma = 0.95 \n",
    "# batch size\n",
    "batch_size = 32  \n",
    "\n",
    "# Create a replay buffer with a maximum length of 2000\n",
    "memory=deque(maxlen=2000)\n",
    "# Create a running rewards list with a length of 100\n",
    "running_rewards=deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0c3e2",
   "metadata": {},
   "source": [
    "The replay buffer has a maximum length of 2000 observations. Each time the function update_Q() is called, 32 observations are randomly selected from the buffer and used to update the deep Q network. The running_rewards list has a maximum value of 100 and it is used to keep track of the scores in the last 100 games. If the average score in these 100 games is above 195, we consider the model is trained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc06a1",
   "metadata": {},
   "source": [
    "Next, we simulate games to train the model until the model can keep the cart pole upright for 195 consecutive time steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03351421",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(1, 10001): \n",
    "    # reset state and episode reward before each episode\n",
    "    state = np.array(env.reset())\n",
    "    episode_reward = 0\n",
    "    for timestep in range(1, 201):\n",
    "        # Calculate current epsilon based on frame count\n",
    "        epsilon = max(1 - episode / 500, 0.05)\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if epsilon> np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            state_tensor = tf.convert_to_tensor(state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = tf.argmax(action_probs[0]).numpy()\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state_next = np.array(state_next)\n",
    "        episode_reward += reward\n",
    "        # Change done from True/False to 1.0 or 0.0\n",
    "        if done==True:\n",
    "            done=1.0  \n",
    "        else:\n",
    "            done=0.0\n",
    "        # Save actions and states in replay buffer\n",
    "        memory.append([state, state_next, action, reward, done])\n",
    "        # current state becomes the next state in next round\n",
    "        state = state_next\n",
    "        # Update Q once batch size is over 32\n",
    "        if len(memory) > batch_size:\n",
    "            update_Q()\n",
    "        if done==1.0 or done==1 or done==True:\n",
    "            running_rewards.append(episode_reward)\n",
    "            break\n",
    "    running_reward = np.mean(np.array(running_rewards))\n",
    "    if episode%20==0:\n",
    "        # Log details\n",
    "        template = \"running reward: {:.2f} at episode {}, \"\n",
    "        print(template.format(running_reward, episode ))\n",
    "    if running_reward>=195:\n",
    "        # Log details\n",
    "        template = \"running reward: {:.2f} at episode {}, \"\n",
    "        print(template.format(running_reward, episode))\n",
    "        # Periodically save the model\n",
    "        model.save(\"files/ch13/cartpole_deepQ.h5\")\n",
    "        print(f\"solved at episode {episode}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8542b",
   "metadata": {},
   "source": [
    "The model is considered trained if the averge score in the past 100 games is 195 or above, as stated by the OpenAI Gym rules. That's the criteria used in the training process. Once the goal is achieved, the training stops. \n",
    "\n",
    "The above program takes about an hour to run, depending on the speed of your computer. Here is the output from my computer\n",
    "\n",
    "```python\n",
    "...\n",
    "...\n",
    "running reward: 193.93 at episode 859, \n",
    "running reward: 194.24 at episode 860, \n",
    "running reward: 194.24 at episode 861, \n",
    "running reward: 194.37 at episode 862, \n",
    "running reward: 194.67 at episode 863, \n",
    "running reward: 194.75 at episode 864, \n",
    "running reward: 194.89 at episode 865, \n",
    "running reward: 195.20 at episode 866, \n",
    "Solved at episode 866!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab6eaf",
   "metadata": {},
   "source": [
    "# 3. Test the Deep Q Network\n",
    "\n",
    "Now that the model is trained, we can use it to play the OpenAI Gym Cart Pole game and see if it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ddcfe",
   "metadata": {},
   "source": [
    "## 3.1. Test One Game\n",
    "Next, we'll test one game, with the graphical rendering turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df12a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "score is 200\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "for i in range(1,201):\n",
    "    env.render()\n",
    "    # Use the trained model to predict the prob of winning \n",
    "    X_state = np.array(state).reshape(-1,4)\n",
    "    prediction = model.predict(X_state)\n",
    "    # pick the action with highest probability of winning\n",
    "    action = np.argmax(prediction)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    score += 1\n",
    "    if done == True:\n",
    "        print(f\"score is {score}\")\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6a8d3",
   "metadata": {},
   "source": [
    "The score is 200. So the cart pole stayed upright for all 200 time steps. The deep Q network really works!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794163",
   "metadata": {},
   "source": [
    "## 3.2. Test the Efficacy of the Deep Q Network\n",
    "Next, we play the game 100 times using the trained deep Q network and see how effective the trained deep Q network is on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "677a0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "def test_cartpole():\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    for i in range(1,201):\n",
    "        # Use the model to predict the prob of winning \n",
    "        X_state = np.array(state).reshape(-1,4)\n",
    "        prediction = model.predict(X_state)\n",
    "        # pick the action with highest prob of winning\n",
    "        action = np.argmax(prediction)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score += 1\n",
    "        if done == True:\n",
    "            break\n",
    "    return score\n",
    "\n",
    "#repeat the game 100 times and record all game outcomes\n",
    "results=[]        \n",
    "for x in range(100):\n",
    "    result=test_cartpole()\n",
    "    results.append(result)    \n",
    "\n",
    "#print out the average score\n",
    "average_score = np.array(results).mean()\n",
    "print(\"the average score is\", average_score)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea611d",
   "metadata": {},
   "source": [
    "the average score is 200.0\n",
    "\n",
    "So the trained deep Q network managed to make the cart pole stay upright for 200 consecutive time steps in every sigle game. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
