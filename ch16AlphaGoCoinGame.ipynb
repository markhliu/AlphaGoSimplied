{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 16: Implement AlphaGo in the Coin Game\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***“I thought AlphaGo was based on probability calculation and that it was merely a machine. But when I saw this move, I changed my mind. Surely, AlphaGo is creative.”***\n",
    "\n",
    "-- Lee Sedol, winner of 18 world Go titles\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The AlphaGo algorithm combines combines deep reinforcement learning (namely, the policy gradient method) with traditional rule-based AI (specifically, Monte Carlo Tree Search or MCTS) to generate intelligent game strategies in Go. Now that you have learned both deep reinforcement learning and MCTS, you’ll learn to combine them together as the DeepMind team did and apply the algorithm\n",
    "to the three games in this book: the coin game, Tic Tac Toe, and Connect Four.\n",
    "\n",
    "In this chapter, you’ll implement AlphaGo in the coin game. First, we’ll go over the AlphaGo algorithm and see how it brings various pieces together to create a powerful AI player. After that, you’ll apply the same logic to the coin game to create your very own AlphaGo agent.\n",
    "\n",
    "MCTS is the core of AlphaGo’s decision-making process. MCTS is used to find the most promising moves by building a search tree. Each node in the tree represents a game position, and branches represent possible moves. The search through this tree is guided by statistical analysis of the moves. AlphaGo also uses three deep neural networks - a fast policy network, a strengthened policy network through self-play deep reinforcement learning, and a value network. The fast policy network helps in narrowing down the selection of moves to consider in game rollouts. It’s trained on expert games and learns to predict their moves. This network is used to guide the tree search to more promising paths. The strengthened policy network is used to select child nodes in game rollouts so that the most valuable child nodes are selected to roll out games. The value network evaluates board positions and predicts the winner of the game from that position. It’s crucial for looking ahead and evaluating future\n",
    "positions without having to play out the entire game.\n",
    "\n",
    "AlphaGo’s success lies in the effective combination of traditional tree search methods with powerful machine-learning techniques. This allowed it to tackle the immense complexity of Go, a game with more possible positions than atoms in the observable universe.\n",
    "\n",
    "The three games we consider in this book are much simpler than the game of Go. Nonetheless, we’ll mimic the strategies used by the DeepMind team and implement the AlphaGo algorithm in these games. Along the way, you’ll pick valuable skills in both rule-based AI and cutting-edge developments in deep learning.\n",
    "\n",
    "To implement AlphaGo in the coin game, we’ll utilize the skills and the trained networks earlier in this book. Specifically, we’ll use the skills you learned about MCTS from Chapter 8. However, instead of rolling out games with random moves, you’ll roll out games by letting both players choose moves based on the fast policy network we trained in Chapter 9. More intelligent moves in rollouts lead to more informative game outcomes. Further, instead of playing out the entire game during rollouts, you’ll use the value network we trained in Chapter 13 to evaluate game states after playing out games after a fixed number of moves (that is, after a certain depth). Finally, to select a child node to roll out games in MCTS, instead of using the UCT formula from Chapter 8, you’ll use a weighted average of the rollout values and the prior probabilities recommended by the strengthened policy network from Chapter 13.\n",
    "\n",
    "After creating the AlphaGo agent in the coin game, you’ll test it against both random moves and the perfect rule-based AI player from Chapter 1. You’ll see that when moving second, the AlphaGo agent beats the ruled-based AI player in all ten games. When moving first, the AlphaGo agent wins all ten games against random moves. This shows that the AlphaGo algorithm is as strong as any possible game strategy we could have designed for the coin game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. The AlphaGo Architecture \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a933d3",
   "metadata": {},
   "source": [
    "# 2. AlphaGo in the Coin Game\n",
    "\n",
    "\n",
    "## 2.1. Select the Best Child Node and Expand the Game Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4ea2c",
   "metadata": {},
   "source": [
    "```python\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the trained fast policy network from Chapter 9\n",
    "fast_net=keras.models.load_model(\"files/fast_coin.h5\")\n",
    "# Load the strengthend strong net from Chapter 13\n",
    "PG_net=keras.models.load_model(\"files/PG_coin.h5\")\n",
    "# Load the trained value network from Chapter 13\n",
    "value_net=keras.models.load_model(\"files/value_coin.h5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0d349",
   "metadata": {},
   "source": [
    "```python\n",
    "def onehot_encoder(state):\n",
    "    onehot=np.zeros((1,22))\n",
    "    onehot[0,state]=1\n",
    "    return onehot\n",
    "\n",
    "def best_move_fast_net(env):\n",
    "    state = env.state\n",
    "    onehot_state = onehot_encoder(state)\n",
    "    action_probs = fast_net(onehot_state)\n",
    "    return np.random.choice([1,2],\n",
    "        p=np.squeeze(action_probs))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0b0e2",
   "metadata": {},
   "source": [
    "```python\n",
    "def select(priors,env,results,weight):    \n",
    "    # weighted average of priors and rollout_value\n",
    "    scores={}\n",
    "    for k,v in results.items():\n",
    "        # rollout_value for each next move\n",
    "        if len(v)==0:\n",
    "            vi=0\n",
    "        else:\n",
    "            vi=sum(v)/len(v)\n",
    "        # scale the prior by (1+N(L))\n",
    "        prior=priors[0][k-1]/(1+len(v))\n",
    "        # calculate weighted average\n",
    "        scores[k]=weight*prior+(1-weight)*vi\n",
    "    # select child node based on the weighted average     \n",
    "    return max(scores,key=scores.get)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74702e59",
   "metadata": {},
   "source": [
    "```python\n",
    "# expand the game tree by taking a hypothetical move\n",
    "def expand(env,move):\n",
    "    env_copy=deepcopy(env)\n",
    "    state,reward,done,info=env_copy.step(move)\n",
    "    return env_copy, done, reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4f9e0",
   "metadata": {},
   "source": [
    "## 2.2. Roll Out A Game and Backpropagate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33efb1c",
   "metadata": {},
   "source": [
    "```python\n",
    "# roll out a game till terminal state or depth reached\n",
    "def simulate(env_copy,done,reward,depth):\n",
    "    # if the game has already ended\n",
    "    if done==True:\n",
    "        return reward\n",
    "    # select moves based on fast policy network\n",
    "    for _ in range(depth):\n",
    "        move=best_move_fast_net(env_copy)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        # if terminal state is reached, returns outcome\n",
    "        if done==True:\n",
    "            return reward\n",
    "    # depth reached but game not over, evaluate\n",
    "    onehot_state=onehot_encoder(state)\n",
    "    # use the trained value network to evaluate\n",
    "    ps=value_net.predict(onehot_state,verbose=0)\n",
    "    # output is prob(1 wins)-prob(2 wins)\n",
    "    reward=ps[0][1]-ps[0][0]  \n",
    "    return reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff8ab61",
   "metadata": {},
   "source": [
    "```python\n",
    "def backpropagate(env,move,reward,results):\n",
    "    # if current player is Player 1, update results\n",
    "    if env.turn==1:\n",
    "        results[move].append(reward)\n",
    "    # if current player is Player 2, multiply outcome with -1\n",
    "    elif env.turn==2:\n",
    "        results[move].append(-reward)                  \n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f81d44",
   "metadata": {},
   "source": [
    "## 2.3 Create An AlphaGo Agent in the Coin Game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f027c",
   "metadata": {},
   "source": [
    "```python\n",
    "def alphago_coin(env,weight,depth,num_rollouts=100):\n",
    "    # if there is only one valid move left, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # get the prior from the PG policy network\n",
    "    priors = PG_net(onehot_encoder(env.state))    \n",
    "    # create a dictionary results\n",
    "    results={}\n",
    "    for move in env.validinputs:\n",
    "        results[move]=[]\n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        # select\n",
    "        move=select(priors,env,results,weight)\n",
    "        # expand\n",
    "        env_copy, done, reward=expand(env,move)\n",
    "        # simulate\n",
    "        reward=simulate(env_copy,done,reward,depth)\n",
    "        # backpropagate\n",
    "        results=backpropagate(env,move,reward,results)\n",
    "    # select the most visited child node\n",
    "    visits={k:len(v) for k,v in results.items()}\n",
    "    return max(visits,key=visits.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5d06",
   "metadata": {},
   "source": [
    "# 3. Test  the AlphaGo Algorithm in the Coin Game\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "## 3.1. When the AlphaGo Agent Moves Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab56e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rule_based_AI(env):\n",
    "    if env.state%3 != 0:\n",
    "        move = env.state%3\n",
    "    else:\n",
    "        move = random.choice([1,2])\n",
    "    return move "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f806366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "from utils.ch16util import alphago_coin\n",
    "\n",
    "# initiate game environment\n",
    "env=coin_game()\n",
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # The rule-based AI player moves first\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            # print out the winner\n",
    "            print(\"Rule-based AI wins!\")\n",
    "            break        \n",
    "        # The AlphaGo agent moves second\n",
    "        action=alphago_coin(env,0.9,10,num_rollouts=100)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            # print out the winner\n",
    "            print(\"AlphaGo wins!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2649a",
   "metadata": {},
   "source": [
    "## 3.2. Against Random Moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13cfa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "# test ten games against random moves\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # The AlphaGo agent moves first\n",
    "        action=alphago_coin(env,0.9,10,num_rollouts=100)    \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            print(\"AlphaGo wins!\")\n",
    "            break\n",
    "        # The random player moves second\n",
    "        action=random.choice(env.validinputs)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            print(\"AlphaGo loses!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53dc8fc",
   "metadata": {},
   "source": [
    "# 4. Redundancy in the AlphaGo Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91bdca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "from utils.ch16util import alphago_coin\n",
    "\n",
    "# initiate game environment\n",
    "env=coin_game()\n",
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # The rule-based AI player moves first\n",
    "        action=rule_based_AI(env)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            # print out the winner\n",
    "            print(\"Without value network, rule-based AI wins!\")\n",
    "            break        \n",
    "        # The AlphaGo agent moves second\n",
    "        action=alphago_coin(env,0.8,22,num_rollouts=100)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            # print out the winner\n",
    "            print(\"Without value network, AlphaGo wins!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d97e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n",
      "Without value network, AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "# test ten games against random moves\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # The AlphaGo agent moves first\n",
    "        action=alphago_coin(env,0.8,22,num_rollouts=100)    \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            print(\"Without value network, AlphaGo wins!\")\n",
    "            break\n",
    "        # The random player moves second\n",
    "        action=random.choice(env.validinputs)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            print(\"Without value network, AlphaGo loses!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
