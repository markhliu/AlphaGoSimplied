{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 16: An Actor-Critic Agent in Tic Tac Toe\n",
    "\n",
    "We discussed how the actor-critic method works in Chapter 15 and applied the method to the coin game. In this chapter, we'll apply the actor-critic method to Tic Tac Toe. We'll address four issues that are associated with a moderately complicated game such as Tic Tac Toe: First, how to handle illegal moves; Second, how to add a convoluational layer to the model so that we can use the spatial features on the game board to train game strategies; Third, how to use vectorization to speed up training; Fourth, how to encode the board when the game state is player dependent.  \n",
    "\n",
    "To teach the model not to play illegal moves, we'll assign a negative reward each time the agent makes an illegal move. However, we need to address the credit assignment problem when assigning negative rewards: only the illegal move itself should be punished, not the moves before the illegal move. This is different from a sequence of moves that lead to a win or loss for the agent. \n",
    "\n",
    "To use the spatial features of the game board, we add a convolutional layer as the input. As a result, the model now has two input networks: one dense network and one convolutional network. The model has also two output networks: a policy network (the actor) and a value network (the critic). \n",
    "\n",
    "Training such a complicted network is computationally costly. To speed up training, you'll learn to use vectorization to make the program more efficient. \n",
    "\n",
    "In the Coin Game, the game state is player-independent in the following sense. For example, if four coins are left in the pile, the player with the turn will use the same strategy no matter whether the current player is Player 1 or Player 2. However, in Tic Tac Toe and Connect Four, the game state is player-dependent since wether Player 1 wins or Player 2 wins depends on the game pieces on the board. Therefore, in order to use the same network to train both players, we need to multiply the board by -1 when it's Player 2's turn. Specifically, we'll create one single model for Player X and Player O in Tic Tac Toe. Since a cell is represented by 1 if it's occupied by Player X, -1 if occupied by Player O, and 0 if empty, we'll multiply the game board by -1 if and only if it's Player O's turn. This way, when the board is fed into the deep neural network, it's encoded consistently: 1 indicates a cell occupied by the current player, -1 a cell occupied by the opponent, and 0 an empty cell. By making such a change, the same model can be used to predict moves for both Player X and Player O."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 16}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 16 in a subfolder /files/ch16. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch16\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. How to Handle Illegal Moves?\n",
    "In the coin game we discussed in Chapter 15, the AC agent makes one of the two moves each step: taking one or two coins from the pile. The number of legal moves is the same in every step: two. This is not the case in more complicated games such as Tic Tac Toe or Connect Four. \n",
    "\n",
    "In Tic Tac Toe, the first player has nine legal moves in the first step: cells 1 though 9. After that, it's Player O's turn and there are only eight legal moves left: if Player X has taken cell 1 in the first move, for example, cell 1 is not a legal move for Palyer O any more. How to train the model so that the agent avoids illegal moves? The obvious answer is to assign a negative reward each time the agent makes an illege move. However, this relates to the credit assignmnet problem in reinforcement learning and we need to handle the negative rewards with a little more care. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c437e8",
   "metadata": {},
   "source": [
    "## 1.1. The Credit Assignment Problem in Reinforcement Learning\n",
    "In reinforcement learning, agents learn the best actions through the feedback from rewards (which can be either positive or negative). However, rewards are sparse and delayed and the agent needs to figure out how to assign proper credits to a sequence of actions that lead to a good or a bad outcome. The discounted rewards are usually the solution. \n",
    "\n",
    "To illustrate the point, let's assume that in a Tic Tae Toe game, the sequence of moves are the following:\n",
    "* Player X takes cell 1;\n",
    "* Player O takes cell 2;\n",
    "* Player X takes cell 5;\n",
    "* Player O takes cell 3;\n",
    "* Player X takes cell 9\n",
    "At this point, Player X has connected three in a row diagonally in cells 1, 5, and 9. The undiscounted rewards for the three steps made by Player X are 0, 0, and 1, respectively. However, the third step alone didn't win the game, so we should give credits to the first two moves as well by discounting rewards. Assuming the discount rate is 0.9, the discounted rewards to the three steps made by Player X are 0.81, 0.9, and 1, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa92c7",
   "metadata": {},
   "source": [
    "## 1.2. The Credit Assignment Problem in Illegal Moves\n",
    "To train the AC agent to avoid illegal moves, we can assign negative rewards to the move each time the agent makes an illegal move. However, we should make sure that only the illegal move gets the blame, not the moves before it. \n",
    "\n",
    "To make the point clear, let's consider two different cases. In case 1, the following sequence of moves are made by the two players:\n",
    "* Player X takes cell 1;\n",
    "* Player O takes cell 5;\n",
    "* Player X takes cell 2;\n",
    "* Player O takes cell 3;\n",
    "* Player X takes cell 4;\n",
    "* Player O takes cell 7.\n",
    "\n",
    "At this point, Player O has connected three in a row diagonally in cells 3, 5, and 7. The rewards for the three steps made by Player X are 0, 0, and -1, respectively. Assuming a discount rate of 0.9, the discounted rewards to the three steps made by Player X are -0.81, -0.9, and -1, respectively. This is approporiate because the third step alone didn't lose the game for Player X: the first two moves should get punished as well. \n",
    "\n",
    "Now let's consider a separate situation: \n",
    "* Player X takes cell 1;\n",
    "* Player O takes cell 7;\n",
    "* Player X takes cell 5;\n",
    "* Player O takes cell 9;\n",
    "* Player X takes cell 7;\n",
    "* Player X takes cell 6;\n",
    "* Player O takes cell 8.\n",
    "\n",
    "Player X made an illegal move in cell 7 since cell 7 was already taken by Player O. So we should assign a reward of -1 to the move. However, the two previous moves made by Player X (1 and 5), should not get punished for the illegal move. We should treat the illegal move by X (7) and the three bad moves (1, 5, and 6) that lead to a loss differently as follows:\n",
    "* Move 7 should be assigned a reward of -1; this reward is final and should not be discounted. \n",
    "* Moves 1, 5, and 6 should be assigned undiscounted rewards of 0, 0, and -1, respectively; Assuming the discount rate is 0.9, the discounted rewards to the three steps made by Player X (1, 5, and 6) should be 0.81, 0.9, and 1, respectively. \n",
    "\n",
    "To summarize, we should assign a negative reward each time the AC agent makes an illegal move, but we don't discount this reward; further, other moves made by the same player should not be get punished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e3444",
   "metadata": {},
   "source": [
    "# 2. Use Vectorization to Speed Up Training\n",
    "Traing deep neural networks can be extremely computationally costly. For example, to train the AlphaGo to play against Lee Sedol, DeepMind used 1920 CPUs and 280 GPUs, according to an article by the Economist (Showdown, The Economist, March 12, 2016). \n",
    "\n",
    "Our games are not as complicated as the Go game. However, training these simple games can still be computationally costly without the help of supercomputing facilities. Therefore, making our code more efficient will certainly reduce the computational cost. In particular, vectorization makes computations more efficient. Below we use an example to show how vectorization saves time. Assume we want to add up all natural numbers between 1 and 10,000,000. You can do this using a loop as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e06acc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum is 50000005000000\n",
      "the calculation took 0.9935817718505859 seconds to finish\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "total=0\n",
    "for i in range(1,10000001):\n",
    "    total += i\n",
    "print(f\"the sum is {total}\")  \n",
    "end=time.time()\n",
    "print(f\"the calculation took {end-start} seconds to finish\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6daeb57",
   "metadata": {},
   "source": [
    "The calculation took roughly 1 second to finish. Note that the number you get is likely to be different since the speed depends on the hardware configurations of your computer. \n",
    "\n",
    "Now, let's use vectorization to do the same calculation and see how long it takes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8966c710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum is 50000005000000.0\n",
      "the calculation took 0.04994463920593262 seconds to finish\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "start=time.time()\n",
    "constant=np.ones(10000000)\n",
    "numbers=np.arange(10000000)+1\n",
    "total=total=np.matmul(constant,numbers)\n",
    "print(f\"the sum is {total}\")  \n",
    "end=time.time()\n",
    "print(f\"the calculation took {end-start} seconds to finish\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2afc09a",
   "metadata": {},
   "source": [
    "Here we have created two vectors: *constant* is a vector with values 1 everywhere, and *numbers* is a vector with values from 1 to 10,000,000. We use the *matmul()* method in NumPy to conduct matrix multiplication to calculate the sum. While we get the same correct answer, it took only about 0.05 seconds. In this example, we have saved 95% of the computational time. \n",
    "\n",
    "We'll use similar techniques when we train game strategies for the rest of the book: we'll use vectors instead of loops to speed up training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "# 3. Use Actor-Critic to Play Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "You have learned how to play the Coin Game using the actor critic approach in Chapter 15. Specifically, you have adjusted the model parameters proportional to the product of the gradients and teh advanrtage. In this section, we'll apply the same technique to Tic Tac Toe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "## 3.1. Combine Two Networks into One\n",
    "Below, we'll create two separate input networks first: one is a dense layer with 9 neurons and the other network has a convolutional layer that can detect the spatial features on the Tic Tac Toe board. We'll then combine the two networks together using the *concat()* method in TensorFlow. We'll use the combined network as the input to our actor-critic model. The model has two output network, similar to the two output networks we used in Chapter 15: a policy network for the actor and a value network for the critic. \n",
    "\n",
    "The code in teh cell below creates such a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_inputs = 9\n",
    "num_actions = 9\n",
    "num_hidden = 32\n",
    "# The convolutional input layer\n",
    "conv_inputs = layers.Input(shape=(3,3,1))\n",
    "conv = layers.Conv2D(filters=64, kernel_size=(3,3),padding=\"same\",\n",
    "     input_shape=(3,3,1), activation=\"relu\")(conv_inputs)\n",
    "flat = layers.Flatten()(conv)\n",
    "# The dense input layer\n",
    "inputs = layers.Input(shape=(9,))\n",
    "# Combine the two into a single input layer\n",
    "two_inputs = tf.concat([flat,inputs],axis=1)\n",
    "common = layers.Dense(128, activation=\"relu\")(two_inputs)\n",
    "# Policy output network (actor)\n",
    "action = layers.Dense(32, activation=\"relu\")(common)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(action)\n",
    "# Value output network (critic)\n",
    "critic = layers.Dense(32, activation=\"relu\")(common)\n",
    "critic = layers.Dense(1, activation=\"tanh\")(critic)\n",
    "# The final model\n",
    "model = keras.Model(inputs=[inputs, conv_inputs],\\\n",
    "                    outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1def0",
   "metadata": {},
   "source": [
    "The model has two input networks. The first one takes in the game board as a three by three image and use a convolutional layer to extract the spatial features on the game board. The output is then falttened into a one-dimensional vector. The second input network is a dense layer with nine neurons. We combine the two input layers togehter by using the *concat()* method from TensorFlow. The combined layer is then fed into the common layer. We also create two output networks, a policy network and a value network. \n",
    "\n",
    "Below, we specify the optimizer and the loss function we use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.0005)\n",
    "loss_func=keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398c385",
   "metadata": {},
   "source": [
    "The optimizer is Adam with a learning rate of 0.0005. The loss function is the mean absolute error loss function, which punishes outliner less compared to other loss functions such as Huber or mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 3.2. Simulate Tic Tac Toe Games\n",
    "We have shown in Chapters 4 to 6 that the minimax algorithm can solve the Tic Tac Toe game so no other algorithm can beat it. We'll train the actor-critic agent by playing against the minimax agent that we developed in Chapter 6. However, we make a couple of minor modifications to the minimax agent to speed up the game: first, whenever there is one move left, the agent takes the move without searching; second, if cell 5 (the middle cell in the middle row) is empty, the player occupies cell 5. By making these two changes, we greatly speed up the game while preserving the effectiveness of the minimax algorithm. \n",
    "\n",
    "For that purpose, we define the function *minimax_ttt()* in the local module ch16util. It's the same as the minimax algorithm with alpha-beta pruning we defined in Chapter 6 except the following four lines of code:\n",
    "\n",
    "```python\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    if 5 in env.validinputs:\n",
    "        return 5\n",
    "```\n",
    "\n",
    "Below, we import the functiuons *minimax_ttt()* as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b08faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch16util import minimax_ttt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfd813",
   "metadata": {},
   "source": [
    "To train Player X, we define a *playing_X()* function. We let the minimax agent move second to train the AC agent Player X. The function simulates a full game as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "\n",
    "# allow a maximum of 50 steps per game\n",
    "max_steps=50\n",
    "env=ttt()\n",
    "\n",
    "def playing_X():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,9,)\n",
    "        conv_state = state.reshape(-1,3,3,1)\n",
    "        # Predict action probabilities and future rewards\n",
    "        action_probs, critic_value = model([state,conv_state])\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "            #episode_reward += -1 \n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(reward)\n",
    "                episode_reward += reward \n",
    "                break\n",
    "            else:\n",
    "                state,reward,done,_=env.step(minimax_ttt(env))\n",
    "                rewards_history.append(reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            wrongmoves_history,rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387db1da",
   "metadata": {},
   "source": [
    "The playing_X() function simulates a full game and records all the intermediate steps made by the AC agent. The function returns five values: \n",
    "* a list action_probs_history with the natural logorithm of the recommended probability of the action taken by the agent from the policy network; \n",
    "* a list critic_value_history with the estimated future rewards from the value network; \n",
    "* rewards_history with the rewards to each action taken by the agent in the game;\n",
    "* wroingmoves_history with the rewards to each action associated with wrong moves;\n",
    "* a number episode_reward showing the total rewards to the agent during the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1121b",
   "metadata": {},
   "source": [
    "The playing_X() function calculates the gradients and rewards from the game. Since in reinforcement learning, actions affect not only current period rewards, but also future rewards, we therefore use discounted rewards to assign credits properly to legal moves. The following function reflects such disounting of rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def discount_rs(r,wrong):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        if wrong[i]==0:  \n",
    "            running_add = gamma*running_add + r[i]\n",
    "            discounted_rs[i] = running_add  \n",
    "    return discounted_rs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4af74",
   "metadata": {},
   "source": [
    "The function takes two lists, *r* and *wrong*, as inputs, where *r* are the rewards related to legal moves while *wrong* are rewards related to illegal moves. We discount *r* but not *wrong*. The output is the properly disounted reward in each time period when a legal move is made. Note that if an illegal move is made in a time period, the reward is 0 from the output. We'll add in the reward of -1 for illgal moves later at training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb80cf1a",
   "metadata": {},
   "source": [
    "Similarly, to train Player O, we define a *playing_O()* function. The function simulates a full game, with the minimax agent as the first player, and the actor-critic (AC) agent as the second player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65b28969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_O():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    state, reward, done, _ = env.step(minimax_ttt(env))\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,9,)\n",
    "        conv_state = state.reshape(-1,3,3,1)\n",
    "        # Predict action probabilities and future rewards\n",
    "        action_probs, critic_value = model([-state,-conv_state])\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "            #episode_reward += -1 \n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(-reward)\n",
    "                episode_reward += -reward \n",
    "                break\n",
    "            else:\n",
    "                state,reward,done,_=env.step(minimax_ttt(env))\n",
    "                rewards_history.append(-reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += -reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            wrongmoves_history,rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c214a32",
   "metadata": {},
   "source": [
    "The *playing_O()* function simulates a full game and records all the intermediate steps made by the AC agent. The function returns five values as those from the function *playing_X()*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340a29b",
   "metadata": {},
   "source": [
    "Most importantly, we use [-state,-conv_state] instead of [state,conv_state] when we feed the game board to the model. This is because the game board has value 1 in a cell if Player X has placed a game piece in it, -1 if Player O has placed a game piece in it. To use the same network for both palyers, we multiply the input by -1 so that the model treat 1 as occupied by the current player and -1 as occupied by the opponent. \n",
    "\n",
    "The second most important thinng is to change the rewards. Instead of using 1 and -1 to denote a win by the Player X and Player O, respectively, we'll use a reward of 1 to denote the current player has won and a reward of -1 to denote that the current player has lost. We accomplish this by use -reward instead of reward in the above cell. Therefore, after Player O makes a move, the reward is now 1 if Player O wins and -1 if Player O loses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 3.3. Train Players X and O\n",
    "Instead of updating model parameters after one episode, we update after a certain number of episodes to make the model stable. Here we update parameters every ten games to train Player X, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10     \n",
    "def create_batch_X(batch_size):\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,cvs,wms,rs,er = playing_X()\n",
    "        # rewards are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        critic_value_history += cvs\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er)        \n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cd09207",
   "metadata": {},
   "source": [
    "Note above we discount the rewards associated with wins and losses, but not the rewards associated with wrong moves, due to the credit assignment problem we discussed earlier. We then combined the discounted rewards with punishments associated with wrong moves and put the combined values in the list rewards_history.  \n",
    "\n",
    "Similarly, we also simulate ten games to train Player O, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36881c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_O(batch_size):\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,cvs,wms,rs,er = playing_O()\n",
    "        # reward related to legal moves are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        critic_value_history += cvs\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er)        \n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef0e21",
   "metadata": {},
   "source": [
    "We'll train the model and update the parameters until the average episode reward to the AC agent in the last 100000 games reaches -0.005. An average score of -0.005 against a perfect player is very high. This means when the AC agent plays against the minimax agent, the percentage of games that the AC agent loses is less than 0.5%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48963d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "running_rewards=deque(maxlen=100000)\n",
    "gamma = 0.95  \n",
    "episode_count = 0\n",
    "batches=0\n",
    "# Train the model\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        if batches%2==0:\n",
    "            action_probs_history,critic_value_history,\\\n",
    "    rewards_history,episode_rewards=create_batch_X(batch_size)\n",
    "        else:\n",
    "            action_probs_history,critic_value_history,\\\n",
    "    rewards_history,episode_rewards=create_batch_O(batch_size)            \n",
    "                     \n",
    "        # Calculating loss values to update our network        \n",
    "        tfdif=tf.convert_to_tensor(rewards_history,\\\n",
    "    dtype=tf.float32)-tf.convert_to_tensor(critic_value_history,\\\n",
    "                                           dtype=tf.float32)\n",
    "        alosses=-tf.multiply(tf.convert_to_tensor(\\\n",
    "          action_probs_history,dtype=tf.float32),tfdif)\n",
    "\n",
    "        closs=loss_func(tf.convert_to_tensor(rewards_history,\\\n",
    "     dtype=tf.float32),tf.convert_to_tensor(critic_value_history,\\\n",
    "                                            dtype=tf.float32))\n",
    "        # Backpropagation\n",
    "        loss_value = tf.reduce_sum(alosses) + closs\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,\\\n",
    "                                  model.trainable_variables))\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    batches += 1\n",
    "    for r in episode_rewards:\n",
    "        running_rewards.append(r)\n",
    "    running_reward=np.mean(np.array(running_rewards)) \n",
    "    # print out progress\n",
    "    if episode_count % 1000 == 0:\n",
    "        template = \"running reward: {:.6f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))   \n",
    "    # Stop if the game is solved\n",
    "    if running_reward > -0.005 and episode_count>100:  \n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "model.save(\"ac_ttt.h5\")           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c598877",
   "metadata": {},
   "source": [
    "It takes about five hours to train the model. The exact amount of time depends on your computer hardward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa7b9a",
   "metadata": {},
   "source": [
    "# 4. Play Tic Tac Toe with the Actor-Critic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2033eed1",
   "metadata": {},
   "source": [
    "We'll use the trained model to play against the rule-based AI. We'll first use the stochastic policy: select the moves according to the probabilities produced by the policy network. Later we'll also use the deterministic policy in which the AC agent chooses the move with the highest probability recommended by the policy network in the trained model. \n",
    "\n",
    "We'll use the trained model to play against the think-three-steps-ahead rule-based AI. We'll first let the AC agent move first and play 100 games and see how many games the AC agent can win. After that, we'll let the think-three-steps-ahead AI move first and again play 100 gains and see the game outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edf2ef2",
   "metadata": {},
   "source": [
    "## 4.1. When the Actor-Critic Agent Moves First "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f9c06",
   "metadata": {},
   "source": [
    "We'll let the AC agent play 100 games against the rule-based AI. We define a ACplayer() function that produces the best move based on the trained AC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ca4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_think3 import X_think3,O_think3\n",
    "import numpy as np\n",
    "\n",
    "model=keras.models.load_model(\"files/ch16/AC_ttt.h5\")\n",
    "\n",
    "def ACplayer(env): \n",
    "    state = env.state.reshape(-1,9,)\n",
    "    conv_state = state.reshape(-1,3,3,1)\n",
    "    if env.turn==\"X\":\n",
    "        action_probs, critic_value = model([state,conv_state])\n",
    "    else:\n",
    "        action_probs, critic_value = model([-state,-conv_state])        \n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return np.random.choice(sorted(env.validinputs),p=ps)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ee2555",
   "metadata": {},
   "source": [
    "The AC player chooses the next action based on the probability distrinution recommended by the policy network from the trained model. Here we exclude invalid actions so only legal moves are selected.\n",
    "\n",
    "We also define a deterministic AC player, who chooses the best move with 100% probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a1a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACdeterministic(env): \n",
    "    state = env.state.reshape(-1,9,)\n",
    "    conv_state = state.reshape(-1,3,3,1)\n",
    "    if env.turn==\"X\":\n",
    "        action_probs, critic_value = model([state,conv_state])\n",
    "    else:\n",
    "        action_probs, critic_value = model([-state,-conv_state])        \n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return sorted(env.validinputs)[np.argmax(ps)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57d90a",
   "metadata": {},
   "source": [
    "We create a list *results*. In each game, the AC agent moves first and the rule-based AI moves second. If the AC agent wins, we add an outcome 1 to *results*. If the rule-based AI wins, we add an outcome of -1 to *results*. If the game is tied, we add a 0 to *results*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c3f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env=ttt()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action=ACplayer(env) \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            if reward!=0:\n",
    "                # record 1 if AC agent won\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0) \n",
    "            break\n",
    "        # select action based on policy network\n",
    "        action=O_think3(env) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record -1 if rule-based AI player won\n",
    "            results.append(-1)            \n",
    "            break             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea611d",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdcf55ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 72 games\n",
      "The AC player has lost 0 games\n",
      "There are 28 tied games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")              \n",
    "# Print out the number of tie games\n",
    "ties=results.count(0)\n",
    "print(f\"There are {ties} tied games\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4666b",
   "metadata": {},
   "source": [
    "The AC agent has won 72 times and never lost. The remaining 28 games are tied. The results show that the AC agent is much better than the look-three-steps-ahead rule-based AI when it has the first-mover's advantage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a5d9a",
   "metadata": {},
   "source": [
    "## 4.2. When the Actor-Critic Agent Moves Second "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61f3be",
   "metadata": {},
   "source": [
    "Next, we test if the AC agent can beat the think-three-steps-ahead rule-based AI when it moves second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6a481",
   "metadata": {},
   "source": [
    "We again create an empty list *results*. We simulate 100 games. In each game, the rule-based AI moves first and the AC agent moves second. If the AC agent wins, we add an outcome 1 to *results*. If the rule-based AI wins, we add an outcome of -1 to *results*. If the game is tied, we add a 0 to *results*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6aef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env = ttt()\n",
    "    state=env.reset()     \n",
    "    while True:         \n",
    "        action = X_think3(env)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            if reward!=0:\n",
    "                # record -1 if rule-based AI player won\n",
    "                results.append(-1)\n",
    "            else:\n",
    "                results.append(0) \n",
    "            break\n",
    "        # select action based on policy network\n",
    "        action=ACdeterministic(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # record 1 if AC agent won\n",
    "            results.append(1)            \n",
    "            break             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01541f",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "352f637b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 9 games\n",
      "The AC player has lost 0 games\n",
      "There are 91 tied games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")              \n",
    "# Print out the number of tie games\n",
    "ties=results.count(0)\n",
    "print(f\"There are {ties} tied games\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780455e9",
   "metadata": {},
   "source": [
    "Most games are tied. The AC agent has won 9 times and never lost. The results show that the AC agent is unbeatable.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
