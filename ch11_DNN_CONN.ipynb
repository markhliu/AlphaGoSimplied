{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 11: Deep Learning Game Strategies\n",
    "\n",
    "In this chapter, you’ll continue with what you learned in Chapter 10 and apply deep learning to another game: Connect Four. \n",
    "\n",
    "You'll use similated games as input data to feed into a deep neural network. The neural network consists of an input layer, some hidden layers, and an output layer. The output layer has three neurons, representing the three possible game outcomes: a win, a loss, or a tie. Essentially we are conducting a multi-category classification problem. The neural network we create includes both dense layers and a convolutional layer. You'll learn to treat the Connect Four game board as a two-dimensional image and extract spatial features from the board (four game pieces in a row horizontally, vertically, or diagonally) and associate these features with game outcome. After the model is trained, you'll use it to design game strategies to play Connect Four. At each step of the game, you'll look at all possible next moves. The model predicts the probability of winning the game with each hypothetical move. You'll pick the move with the highest probability of winning the game for the current player.\n",
    "\n",
    "Finally, you'll test the game strategies against the rule-based AI players and see how strong the deep learning game strategies are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 11}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 11 in a subfolder /files/ch11. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch11\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "## 1. Deep Learning Game Strateies in Connect Four\n",
    "In this section, you’ll learn how to use deep neural network to train intelligent game strategies for Connect Four. In particular, you’ll use the convolutional neural network that you used in image classification to train the game strategy. By treating the game board as a two-dimensional graph instead of a one-dimensional vector, you’ll greatly improve the intelligence of your game strategies.\n",
    "\n",
    "You’ll learn how to prepare data to train the model, how to interpret the prediction from the model. How to use the prediction to play games, and how to check the efficacy of your strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60737d21",
   "metadata": {},
   "source": [
    "## 1.1. Summarize the Deep Learning Game Strategy\n",
    "Here is a summary of what we’ll do to train the game strategy:\n",
    "\n",
    "1.\tWe’ll let two computer players automatically play a game with random moves, and record the whole game history. The game history will contain all the game board positions from the very first move to the very last move.\n",
    "2.\tWe then associate each board position with a game outcome (win, tie, or lose). The game board position is similar to features X in our image classification problem, and the outcome is similar to labels y in our classification problem.\n",
    "3.\tWe’ll simulate 1,000,000 games. By using the histories of the games and the corresponding outcomes as Xs and ys, we feed the data into a Deep Neural Networks model. After the training is done, we have a trained model.\n",
    "4.\tWe can now use the trained model to play a game. At each move of the game, we look at all possible next moves, and feed the hypothetical game board into the pretained model. The model will tell you the probabilities of win, lose, and tie.\n",
    "5.\tYou select the move that the model predicts with the highest chance of winning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce5672b",
   "metadata": {},
   "source": [
    "## 1.2. Simulate Connect Four Games\n",
    "You’ll learn how to generate data to train the DNN. The logic is as follows: you’ll generate 100,000 games in which both players use random moves. You’ll then record the board positions of all intermediate steps and the eventual outcomes of each board position (win, lose, or tie). \n",
    "\n",
    "First, let's simulate one game. The code in the cell below accomplishes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fca1e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0]]),\n",
      " array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]]),\n",
      " array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0]]),\n",
      " array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0]]),\n",
      " array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  1,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  1, -1,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  1,  0,  0,  0],\n",
      "       [ 1,  1, -1,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  1,  0,  0,  0],\n",
      "       [ 1,  1, -1,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  1,  0,  0,  0],\n",
      "       [ 1,  1, -1,  1,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]]),\n",
      " array([[-1,  1,  1,  0,  0,  0],\n",
      "       [ 1,  1, -1,  1,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1, -1,  0,  0]]),\n",
      " array([[-1,  1,  1,  0,  0,  0],\n",
      "       [ 1,  1, -1,  1,  1,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1, -1,  0,  0]]),\n",
      " array([[-1,  1,  1, -1,  0,  0],\n",
      "       [ 1,  1, -1,  1,  1,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [-1,  1,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1, -1,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1, -1,  0,  0]])]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "\n",
    "# Define the one_game() function\n",
    "def one_game():\n",
    "    history = []\n",
    "    state=env.reset()   \n",
    "    while True:   \n",
    "        action = random.choice(env.validinputs)  \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        history.append(np.array(new_state).reshape(7,6))\n",
    "        if done:\n",
    "            break\n",
    "    return history, reward\n",
    "\n",
    "# Simulate one game and print out results\n",
    "history, outcome = one_game()\n",
    "pprint(history)\n",
    "pprint(outcome)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43138ed7",
   "metadata": {},
   "source": [
    "Note here we convert the game board to a 7 by 6 array so it's easy for you to see the positions of the game pieces. \n",
    "\n",
    "Now let's simulate 100,000 games and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2dee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the game 100000 times and record all games\n",
    "results = []        \n",
    "for x in range(100000):\n",
    "    history, outcome = one_game()\n",
    "    # Associate each board with the game outcome\n",
    "    for board in history:\n",
    "        results.append((outcome, board))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f4f38",
   "metadata": {},
   "source": [
    "Now let's save the data on your computer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c6eed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1,\n",
      "  array([[0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 0,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1,  0,  0,  0]])),\n",
      " (1,\n",
      "  array([[ 0,  0,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  0,  0,  0,  0],\n",
      "       [-1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 1,  0,  0,  0,  0,  0],\n",
      "       [ 1, -1,  1, -1,  0,  0]]))]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('files/ch11/games_conn100K.p', 'wb') as fp:\n",
    "    pickle.dump(results,fp)\n",
    "# read the data and print out the first 10 observations       \n",
    "with open('files/ch11/games_conn100K.p', 'rb') as fp:\n",
    "    games = pickle.load(fp)\n",
    "pprint(games[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af9260",
   "metadata": {},
   "source": [
    "Each observation has two values. The first is the outcome, in the form of -1, 0, or 1. The second is the game board position as a 7 by 6 numpy array. The data seem to have been stored correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d244530",
   "metadata": {},
   "source": [
    "We have the data we need. You’ll learn how to train the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 2. A Deep Neural Network for Connect Four\n",
    "\n",
    "We'll use Keras to create a deep neural network to train game strategies in Connect Four. Compared to the neural network we used in Chapter 10 to train game stategies for Tic Tac Toe, only a few small changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87043d9",
   "metadata": {},
   "source": [
    "## 2.1. Create the Connect Four Model\n",
    "The model we created below has one convolutional layer and several dense layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=128, \n",
    "kernel_size=(4,4),padding=\"same\",activation=\"relu\",\n",
    "                 input_shape=(7,6,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26be1",
   "metadata": {},
   "source": [
    "We first use a convoluttional layer with 128 filters. The kernel size is 4 by 4. This is different from teh kernel size we used in chapter 10. Since in the connecgt four gaem, we need four pieces in a row to win the game, so we use a four by four kernel to scan over the game board to identify spatial patterns. In particular, the kernetl will identify four piecdes in a row and assocate them wti hte game outcome. \n",
    "\n",
    "We then flatten the output form the convolutional layer to a vector and feed it to two hidden dense layers with 64 neurons each. The output layer has three neurons, representing there possible game outcomes: a win, a tie, or a loss. The softmax activation ensures that the proabilities add up to 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "## 2.2. Train the Deep Learning Model for Connect Four\n",
    "We'll train the deep neural network we created in the last section. We first preprocess the data so that we can feed them into the mocdel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bc75d",
   "metadata": {},
   "source": [
    "The outcome data is a variable with three possible values: -1, 0, and 1. We'll convert them into one-hot variables so that the deep neural network can process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e2ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "labels=[0,1,-1]\n",
    "one_hot=tf.keras.utils.to_categorical(labels,3)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43472ce9",
   "metadata": {},
   "source": [
    "In the example above, we have three labels: 0, 1, and -1. They represent a tie, a win for Player X, and a loss for Player X (i.e., a win for Player O).\n",
    "\n",
    "We can use the *to_categorical()* method in TensorFlow to change them into one-hot variables. The second argument in the *to_categorical()* method, 3, indicates the depth of the one-hot variable. This means each one-hot variable will be a vector with a length of 3, with value 1 in one position and 0\n",
    "in all others.\n",
    "\n",
    "A tie, which has an initial label of 0, now becomes a one-hot label: a 3-value\n",
    "vector [1, 0, 0]. The first value (i.e., index 0) is turned on as 1, and the other two values are turned off as 0. Similarly, a win for Player X, which has a label of 1 originally, now becomes a one-hot label of [0, 1, 0]. The second value (i.e., index 1) is turned on as 1, and the rest are turned off as 0. By the same logic, a loss for Player X, with an original value of -1, is now represented by\n",
    "[0, 0, 1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b0cf8",
   "metadata": {},
   "source": [
    "Next, we load up the simulated game data and convert them into Xs and ys so that we can feed them into the deep neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cfc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/ch11/games_conn100K.p', 'rb') as fp:\n",
    "    tttgames=pickle.load(fp)\n",
    "\n",
    "boards = []\n",
    "outcomes = []\n",
    "for game in tttgames:\n",
    "    boards.append(game[1])\n",
    "    outcomes.append(game[0])\n",
    "\n",
    "X = np.array(boards).reshape((-1, 7, 6, 1))\n",
    "# one_hot encoder, three outcomes: -1, 0, and 1\n",
    "y = tf.keras.utils.to_categorical(outcomes, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea807c",
   "metadata": {},
   "source": [
    "Next, we train the model for 100 epochs and save the model on the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fc541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 100 epochs\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "model.save('files/ch11/trained_conn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4185711",
   "metadata": {},
   "source": [
    "It takes several hours to train the model since we have close to a million observations. The trained model is saved on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "## 3. Use the Trained Model to Play Connect Four\n",
    "Next, we’ll use the strategy to play a game. \n",
    "\n",
    "The first player will use the best move from the trained model. The second player will randomly select a move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a00ae",
   "metadata": {},
   "source": [
    "## 3.1. Best Moves Based on the Trained Model\n",
    "First, we'll define a *best_move_red()* function for the red player. The function will go over each move hypothetically, and use the trained deep neural network to predict the probability of the red player winning the game. The function returns the move with the highest chance of the red player winning.\n",
    "\n",
    "What the function does is as follows:\n",
    "1.\tLook at the current board.\n",
    "2.\tLook at all possible next moves, and add a move to the current board to form a hypothetical board.\n",
    "3.\tUse the pretained model to predict the chance of the red player winning with the hypothetical board.\n",
    "4.\tChoose the move that produces the highest chance of winning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2efd4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_red(env):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome=-2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1,7,6,1)\n",
    "        prediction=reload.predict(state, verbose=0)\n",
    "        # output is prob(red wins) - prob(yellow wins)\n",
    "        win_lose_dif=prediction[0][1]-prediction[0][2]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c112990",
   "metadata": {},
   "source": [
    "Similarly, we'll define a *best_move_yellow()* function for Player 2. The function will go over each move hypothetically, and use the trained deep neural network to predict the probability of the yellow player winning the game. The function returns the move with the highest chance of winning for Player 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c142967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_yellow(env):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome=-2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1,7,6,1)\n",
    "        prediction=reload.predict(state, verbose=0)\n",
    "        # output is prob(yellow wins) - prob(red wins)\n",
    "        win_lose_dif=prediction[0][2]-prediction[0][1]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fe8eb",
   "metadata": {},
   "source": [
    "## 3.2. Play A Game with the Trained Model\n",
    "Now let's use the best move functions to choose moves for the red player and play a game against random moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21f51838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player red has chosen action=4\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "Player yellow has chosen action=7\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0 -1]]\n",
      "Player red has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  0  0 -1]]\n",
      "Player yellow has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0]\n",
      " [ 0  0  1  1  0  0 -1]]\n",
      "Player red has chosen action=5\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  0  0  0  0]\n",
      " [ 0  0  1  1  1  0 -1]]\n",
      "Player yellow has chosen action=5\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  0 -1  0  0]\n",
      " [ 0  0  1  1  1  0 -1]]\n",
      "Player red has chosen action=2\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  0 -1  0  0]\n",
      " [ 0  1  1  1  1  0 -1]]\n",
      "Player red has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "reload=tf.keras.models.load_model('files/ch11/trained_conn.h5')\n",
    "\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "while True:\n",
    "    # Use the best_move() function to select the next move\n",
    "    action = best_move_red(env)\n",
    "    print(f\"Player red has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player red has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break   \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==-1:\n",
    "            print(f\"Player yellow has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8442847",
   "metadata": {},
   "source": [
    "The red player has connect four horizontally and won the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece520",
   "metadata": {},
   "source": [
    "# 4. Test the Efficacy of the DNN Model\n",
    "Next, we’ll test how often the DNN trained game strategy wins against a player who makes random moves. \n",
    "The following script does that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e5801",
   "metadata": {},
   "source": [
    "## 5.1. Deep Learning versus Random Moves\n",
    "We'll see how the deep learning game strategy fairs against a random-move agent. We simulate 100 games. If the deep learning agent wins, we record an outcome of 1. Otherwise, we record an outcome of -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d392cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        if env.turn==\"red\":\n",
    "            action = best_move_red(env) \n",
    "        else:\n",
    "            action = best_move_yellow(env)    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the DL agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = random.choice(env.validinputs)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the DL agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "Among 50 games, the deep learning agent moves. In the remaining 50 games, the random-move agent goes first. This way, no player has a first-mover's advantage. We first create an empty list *results*. Whenever the deep learning agent wins, we append a value of 1 to the list. Otherwise we add an element of -1 to the list.\n",
    "\n",
    "Next, we count how many times the deep learning agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f806366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the deep learning agent has won 100 games\n",
      "the deep learning agent has lost 0 games\n",
      "the game has tied 0 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the deep learning agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the deep learning agent has lost {losses} games\")         \n",
    "# count how many times the game ties\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d519042",
   "metadata": {},
   "source": [
    "The deep learning agent wins all 100 games. So the deep learning game strategy works really well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400a560",
   "metadata": {},
   "source": [
    "## 4.2. Deep Learning versus Think-Three-Steps-Ahead AI\n",
    "Next, we see how the deep learning agent fairs against the think-three-steps-ahead AI agent that we developed in Chapter 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e56833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import conn_think3\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=conn_think3(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        if env.turn==\"red\":\n",
    "            action = best_move_red(env) \n",
    "        else:\n",
    "            action = best_move_yellow(env)    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the deep learning agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = action=conn_think3(env)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the deep learning agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbac9de",
   "metadata": {},
   "source": [
    "We test 100 games and in 50 of them, we let the think-three-steps-ahead agent go first. In the other 50 games, the deep learning agent moves first. We record game outcomes in a list results. If the deep learning agent wins, we record an outcome of 1 in the list results. If the deep learning agent loses, we record an outcome of -1. If the game is tied, we record an outcome of 0.\n",
    "\n",
    "Next, we check how many times the deep learning agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "980c2b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the deep learning agent has won 84 games\n",
      "the deep learning agent has lost 16 games\n",
      "the game has tied 0 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the deep learning agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the deep learning agent has lost {losses} games\")         \n",
    "# count how many times the game ties\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52acf2e0",
   "metadata": {},
   "source": [
    "Results show that the deep learning agent has won 84 games and lost 16 games out of 100 games. So the deep learning game strategy works really well and seems to be better than a think-three-steps-ahead agent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
