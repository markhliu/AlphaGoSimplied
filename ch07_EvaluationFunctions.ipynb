{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 7: Position Evaluation Functions in MiniMax\n",
    "\n",
    "With depth pruning and alpha-beta pruning to reduce the time it needs to make a move, the minimax algorithm can produce fairly powerful agents with the advancements in hardwares. For example, \"the 1997 version of Deep Blue searched between 100 million and 200 million positions per second, depending on the type of position. The system could search to a depth of between six and eight pairs of moves—one white, one black—to a maximum of 20 or even more pairs in some situations.” accordingly to an article in Scientific America by Larry Greenemeier in 2017. \n",
    "\n",
    "What made Deep Blue even more powerful is the position evaluation function it used. In Chapter 6, we assume that the game is tied when the number of depth is reached and the game is not over. In many real world games, however, even when the game is not over, we usually have a good estimate on the final outcome of the game based on heuristics. For example, in Chess, we can count the value of each game piece. Whichever side has a higher value of pieces tends to win. \n",
    "\n",
    "In this chapter, we introduce the concept of position evaluation functions for Connect Four. We show that these evaluation functions make the minimax agent much stronger. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 7}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 7 in a subfolder /files/ch07. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch07\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. What Are Position Evaluation Functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "Position evaluation functions estimate the likelihood of the game outcome. We normalize it to a range between -1 and 1, where -1 means the second player wins for sure, and 1 means the first player wins for sure. A value fo 0 indicates that the game is most likely to be tied. A value of 0.5, for example, indicates that player 1 is likely to win. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. A Model to Predict Outcome in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "We'll add in a position evaluation function in Connect Four to show how it works. The position evaluation function we use are generated by a deep neural network later in Chapter 13 later in this book. For now, all you need to know is that it takes a game board (a vector with 42 values of 1s and 0s) as the input and generates a value between -1 and 1. \n",
    "\n",
    "Download the file DNN_conn.h5 from the book's GitHub page and save it in the folder /Desktop/ai/files/ch07/ on your computer. After that, we load the model using Keras as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1337aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model=tf.keras.models.load_model('files/ch07/DNN_conn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f0581",
   "metadata": {},
   "source": [
    "If you feed a game board to the model, the output has three numbers in it: the probability of tying the game, the probabiltiy that Player 1 wins, and the proability that Player 2 wins. Dowload the file ch07util.py from the book's GitHub repository and place it in the folder /Desktop/ai/utils/ on your computer. In it, we define a predictions() function to generate game outcome probabilities based on the game state: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a76d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(env,model):\n",
    "    # obtain the current state, reshape it      \n",
    "    state=env.state.reshape(-1,7,6,1)\n",
    "    # make predictions\n",
    "    probabilities=model.predict(state,verbose=0)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddc025",
   "metadata": {},
   "source": [
    "We obtain the current state of the game and reshape it to a 7 by 6 two-dimensional game board and feed it to the model. The model has a convolutional layer in it. The output layer has three numbers in it: the probability of tying the game, the probabiltiy that Player 1 wins, and the proability that Player 2 wins. \n",
    "\n",
    "We can design game strategies based on these predictions. Below, we play a game by using the model's predictions to make moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 1.2. Play A Game with the Model Predictions\n",
    "To have a better understanding of the model, we can play a Connect Four game based on the model predictions. \n",
    "\n",
    "Specifically, we'll make a hypothetical move and ask the model to tell us the probability of winning versus losing. We then select the move with the highest probability of winning the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc0f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "Player red, what's your move?4\n",
      "Player red chose column 4\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "{1: -0.0876, 2: -0.0547, 3: 0.0292, 4: 0.0772, 5: 0.0626, 6: -0.0005, 7: -0.0454}\n",
      "Player yellow chose column 4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]]\n",
      "Player red, what's your move?3\n",
      "Player red chose column 3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]]\n",
      "{1: -0.0779, 2: 0.0444, 3: 0.0219, 4: 0.0734, 5: 0.2601, 6: 0.0734, 7: -0.073}\n",
      "Player yellow chose column 5\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1 -1  0  0]]\n",
      "Player red, what's your move?6\n",
      "Player red chose column 6\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1 -1  1  0]]\n",
      "{1: 0.2648, 2: 0.3158, 3: 0.453, 4: 0.5039, 5: 0.4253, 6: 0.4137, 7: 0.22}\n",
      "Player yellow chose column 4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1 -1  1  0]]\n",
      "Player red, what's your move?7\n",
      "Player red chose column 7\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1 -1  1  1]]\n",
      "{1: 0.5212, 2: 0.5325, 3: 0.6456, 4: 0.7368, 5: 0.6346, 6: 0.6002, 7: 0.5543}\n",
      "Player yellow chose column 4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1 -1  1  1]]\n",
      "Player red, what's your move?2\n",
      "Player red chose column 2\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  1  1  1 -1  1  1]]\n",
      "{1: 0.7848, 2: 0.4456, 3: 0.4897, 4: 1.0, 5: 0.4884, 6: 0.4572, 7: 0.4519}\n",
      "Player yellow chose column 4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  1  1  1 -1  1  1]]\n",
      "Player yellow won!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "from utils.ch07util import predictions\n",
    "from copy import deepcopy\n",
    "\n",
    "env=conn()\n",
    "state=env.reset()  \n",
    "print(f\"the current state is \\n{state.T[::-1]}\") \n",
    "while True:\n",
    "    action = int(input(\"Player red, what's your move?\")) \n",
    "    print(f\"Player red chose column {action}\")\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\") \n",
    "    if done: \n",
    "        print(\"Player red won!\")\n",
    "        break \n",
    "    # player yellow use the model to predict\n",
    "    values={}\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move\n",
    "        env_copy=deepcopy(env)\n",
    "        s,r,d,_=env_copy.step(m)\n",
    "        probabilities=predictions(env_copy,model)\n",
    "        # value function is prob(O wins)-prob(X wins)\n",
    "        value=probabilities[0][2]-probabilities[0][1]\n",
    "        # add value to the dictionary\n",
    "        values[m]=round(value,4)\n",
    "    # print out valuations for all moves\n",
    "    print(values)    \n",
    "    # choose the move with the highest evaluation    \n",
    "    action = max(values,key=values.get) \n",
    "    print(f\"Player yellow chose column {action}\")\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done: \n",
    "        if reward==-1:\n",
    "            print(\"Player yellow won!\")\n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfde6",
   "metadata": {},
   "source": [
    "At each step, the model calculates the probabilty of Player 1 and Player 2 winning the game if a hypothetical move is made. Player yellow then selects the move with the higest value of Prob(Player 2 wins) -  Prob(Player 1 wins). The strategy generates fairly good moves and wins the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "## 1.3. Position Evaluation in Connect Four\n",
    "In the file ch07util.py, you can also see the following position_eval() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b78e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_eval(env,model):\n",
    "    probs=predictions(env,model)\n",
    "    prob_player1_win=probs[0][1]\n",
    "    prob_player2_win=probs[0][2]\n",
    "    if env.turn==\"red\":\n",
    "        evaluation=prob_player1_win-prob_player2_win\n",
    "    elif env.turn==\"yellow\":\n",
    "        evaluation=prob_player2_win-prob_player1_win\n",
    "    return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720ea70",
   "metadata": {},
   "source": [
    "We obtain the current state of the game and reshape it to a 7 by 6 two-dimensional game board and feed it to the model. The model has a convolutional layer in it. The output layer has three numbers in it: the probability of tying the game, the probabiltiy that Player 1 wins, and the proability that Player 2 wins. \n",
    "\n",
    "If the current player is red, the evaluation is the probability of Player 1 winning minus the proability of Player 2 winning. Note that the evaluation is a value between -1 and 1. Similarly, if the current player is yellow, the evaluation is the probability of Player 2 winning minus the proability of Player 2 winning. \n",
    "\n",
    "Once we have this evaluation function, we can add it to the minimax algorithm with depth pruning and alpha-beta pruning in Connect Four. This will make the algorithm more powerful because the evaluation function provides a more accurate valuation of the game state, instead of assuming the valuation is zero as long as the game has not ended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe6000",
   "metadata": {},
   "source": [
    "# 2. Position Evaluation in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1417875",
   "metadata": {},
   "source": [
    "We'll modify the minimax algorithm with alpha-beta pruning and depth pruning from Chapter 6. The difference is that once the algorithm reaches a depth of 0, it evaluates the position based on the position evaluation functions we defined in the last section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c0ce",
   "metadata": {},
   "source": [
    "## 2.1. The eval_payoff_conn() Function\n",
    "We'll define a eval_payoff_conn() function. The function keeps track of the best outcomes so far for the red and yellow players and call them alpha and beta, respectively. Whenever the condition alpha>-beta or beta>-alpha is met, the algorithm stops searching the current branch. That is, we use alpha-beta pruning here. Second, there is a depth argument in the function and if the depth reaches 0, the function uses the position evaluation function to assess the game board. \n",
    "\n",
    "The eval_payoff_conn() function is defined as follows. It's saved in the file ch07util.py that you just downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b94f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_payoff_conn(env,model,reward,done,depth,alpha,beta):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # If the maximum depth is reached, assume tie game\n",
    "    if depth==0:\n",
    "        return position_eval(env,model)    \n",
    "    if alpha==None:\n",
    "        alpha=-2\n",
    "    if beta==None:\n",
    "        beta=-2\n",
    "    if env.turn==\"red\":\n",
    "        best_payoff = alpha\n",
    "    if env.turn==\"yellow\":\n",
    "        best_payoff = beta         \n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=eval_payoff_conn(env_copy,model,\\\n",
    "                             reward,done,depth-1,alpha,beta)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff > best_payoff:        \n",
    "            best_payoff = my_payoff\n",
    "            if env.turn==\"red\":\n",
    "                alpha=best_payoff\n",
    "            if env.turn==\"yellow\":\n",
    "                beta=best_payoff       \n",
    "        if alpha>=-beta:\n",
    "            break        \n",
    "    return best_payoff               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b208ef",
   "metadata": {},
   "source": [
    "Next, we'll design a minimax algorithm with the position evaluation function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cead4",
   "metadata": {},
   "source": [
    "## 2.2. The minimax_conn_eval() Function\n",
    "We also define a minimax_conn_eval() function to produce the best move for the minimax agent. The function is similar to the minimax_conn() function we used in Chapter 6. However, instead of assuming the game is tied once the search algorithm reaches a depth of 0, the function uses the position evaluation function to assess the game board. \n",
    "\n",
    "The minimax_conn_eval() function is defined as follows. It's saved in the file ch07util.py that you just downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ef98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_conn_eval(env,model,depth=3,evaluation):\n",
    "    values={} \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=eval_payoff_conn(env_copy,\\\n",
    "                             model,reward,done,depth,-2,-2)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        values[m]=my_payoff\n",
    "    # pick the move with the highest value       \n",
    "    best_move=max(values,key=values.get)\n",
    "    return best_move    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bfefc",
   "metadata": {},
   "source": [
    "With this, we have created a minimax algorithm with position evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d4e3",
   "metadata": {},
   "source": [
    "## 2.3. Minimax with Position Evaluations\n",
    "Next, we maually test a game with the minimax agent with the position evaluation function. We let the minimax agent play first and the human player move second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20703e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "What's your move, player yellow?3\n",
      "Player yellow chose cell 3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "What's your move, player yellow?4\n",
      "Player yellow chose cell 4\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]\n",
      " [ 0  0 -1  1  0  0  0]]\n",
      "What's your move, player yellow?5\n",
      "Player yellow chose cell 5\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]\n",
      " [ 0  0 -1  1 -1  0  0]]\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  1  0  0]\n",
      " [ 0  0 -1  1 -1  0  0]]\n",
      "What's your move, player yellow?2\n",
      "Player yellow chose cell 2\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  1  0  0]\n",
      " [ 0 -1 -1  1 -1  0  0]]\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  1  1  1  1  0  0]\n",
      " [ 0 -1 -1  1 -1  0  0]]\n",
      "Player red won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch07util import minimax_conn_eval\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    action = minimax_conn_eval(env,model)   \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done: \n",
    "        print(\"Player red won!\")\n",
    "        break    \n",
    "    action = int(input(\"What's your move, player yellow?\")) \n",
    "    print(f\"Player yellow chose cell {action}\")\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done: \n",
    "        if reward==-1:\n",
    "            print(\"Player yellow won!\")\n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a7d1a",
   "metadata": {},
   "source": [
    "The agent has won. It takes longer for the agent to make a move, but the minimax agent is much more sophisticated than the naive minimax agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df6cd7",
   "metadata": {},
   "source": [
    "# 3. Effectiveness of Position Evaluations\n",
    "Next, we test the performance improvements of the minimax agent due to the use of the position evaluation function. We play ten games. In five games, the minimax agent with position evaluation moves first. In the other five games, the naive minimax agent (who assumes that the game is tied when the depth reaches 0) moves first. This way, no agent has a first-mover's advantage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c3d78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch07util import minimax_conn_eval\n",
    "from utils.ch06util import minimax_conn\n",
    "\n",
    "results=[]\n",
    "for i in range(10):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=minimax_conn(env,depth=3)    \n",
    "        state,reward,done,_=env.step(action)\n",
    "    while True:\n",
    "        action=minimax_conn_eval(env,model,depth=3) \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done: \n",
    "            if reward!=0:\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "            break \n",
    "        action=minimax_conn(env,depth=3) \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done: \n",
    "            if reward!=0:\n",
    "                results.append(-1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "            break             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fa1bb",
   "metadata": {},
   "source": [
    "We create a list *results* to record game outcome. If the minimax agent with position evaluation wins, we record a value of 1 in the list *results*. If the minimax agent with position evaluation loses, we record a value of -1 in the list *results*. If the game is tied, we record a value of 0. \n",
    "\n",
    "Next, we count how many times the minimax agent with position evaluation has won and how many times the agent has lost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimax agent with evaluation has won 10 games\n",
      "the minimax agent with evaluation has lost 0 games\n",
      "the game has tied 0 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times minimax with evaluation won\n",
    "wins=results.count(1)\n",
    "print(f\"the minimax agent with evaluation has won {wins} games\")\n",
    "# count how many times minimax with evaluation lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the minimax agent with evaluation has lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game has tied {ties} times\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907880a",
   "metadata": {},
   "source": [
    "The above results show that the minimax agent with the evaluation function has won all ten games against the naive minimax agent. This indicates that the evaluation function has greatly improved the effectiveness of the minimax algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
