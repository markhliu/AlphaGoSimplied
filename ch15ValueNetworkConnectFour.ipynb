{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 15: A Value Network in Connect Four\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***“One neural network — known as the “policy network” — selects the next move to play. The other neural network — the “value network” — predicts the winner of the game.”***\n",
    "\n",
    "-- Demis Hassabis, CEO and Co-Founder, DeepMind \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Chapter 14, we applied the policy gradient method to Tic Tac Toe. Specifically, you learned how to handle illegal moves and use vectorization to speed up training. After several hours of training, the policy gradient agent becomes as effective as the strong policy agent that we trained in Chapter 10: it ties all games when playing against perfect moves.\n",
    "\n",
    "In this chapter, you’ll apply the policy gradient method to Connect Four. You’ll handle illegal moves the same way you did in Chapter 14: assigning a reward of −1 every time the agent makes an illegal move. You’ll also use vectorization to speed up training. You’ll create one single model to train both the red player and the yellow player in Connect Four. We’ll train the model as follows: in the first ten games, we’ll let the policy gradient agent move first. The strong policy agent acts as the opponent. We’ll then collect information such as the natural logarithms of the predicted probabilities and discounted rewards to update model parameters (i.e., train the model). In the second ten games, the policy gradient agent moves second and selects actions based on recommendations from the policy network in the model to play against the strong policy agent. We’ll also collect information from gameplays to train the model. We’ll alternate between training the red player and training the yellow player after every ten games. After the policy gradient agent beats or ties the strong policy agent at least 90% of the time, we stop the training process.\n",
    "\n",
    "We also use the game experience data from the above training process to train a value network: the network will predict the game outcome based on the board position. You also learn how to use the trained value network to design a game strategy in Connect Four. Since the strong policy agent we trained in Chapter 11 doesn’t make perfect moves, the trained value network generates better game strategies than the strong policy agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "# 1. The Policy Gradient Method in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "## 1.1. Create the Policy Gradient Model in Connect Four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_inputs=42\n",
    "num_actions=7\n",
    "# The convolutional input layer\n",
    "conv_inputs=layers.Input(shape=(7,6,1))\n",
    "conv=layers.Conv2D(filters=128, kernel_size=(4,4),padding=\"same\",\n",
    "     input_shape=(7,6,1), activation=\"relu\")(conv_inputs)\n",
    "flat=layers.Flatten()(conv)\n",
    "# The dense input layer\n",
    "inputs = layers.Input(shape=(42,))\n",
    "# Combine the two into a single input layer\n",
    "two_inputs = tf.concat([flat,inputs],axis=1)\n",
    "# hidden layers\n",
    "common = layers.Dense(256, activation=\"relu\")(two_inputs)\n",
    "common = layers.Dense(64, activation=\"relu\")(common)\n",
    "# Policy output network\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "# The final model\n",
    "model = keras.Model(inputs=[inputs, conv_inputs],\\\n",
    "                    outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.00025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 1.2. Use the Strong Policy Agent as the Opponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "663bc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the trained strong policy model as opponent\n",
    "trained=keras.models.load_model('files/policy_conn.h5')\n",
    "def opponent(env):\n",
    "    state = env.state.reshape(-1,7,6,1)\n",
    "    if env.turn==\"red\":\n",
    "        action_probs=trained(state)\n",
    "    else:\n",
    "        action_probs=trained(-state) \n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return np.random.choice(sorted(env.validinputs),p=ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "\n",
    "# allow a maximum of 50 steps per game\n",
    "max_steps=50\n",
    "env=conn()\n",
    "\n",
    "def playing_red():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    states=[]\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        conv_state = state.reshape(-1,7,6,1)\n",
    "        # Predict action probabilities and future rewards\n",
    "        action_probs = model([state,conv_state])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "            #episode_reward += -1 \n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            states.append(state)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(reward)\n",
    "                episode_reward += reward \n",
    "                break\n",
    "            else:\n",
    "                state,reward,done,_=env.step(opponent(env))\n",
    "                rewards_history.append(reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,\\\n",
    "            wrongmoves_history,rewards_history, \\\n",
    "                episode_reward,states,reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rs(r,wrong):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        if wrong[i]==0:  \n",
    "            running_add = gamma*running_add + r[i]\n",
    "            discounted_rs[i] = running_add  \n",
    "    return discounted_rs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b28969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_yellow():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    states=[]\n",
    "    state, reward, done, _ = env.step(opponent(env))\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        conv_state = state.reshape(-1,7,6,1)\n",
    "        # Predict action probabilities and future rewards\n",
    "        action_probs = model([-state,-conv_state])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "            #episode_reward += -1 \n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            states.append(-state)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(-reward)\n",
    "                episode_reward += -reward \n",
    "                break\n",
    "            else:\n",
    "                state,reward,done,_=env.step(opponent(env))\n",
    "                rewards_history.append(-reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += -reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,\\\n",
    "            wrongmoves_history,rewards_history, \\\n",
    "                episode_reward,states,-reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 1.3. Train the Red and Yellow Players Interatively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10     \n",
    "allstates=[]\n",
    "alloutcome=[]\n",
    "def create_batch_red(batch_size):\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,wms,rs,er,ss,outcome = playing_red()\n",
    "        # rewards are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er)  \n",
    "        # record game history for the next section\n",
    "        allstates.append(ss)\n",
    "        alloutcome.append(outcome)\n",
    "    return action_probs_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36881c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_yellow(batch_size):\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,wms,rs,er,ss,outcome = playing_yellow()\n",
    "        # reward related to legal moves are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er) \n",
    "        # record game history for the next section\n",
    "        allstates.append(ss)\n",
    "        alloutcome.append(outcome)\n",
    "    return action_probs_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48963d47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "running_rewards=deque(maxlen=100)\n",
    "gamma = 0.95  \n",
    "episode_count = 0\n",
    "batches=0\n",
    "# Train the model\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        if batches%2==0:\n",
    "            action_probs_history,rewards_history,\\\n",
    "                episode_rewards=create_batch_red(batch_size)\n",
    "        else:\n",
    "            action_probs_history,rewards_history,\\\n",
    "                episode_rewards=create_batch_yellow(batch_size)            \n",
    "                     \n",
    "        # Calculating loss values to update our network        \n",
    "        rets=tf.convert_to_tensor(rewards_history,\\\n",
    "                                  dtype=tf.float32)\n",
    "        alosses=-tf.multiply(tf.convert_to_tensor(\\\n",
    "          action_probs_history,dtype=tf.float32),rets)\n",
    "        # Backpropagation\n",
    "        loss_value = tf.reduce_sum(alosses) \n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,\\\n",
    "                                  model.trainable_variables))\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    batches += 1\n",
    "    for r in episode_rewards:\n",
    "        running_rewards.append(r)\n",
    "    running_reward=np.mean(np.array(running_rewards)) \n",
    "    # print out progress\n",
    "    if episode_count % 100 == 0:\n",
    "        template = \"running reward: {:.6f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))   \n",
    "    # Stop if the game is solved\n",
    "    if running_reward>=0.9 and episode_count>100:  \n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "model.save(\"files/PG_conn.h5\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b121e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"files/PG_games_conn.p\",\"wb\") as f:\n",
    "    pickle.dump((allstates,alloutcome),f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "# 2.  Train A Value Network in Connect Four\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 2.1. How to Train a Value Network in Connect Four\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c5b19",
   "metadata": {},
   "source": [
    "## 2.2. Processing  the Game Experience Data in Connect Four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/PG_games_conn.p\",\"rb\") as f:\n",
    "    history,results=pickle.load(f)              \n",
    "        \n",
    "Xs=[]\n",
    "ys=[]\n",
    "for states, result in zip(history,results):\n",
    "    for state in states:\n",
    "        Xs.append(state)\n",
    "        if result==1:\n",
    "            ys.append(np.array([0,1,0]))\n",
    "        elif result==-1:\n",
    "            ys.append(np.array([0,0,1]))       \n",
    "        elif result==0:\n",
    "            ys.append(np.array([1,0,0]))  \n",
    "                            \n",
    "Xs=np.array(Xs).reshape(-1,7,6,1)\n",
    "ys=np.array(ys).reshape(-1,3)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "## 2.3. Train A Value Network in Connect Four\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Conv2D,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "value_model = Sequential()\n",
    "value_model.add(Conv2D(filters=128, \n",
    "  kernel_size=(4,4),padding=\"same\",activation=\"relu\",\n",
    "                 input_shape=(7,6,1)))\n",
    "value_model.add(Flatten())\n",
    "value_model.add(Dense(units=64, activation=\"relu\"))\n",
    "value_model.add(Dense(units=64, activation=\"relu\"))\n",
    "value_model.add(Dense(3, activation='softmax'))\n",
    "value_model.compile(loss='categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005), \n",
    "    metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6faf686c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model for 100 epochs\n",
    "value_model.fit(Xs, ys, epochs=100, verbose=1)\n",
    "value_model.save('files/value_conn.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "# 3. Play Connect Four with the Value Network\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a00ae",
   "metadata": {},
   "source": [
    "## 3.1. Best Moves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2efd4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design game strategy\n",
    "from copy import deepcopy\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "value_model=load_model('files/value_conn.h5') \n",
    "\n",
    "def best_move(env):\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome=-2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1,7,6,1)\n",
    "        if env.turn==\"red\":\n",
    "            ps=value_model.predict(state,verbose=0)\n",
    "        else:\n",
    "            ps=value_model.predict(-state,verbose=0)\n",
    "        # output is prob(win) - prob(lose)\n",
    "        win_lose_dif=ps[0][1]-ps[0][2]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f926d",
   "metadata": {},
   "source": [
    "## 3.2. Play Against the Policy Agent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f806366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network loses!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network wins!\n",
      "The value network loses!\n"
     ]
    }
   ],
   "source": [
    "# test ten games when move first\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset()  \n",
    "    # half the time, the policy network agent moves first\n",
    "    if i%2==0:\n",
    "        action=opponent(env)\n",
    "        state,reward,done,_=env.step(action)           \n",
    "    while True: \n",
    "        # move recommended by the value network\n",
    "        action=best_move(env)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        # if value network wins, record 1\n",
    "        if done:\n",
    "            if reward==0:\n",
    "                results.append(0)\n",
    "                print(\"It's a tie!\")\n",
    "            else:\n",
    "                results.append(1)\n",
    "                print(\"The value network wins!\")\n",
    "            break\n",
    "        # The policy network agent moves\n",
    "        action=opponent(env)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        # if value network loses, record -1\n",
    "        if done:\n",
    "            if reward==0:\n",
    "                results.append(0)\n",
    "                print(\"It's a tie!\")\n",
    "            else:  \n",
    "                results.append(-1)\n",
    "                print(\"The value network loses!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b275d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the value network agent has won 67 games\n",
      "the value network agent has lost 33 games\n",
      "the game is tied 0 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the value network agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the value network agent has won {wins} games\")\n",
    "# count how many times the value network agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the value network agent has lost {losses} games\")  \n",
    "# count how many tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game is tied {ties} times\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
