{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 8: Introduction to Monte Carlo Tree Search\n",
    "\n",
    "So far we have covered one type of tree search: minimax tree search. In simple games such as Last Coin Standing or Tic Tac Toe, minimax agents solve the game and provide game strategies that are as good as any other intelligent game strategies. In complicated games such as Connect Four or Chess, the number of possibilities is too large and it's infeasible for the minimax agent to solve the game in a short amount of time. We therefore, use depth pruning and alpha-beta pruning, combined with powerful position evaluation functions, for the minimax agent to come up with intelligent moves in the alloted time span. With such an approach, Deep Blue beat the world chess champion Gary Kasporov in 1997. \n",
    "\n",
    "While in games such as Chess, position evaluation functions are relatively accurate, in certain games such as Go, evaluating positions is harder. For example, in Chess, if white has an extra rook than black, it's difficult for the black to win or tie the game. We are fairly certain that white will win without following a game tree all the way to the end. In Go, on the other hand, guessing who will eventually win in the mid-game based on board positions is more difficult. Counting the number of stones for each side can provide a clue, but this can change in an instant if one side captures a large number of the opponent's stones. \n",
    "\n",
    "Therefore, in the game of Go, researchers usually use another type of tree search: Monte Carlo Tree Search (MCTS). In depth-pruned minimax tree search, the agent searches all possible outcomes to a fixed number of moves ahead, this is sometimes called a breadth-first approach. In contrast, in MCTS, the agent simulate games all the way to the terminal state to see the game outcome. It doesn't cover all scenarios. MCTS, therefore, is a depth-first approach. \n",
    "\n",
    "The idea behind MCTS is to roll out random games starting from the current game state and see what is the average game outcome. If you roll out, say 100, games from this point, and Player 1 wins 99 percent of the time, you know that the current game state must favor Player 1 against Player 2. \n",
    "\n",
    "In this chapter, you'll learn to implement a simple version of the MCTS, which we call naive MCTS. We'll implement it in the coin game and show that it provides powerful game strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 8}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 8 in a subfolder /files/ch08. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch08\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. What Is Monte Carlo Tree Search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "Monte Carlo Tree Search (MCTS) is a simulation method in AI to evaluate a game state by rolling out a large number of random games and see what the average outcome is. \n",
    "\n",
    "To help us understand how MCTS works, we'll develop a naive MCTS game strategy and apply it to the coin game that we developed in Chapter 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. A Thought Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "Imagine that you are playing the coin game against a naive MCTS agent. You move first and choose to take one coin from the pile. Let's code that in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1337aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is 21\n",
      "Player 1 has chosen action=1\n",
      "the current state is 20\n"
     ]
    }
   ],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "\n",
    "# Initiate the game environment\n",
    "env = coin_game()\n",
    "state=env.reset()   \n",
    "print(f\"the current state is {state}\")\n",
    "# Player 1 takes one coin from the pile\n",
    "player1_move=1\n",
    "print(f\"Player 1 has chosen action={player1_move}\") \n",
    "state, reward, done, info = env.step(player1_move)\n",
    "print(f\"the current state is {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f0581",
   "metadata": {},
   "source": [
    "The coin_simple_env is a simpliflied coin game environment. It's the same as the coin game environment we used in Chapter 1 except that we removed the graphical rendering functionality. We do this because in MCTS, the agent makes deep copies of the game environmet many times. Using a simplied game environment can greatly speed up the tree search process. The file coin_simple_env.py is in the folder /utils/ in the book's GitHub repository. Download the file and put it in /Desktop/ai/utils/ on your computer. \n",
    "\n",
    "You have chosen action=1. This leaves 20 coins in the pile. Now the naive MCTS agent needs to make a move. The agent thinks as follows: I'll simulate 10000 games from this point on. Sometimes I choose action=1 as my next move and other times I choose action=2 as my next move. I'll see which action leads to more wins for me and I'll pick that action as my next move. \n",
    "\n",
    "To do that, the naive MCTS agent first creates three dictionaries, counts, wins, and losses, to record the total number of games, number of wins, and number of losses associated with each next move. Let's code that in as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5a76d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dictionary counts has values {1: 0, 2: 0}\n",
      "the dictionary wins has values {1: 0, 2: 0}\n",
      "the dictionary losses has values {1: 0, 2: 0}\n"
     ]
    }
   ],
   "source": [
    "counts={}\n",
    "wins={}\n",
    "losses={}\n",
    "for move in env.validinputs:\n",
    "    counts[move]=0\n",
    "    wins[move]=0\n",
    "    losses[move]=0\n",
    "print(f\"the dictionary counts has values {counts}\")   \n",
    "print(f\"the dictionary wins has values {wins}\")    \n",
    "print(f\"the dictionary losses has values {losses}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddc025",
   "metadata": {},
   "source": [
    "If you run the above code cell, you'll see that the three dictionaries all start with values {1: 0, 2: 0}. The naive MCTS agent will then simulate games and update these three dictionaries accordingly. Below, the agent simulates 10,000 games: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0b8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# simulate 10,000 games\n",
    "for _ in range(10000):\n",
    "    # create a deep copy of the game environment\n",
    "    env_copy=deepcopy(env)\n",
    "    # record moves\n",
    "    actions=[]\n",
    "    # play a full game\n",
    "    while True:\n",
    "        #randomly select a next move\n",
    "        move=random.choice(env_copy.validinputs)\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            # see whehter the enxt move is 1 or 2\n",
    "            next_move=deepcopy(actions[0])\n",
    "            # update total number of simulated games\n",
    "            counts[actions[0]] += 1\n",
    "            # update total number of wins\n",
    "            if (reward==1 and env.turn==1) or (reward==-1 and env.turn==2):\n",
    "                wins[actions[0]] += 1\n",
    "            # update total number of losses                \n",
    "            if (reward==-1 and env.turn==1) or (reward==1 and env.turn==2):\n",
    "                losses[actions[0]] += 1                \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736cca0",
   "metadata": {},
   "source": [
    "Next, the MCTS agent counts total number of games, number of wins, and number of losses associated with each next move: action=1 and action=2. Let's code that in as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d6a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dictionary counts has values {1: 5084, 2: 4916}\n",
      "the dictionary wins has values {1: 2529, 2: 2458}\n",
      "the dictionary losses has values {1: 2555, 2: 2458}\n"
     ]
    }
   ],
   "source": [
    "print(f\"the dictionary counts has values {counts}\")   \n",
    "print(f\"the dictionary wins has values {wins}\")    \n",
    "print(f\"the dictionary losses has values {losses}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff3d78",
   "metadata": {},
   "source": [
    "Out of the 10,000 games, the MCTS agent has chosen action=1 5,084 times and the rest 4,916 times the agent has chosen action=2. When action=1 is chosen, the agent has won 2529 times and lost 2555 times. So the agent knows that when action=1 is chosen, he/she is more likely to lose than to win. On the other hand, if the agent chooses action=2 instead, he/she has an equal chance of winning and losing. Therefore, the agent prefers to choose action 2. Let's code in that thought process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5dfac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best move is 2\n"
     ]
    }
   ],
   "source": [
    "# See which action is most promising\n",
    "scores={}\n",
    "for k,v in counts.items():\n",
    "    scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "best_move=max(scores,key=scores.get)  \n",
    "print(f\"the best move is {best_move}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd48ae7",
   "metadata": {},
   "source": [
    "Here, the agent first creates a dictionary *scores*. For each possible next move, it assigns a socre to it. The score is the number of wins minus the number of losses, scaled by the total number of simulations. This score is a value between -1 and 1: if a move leads to winning 100% of the time, the move has a score of 1; if a move leads to losing 100% of the time, the move has a score of -1; a score of 0 means the move leads to an equal chance of winning and losing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 1.2. A Naive MCTS Algorithm\n",
    "Based on the thought experiment in the last subsection, let's create a naive MCTS algorithm. Whenever it's the MCTS agent's turn. The MCTS agent simulates 10,000 game and makes a decision based on which next action leads to better outcome. \n",
    "\n",
    "To do that, we first first define a simulate_a_game() function in the local package. Download ch08util.py from the book's GitHub repository and place it in the folder /Desktop/ai/utils/ on your computer. The function simulate_a_game() is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9332b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_a_game(env,counts,wins,losses):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    # play a full game\n",
    "    while True:\n",
    "        #randomly select a next move\n",
    "        move=random.choice(env_copy.validinputs)\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==1) or \\\n",
    "                (reward==-1 and env.turn==2):\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==1) or \\\n",
    "                (reward==1 and env.turn==2):\n",
    "                losses[actions[0]] += 1                \n",
    "            break\n",
    "    return counts, wins, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfde6",
   "metadata": {},
   "source": [
    "This function take four arguments: the game environment, env, and the three dictionaries: counts, wins, and losses. This function hypothetically plays a game, all the way to the terminal state, by choosing random moves for both players. After the game, it updates the three dictionaries. If the next move is 1, the number of games assocated with action 1 increases by 1 in the dictionary counts. Depending on wether the current player has won or lost, one of the two dictionaries, wins and losses, will update the number of wins or losses associated with action 1. For example, if action 1 is chosen as the next move and the game is lost, then the dictionary wins remains unchanged while the number of losses increases by 1 in the ditionary losses associated with key 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7961b4",
   "metadata": {},
   "source": [
    "In the file ch08util.py, we also define a best_move() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52448c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move(counts,wins,losses):\n",
    "    # See which action is most promising\n",
    "    scores={}\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "    best_move=max(scores,key=scores.get)  \n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e747d",
   "metadata": {},
   "source": [
    "This function selects the best move based on the three dictionaries: counts, wins, and losses. It calculates a score for each potential next move: the score is the difference between the number of wins and losses scaled by the total number of moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e851bb",
   "metadata": {},
   "source": [
    "Finally, we define the naive_mcts() function in ch08util.py as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9a82b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_mcts(env, num_rollouts=100):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0\n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        counts,wins,losses=simulate_a_game(env,counts,\\\n",
    "                                           wins, losses)\n",
    "    best_next_move=best_move(counts,wins,losses)  \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91376624",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 100. If there is only one legal move left, we skip searching and select the only move available. Otherwise, we create three dicitonaries counts, wins, and losses to record the outcomes from simulated games. Once the simulation is complete, we select the best next move based on the simulation results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "# 2. A Naive MCTS Player in the Coin Game\n",
    "In this section, we play games with the naive MCTS algorithm we just developed and see how effective it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c0ce",
   "metadata": {},
   "source": [
    "## 2.1. Play A Game Against the MCTS Agent\n",
    "We'll play a coin game agaist the naive MCTS agent we developed in the last seciton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbedc819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is state=21\n",
      "Player 1, what's your move?1\n",
      "Player 1 has chosen 1\n",
      "the current state is state=20\n",
      "Player 2 has chosen 2\n",
      "the current state is state=18\n",
      "Player 1, what's your move?2\n",
      "Player 1 has chosen 2\n",
      "the current state is state=16\n",
      "Player 2 has chosen 1\n",
      "the current state is state=15\n",
      "Player 1, what's your move?1\n",
      "Player 1 has chosen 1\n",
      "the current state is state=14\n",
      "Player 2 has chosen 1\n",
      "the current state is state=13\n",
      "Player 1, what's your move?2\n",
      "Player 1 has chosen 2\n",
      "the current state is state=11\n",
      "Player 2 has chosen 1\n",
      "the current state is state=10\n",
      "Player 1, what's your move?1\n",
      "Player 1 has chosen 1\n",
      "the current state is state=9\n",
      "Player 2 has chosen 2\n",
      "the current state is state=7\n",
      "Player 1, what's your move?1\n",
      "Player 1 has chosen 1\n",
      "the current state is state=6\n",
      "Player 2 has chosen 2\n",
      "the current state is state=4\n",
      "Player 1, what's your move?1\n",
      "Player 1 has chosen 1\n",
      "the current state is state=3\n",
      "Player 2 has chosen 1\n",
      "the current state is state=2\n",
      "Player 1, what's your move?1\n",
      "Player 1 has chosen 1\n",
      "the current state is state=1\n",
      "Player 2 has chosen 1\n",
      "the current state is state=0\n",
      "Player 2 has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch08util import naive_mcts\n",
    "\n",
    "state=env.reset() \n",
    "print(f\"the current state is state={env.state}\")\n",
    "\n",
    "while True:\n",
    "    action=int(input(\"Player 1, what's your move?\"))\n",
    "    print(f\"Player {env.turn} has chosen {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is state={env.state}\")\n",
    "    if done:\n",
    "        print(\"Player 1 has won!\") \n",
    "        break  \n",
    "    action=naive_mcts(env,num_rollouts=100)\n",
    "    print(f\"Player {env.turn} has chosen {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is state={env.state}\")\n",
    "    if done:\n",
    "        print(\"Player 2 has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b208ef",
   "metadata": {},
   "source": [
    "With 100 rollouts in each step, the naive MCTS agent is not very strong. But let's see whehter it's better than a random player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cead4",
   "metadata": {},
   "source": [
    "## 2.2. Effectiveness of the Naive MCTS Algorithm\n",
    "We will let the naive MCTS agent play against a random player 100 games and see how many times it wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06ef98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env = coin_game()\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action = naive_mcts(env,num_rollouts=1000)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            results.append(1) \n",
    "            break  \n",
    "        action = random.choice(env.validinputs)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the MCTS agent loses\n",
    "            results.append(-1) \n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bfefc",
   "metadata": {},
   "source": [
    "Half the time, the MCTS agent moves first so that no player has an advantage. We record a result of 1 if the MCTS agent wins and a result of -1 if the MCTS agent loses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fa1bb",
   "metadata": {},
   "source": [
    "We now count how many times the MCTS agent with pruning has won and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MCTS agent has won 96 games\n",
      "the MCTS agent has lost 4 games\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MCTS agent has lost {losses} games\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907880a",
   "metadata": {},
   "source": [
    "The above results show that the naive MCTS agent is much better than the random agent: winning 96% of the time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
