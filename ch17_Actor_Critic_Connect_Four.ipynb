{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 17: Apply the Actor-Critic Method to Connect Four\n",
    "\n",
    "In Chapter 16, we applied the actor-critic method to Tic Tac Toe. Specifically, you learned how to handle illegal moves and how to use vectorization to speed up training. After roughly ten hours of training, the actor-critic agent is much better than a rule-based AI who thinks three steps ahead.\n",
    "\n",
    "In this chapter, you'll apply similar techniques to the Connect Four game. You'll handle illegal moves the same way you did in Chapter 16: assigning a reward of -1 every time the agent makes an illegal move. You'll also use vectorization to speed up training. You'll create one single model to train both the red player and the yellow player in Connect Four. We'll train the model as follows: in the first ten games, we'll let the red player select moves based on the recommendations from the policy network in the model. A think-three-steps-ahead AI acts as Player 2. We'll then collect information such as the natural logarithms of the predicted probabilities, discounted rewards, and the expected game outcome from the value network to update model paramaters (i.e., train the model). In the second ten games, we'll let the yellow player select moves based on recommendations from the policy network in the model to play against a think-three-steps-ahead AI. We'll also collect information such as the natural logarithms of the predicted probabilities, discounted rewards, and the expected game outcome from the value network to update model paramaters (i.e., train the model). We'll alternate between training the red player and training the yellow player after every ten games. After the average reward from the actor-critic agent reaches 0.8, we stop the training process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 17}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 17 in a subfolder /files/ch17. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch17\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "# 1. The Actor-Critic Method in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963a4e3",
   "metadata": {},
   "source": [
    "You'll learn how to play Connect Four using the actor critic approach. For the moment, we'll just train an actor-critic model by simulating games with the AC agent moveing first and the think-two-steps-ahead AI moveing second. Later we'll use the same model to simulate games with the AC agent moveing second. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "## 1.1. A Model for Connect Four\n",
    "To save time, we'll create a model with just one input layer with 42 values in it. We'll not use a convolutional layer to extract the spatial features on the Connect Four board. Doing so is extremely computationally costly. Interested readers with access to supercomputing facilities can try this by mimicking what we did in Chatper 16 for the Tic Tac Toe game. Or you can look at Chapter 22 in which we created an actor-critic model for the Connect Four game with the convolutional input layer.  \n",
    "\n",
    "Specifically, the model we use in this chapter is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\.conda\\envs\\deepq\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_inputs = 42\n",
    "num_actions = 7\n",
    "# input layer \n",
    "inputs = layers.Input(shape=(42))\n",
    "# common layer \n",
    "common = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "# policy network\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "# value network\n",
    "critic = layers.Dense(1, activation=\"tanh\")(common)\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1def0",
   "metadata": {},
   "source": [
    "The model has one input network with 42 values in it (that is, we'll flatten the Connect Four game board into a one-dimensional vector and feed it into the model). We create two output networks as we did in Chapter 16: one policy network and one value network. \n",
    "\n",
    "Below, we specify the optimizer and the loss function we use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "loss_func = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398c385",
   "metadata": {},
   "source": [
    "The optimizer is Adam with a learning rate of 0.0005. The loss function is the mean absolute error loss function, which punishes outliner less compared to other loss functions such as Huber or mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 1.2. Simulate Connect Four Games\n",
    "We'll let the agent play against the look-three-steps-ahead AI that we developed in Chapter 3. Below, we import the red_think3() and yellow_think3() functions from the local module conn_think3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75cb7727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_think3 import red_think3,yellow_think3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfd813",
   "metadata": {},
   "source": [
    "Next, we'll define a playing_red() function. The playing_red() function simulates a full game, with the rule-based AI as the second player, and the actor-critic (AC) agent as the first player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "\n",
    "# allow a maximum of 50 steps per game\n",
    "max_steps=50\n",
    "env=conn()\n",
    "def playing_red():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        # estimate action probabilities and future rewards\n",
    "        action_probs, critic_value = model(state)\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                    tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(reward)\n",
    "                episode_reward += reward \n",
    "                break\n",
    "            else:\n",
    "                state, reward, done, _ = env.step(\\\n",
    "                                  yellow_think3(env))\n",
    "                rewards_history.append(reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            wrongmoves_history,rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387db1da",
   "metadata": {},
   "source": [
    "The playing_red() function simulates a full game, and it records all the intermediate steps made by the AC agent. The function returns five values: \n",
    "* a list action_probs_history with the natural logorithm of the recommended probability of the action taken by the agent from the policy network; \n",
    "* a list critic_value_history with the estimated future rewards from the value network; \n",
    "* rewards_history with the rewards to each action taken by the agent in the game;\n",
    "* wroingmoves_history with the rewards to each action associated with wrong moves;\n",
    "* a number episode_reward showing the total rewards to the agent during the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1121b",
   "metadata": {},
   "source": [
    "In reinforcement learning, actions affect not only current period rewards, but also future rewards. This is the credit assignment problem, which is at the heart of every reinforcement learing algorithm. The solution is to give credits to previous moves by using discounted rewards. We therefore use discounted rewards to assign credits properly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def discount_rs(r,wrong):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        if wrong[i]==0:  \n",
    "            running_add = gamma*running_add + r[i]\n",
    "            discounted_rs[i] = running_add  \n",
    "    return discounted_rs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d64a2f",
   "metadata": {},
   "source": [
    "The function takes two lists, *r* and *wrong*, as inputs, where *r* are the rewards related to legal moves while *wrong* are rewards related to illegal moves. We discount *r* but not *wrong*. The output is the properly disounted reward in each time period when a legal move is made. Note that if an illegal move is made in a time period, the reward is 0 from the output. We'll add in the reward of -1 for illgal moves later at training. \n",
    "\n",
    "Similarly, to train the yellow player, we define a *playing_yellow()* function. The function simulates a full game, with the rule-based AI as the first player, and the actor-critic (AC) agent as the second player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2eca865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_yellow():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    state, reward, done, _ = env.step(red_think3(env))\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        # estimate action probabilities and future rewards\n",
    "        action_probs, critic_value = model(-state)\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions, \\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(tf.math.log(\\\n",
    "                                action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(-reward)\n",
    "                episode_reward += -reward \n",
    "                break\n",
    "            else:\n",
    "                state, reward, done, _ = env.step(red_think3(env))\n",
    "                rewards_history.append(-reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += -reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            wrongmoves_history,rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bce1b5",
   "metadata": {},
   "source": [
    "The *playing_yellow()* function simulates a full game and records all the intermediate steps made by the AC agent. The function returns five values as those from the function *playing_red()*.\n",
    "Most importantly, we use *-state* instead of *state* when we feed the game board to the model. This is because the game board is coded as 1 if a cell has a red disc in it, -1 if a cell has a yellow disc in it, and 0f if the cell is empty. To use the same network for both palyers, we multiply the game board by -1 so that the model treat 1 as occupied by the current player and -1 as occupied by the opponent. \n",
    "\n",
    "The second most important thinng is to change the rewards. Instead of using 1 and -1 to denote a win by the red and yellow player, respectively, we'll use a reward of 1 to denote the current player has won and a reward of -1 to denote that the current player has lost. We accomplish this by use *-reward* instead of *reward* in the above code cell. Therefore, after the yellow player makes a move, the reward is now 1 if the yellow player wins and -1 if the yellow player loses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "# 2. Train the Red and Yellow Players\n",
    "In this section, we'll train the red and yellow players using the same model. \n",
    "\n",
    "## 2.1. Create Batches for Training\n",
    "Instead of updating model parameters after one episode, we update after a certain number of episodes to make the model stable. Here we update parameters every ten games when training the red or the yellow player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "def create_batch(playing_function):\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,cvs,wms,rs,er = playing_function()\n",
    "        # rewards are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        critic_value_history += cvs\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er)        \n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history,episode_rewards"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cd09207",
   "metadata": {},
   "source": [
    "The function create_batch() can be used to create a batch to train the red player or the yellow player. If you want to train the red player, use playing_red as the argument in the create_batch() function. Otherwise, use playing_yellow as the argument in the create_batch() function.  \n",
    "\n",
    "Note above we discount the rewards associated with wins and losses, but not the rewards associated with wrong moves, due to the credit assignment problem we discussed in the last chapter. We then combined the discounted rewards with punishments associated with wroing moves and put the combined values in the list rewards_history. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef0043",
   "metadata": {},
   "source": [
    "## 2.2. Train the Model for Both Players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0d413",
   "metadata": {},
   "source": [
    "To alternate between training the red player and training the yellow player, we'll create a variable *batches*. It starts with a value of 1. After each batch of training, we add 1 to the value of the variable *batches*. We'll train the red player when the value of *batches* is even and train the yellow player otherwise. \n",
    "\n",
    "We train the model till the average episode reward to the AC agent is 0.5 or above in the last 10000 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc850b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "running_rewards=deque(maxlen=10000)\n",
    "episode_count = 0\n",
    "batches=0\n",
    "gamma=0.95\n",
    "# Train the model\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        if batches%2==0:\n",
    "            action_probs_history,critic_value_history,\\\n",
    "            rewards_history,episode_rewards=create_batch(playing_red)\n",
    "        else:\n",
    "            action_probs_history,critic_value_history,\\\n",
    "            rewards_history,episode_rewards=create_batch(playing_yellow)                                \n",
    "        # Calculating loss values to update our network        \n",
    "        tfdif=tf.convert_to_tensor(rewards_history,dtype=tf.float32)-\\\n",
    "            tf.convert_to_tensor(critic_value_history,dtype=tf.float32)\n",
    "        alosses=-tf.multiply(tf.convert_to_tensor(action_probs_history,\\\n",
    "                            dtype=tf.float32),tfdif)\n",
    "        closs=loss_func(tf.convert_to_tensor(rewards_history,dtype=tf.float32),\\\n",
    "             tf.convert_to_tensor(critic_value_history,dtype=tf.float32))\n",
    "        # Backpropagation\n",
    "        loss_value = tf.reduce_sum(alosses) + closs\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    batches += 1\n",
    "    for r in episode_rewards:\n",
    "        running_rewards.append(r)\n",
    "    running_reward=np.mean(np.array(running_rewards)) \n",
    "    # print out progress\n",
    "    if episode_count % 1000 == 0:\n",
    "        template = \"running reward: {:.6f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count)) \n",
    "                \n",
    "    # Stop if the game is solved\n",
    "    if running_reward > 0.5 and episode_count>100:  \n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break \n",
    "model.save(\"files/ch17/ac_conn.h5\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2ef29",
   "metadata": {},
   "source": [
    "It takes about 24 hours to train the model. Once done, the model is saved as ac_conn.h5 on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda387a5",
   "metadata": {},
   "source": [
    "# 3. Play Connect Four with the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f4bac",
   "metadata": {},
   "source": [
    "We'll use the trained model to play against the think-three-steps-ahead rule-based AI. We'll use stochastic policy only: select the moves according to the probabilities produced by the policy network. Interested readers can try the deterministic policy yourselves and the results are similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2436f04",
   "metadata": {},
   "source": [
    "## 3.1. Define An AC Player"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351afaf",
   "metadata": {},
   "source": [
    "We define a ACplayer() function that produces the best move based on the trained AC model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6ebff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model(\"files/ch17/ac_conn.h5\")\n",
    "\n",
    "def ACplayer(env): \n",
    "    state = env.state.reshape(-1,42,)\n",
    "    if env.turn==\"red\":\n",
    "        action_probs, critic_value = model(state)\n",
    "    else:\n",
    "        action_probs, critic_value = model(-state)        \n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return np.random.choice(sorted(env.validinputs),p=ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740da4d9",
   "metadata": {},
   "source": [
    "We feed the current game board to the model. It's important that if it's the yellow player's turn, we multiply the game board by -1 before feeding it into the model. We look at output from the policy network of the trained model and focus on valid moves. We normalize the probability distribution among all valid moves so the sume of probabilities add up to one. The agent takes actions based on this probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c36caa",
   "metadata": {},
   "source": [
    "## 3.2. When the AC Player Moves First\n",
    "The AC player chooses the next action based on the proabbiltiy distrinution recommended by the policy network from the trained model. The opponent is the think-three-steps-ahead rule-based AI we developed in Chapter 3. We play 100 games and let the AC player move first and the rule-based AI move second.\n",
    "\n",
    "We create a list *results*. In each game, the AC agent moves first and the rule-based AI moves second. If the AC agent wins, we add an outcome 1 to *results*. If the rule-based AI wins, we add an outcome of -1 to *results*. If the game is tied, we add a 0 to *results*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b875cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env=conn()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action=ACplayer(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(1)\n",
    "            break\n",
    "        # select action based on policy network\n",
    "        action=yellow_think3(env) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)\n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9675a0",
   "metadata": {},
   "source": [
    "We can count how many times the AC agent has won and how many times the AC agent has lost, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26417c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 84 games\n",
      "The AC player has lost 16 games\n",
      "There are 0 tied games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")              \n",
    "# Print out the number of tie games\n",
    "ties=results.count(0)\n",
    "print(f\"There are {ties} tied games\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bb85e",
   "metadata": {},
   "source": [
    "The AC agent has won 84 games and lost 16 games. None of the games is tied. The AC agent is clearly better than the think-three-steps-ahead AI when it moves first. Next, we'll test how the AC agent fairs when it moves second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70381d0",
   "metadata": {},
   "source": [
    "## 3.3. When the AC Player Moves Second\n",
    "Next, we test if the AC agent can beat the think-three-steps-ahead rule-based AI when it moves second. We again create an empty list *results*. We simulate 100 games. In each game, the rule-based AI moves first and the AC agent moves second. If the AC agent wins, we add an outcome 1 to *results*. If the rule-based AI wins, we add an outcome of -1 to *results*. If the game is tied, we add a 0 to *results*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d35da9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    env=conn()\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action=red_think3(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # select action based on policy network\n",
    "        action=ACplayer(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)\n",
    "            break          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c188a",
   "metadata": {},
   "source": [
    "We look at the game outcomes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f1e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AC player has won 74 games\n",
      "The AC player has lost 26 games\n",
      "There are 0 tied games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AC won\n",
    "wins=results.count(1)\n",
    "print(f\"The AC player has won {wins} games\")\n",
    "# Print out the number of games that AI lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AC player has lost {losses} games\")              \n",
    "# Print out the number of tie games\n",
    "ties=results.count(0)\n",
    "print(f\"There are {ties} tied games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af4e8a",
   "metadata": {},
   "source": [
    "The AC agent has won 74 games and lost 26 games. Taken together, the AC agent is clearly much better than the think-three-steps-ahead AI player. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
