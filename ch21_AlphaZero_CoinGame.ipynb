{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 20: Alpha Zero: The Coin Game Version\n",
    "\n",
    "In 2017, the DeepMind team published an improved version of AlphaGo, which they dubbed AlphaGo Zero. In contrast to the previous version of AlphaGo that beat the World Go Champion Lee Sedol in 2016, AlphaGo Zero was trained soley on deep reinforcement learning. The algorithm didn't use any human input or domain knowledge other than game rules. AlphaGo Zero was trained through self-play from scratch. This has huge implications for AI: to tackle the world's most complicated problems, human input and domain knowledge are limited in many cases. How can AI provide powrful solutions in such scenarios is challenging. AlphaGo Zero's success, however, gives us hope. \n",
    "\n",
    "In the next three chapters, you'll learn how to play simple games such as the coing game without any training data or human input other than game rules. We'll start from scratch and use only self-play to train the models. You'll see that intelligent game strategies can be created this way. \n",
    "\n",
    "In this chapter, you'll learn to train a perfect game strategy to play the coin game through self-play only. Specifically, we'll create an actor-critic (AC) deep reinforcement model as we did in previous chapters. However, we'll not use ruled-based AI players to train the model. Instead, we'll let the policy network in the AC model to guide the players to select moves. At the same time, we'll create an AC agent to act as the opponent in the training process, hence the term \"self-play\". After just a few thousand games, the AC model is able to guide players to make perfect moves. In particular, the trained model will win the game 100% of the time if it moves second, even against the perfect rule-based AI player that we built in Chapter 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 21}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 21 in a subfolder /files/ch21. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch21\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "# 1. An Overview of the Training Process\n",
    "In this section, we'll first summarize the steps to train the Alpha Zero agent for the coin game from scratch without any data or human input. We'll then lay out the deep reinforcement learning model we use for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e50de",
   "metadata": {},
   "source": [
    "## 1.1. Steps to Train An Alpha Zero Agent\n",
    "We'll first create a deep reinforcement model for the coin game with the actor-critc method as we did in the last few chapters. As usual, the model has a policy network and a value network. \n",
    "\n",
    "Once we have a model, we'll train player 1 and player 2 in the coin game. In the first ten games, we'll let Player 1 selects moves based on the recommendations from the policy network from the deep reinforcement model we just created. Player 1 plays against the model itself, hence the name self-play. We collect information from the first ten games such as the natural logarithms of the predicted probabilities, discounted rewards, and the expected game outcome from the value network to update model paramaters (i.e., train the model). In the second ten games, we'll let Player 2 selects moves based on recommendations from the same policy network in the model to play against the model itself. We'll also collect information such as the natural logarithms of the predicted probabilities, discounted rewards, and the expected game outcome from the value network to update model paramaters (i.e., train the model). We'll alternate between training Player 1 and Player 2 after every ten games. \n",
    "\n",
    "To make sure we know when to stop training, we'll test the model after every 1000 episodes: if trained model can play the game perfectly as the second player against the rule-based AI player, we'll stop training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "## 1.2. A Deep Reinforcement Model for the Coin Game\n",
    "We'll create a model with just one input layer with 22 values in it. The model has two output layers: a policy network (actor) to predict the next move and a value network to predict the expected game outcome. The modle is the same as the model we used in Chapter 15.\n",
    "\n",
    "Specifically, the model we use is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_inputs = 22\n",
    "num_actions = 2\n",
    "# The input layer\n",
    "inputs = layers.Input(shape=(22,))\n",
    "# The common layer\n",
    "common = layers.Dense(32, activation=\"relu\")(inputs)\n",
    "common = layers.Dense(32, activation=\"relu\")(common)\n",
    "# The policy network\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "# The value network\n",
    "critic = layers.Dense(1, activation=\"tanh\")(common)\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1def0",
   "metadata": {},
   "source": [
    "Below, we specify the optimizer and the loss function we use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_func = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398c385",
   "metadata": {},
   "source": [
    "The optimizer is Adam with a learning rate of 0.01. The loss function is the mean absolute error loss function, which punishes outliner less compared to other loss functions such as Huber or mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0a431",
   "metadata": {},
   "source": [
    "# 2. Train Player 1\n",
    "In this section, we'll discuss in detail how to train the model for Player 1. Specifically, we'll create an AC agent based on the deep reinforcement learning model we created in the last section. The AC agent will act as the opponents for both Player 1 and Player 2 in self-plays. \n",
    "\n",
    "We then simulate ten games at a time. After ten games, we'll collect information such as the predicted probabilities, discounted rewards, and the expected game outcome to update the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 2.1. Create An Opponent\n",
    "We'll let Player 1 play against an AC agent based on the same deep reinforcement learning model. Later, we'll use the same AC agent as the opponent for Player 2 as well. \n",
    "\n",
    "For that purpose, we define a AC_agent() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b08faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encoder(state):\n",
    "    onehot=np.zeros((1,22))\n",
    "    onehot[0,state]=1\n",
    "    return onehot\n",
    "\n",
    "def ACplayer(env): \n",
    "    # estimate action probabilities and future rewards\n",
    "    onehot_state = onehot_encoder(env.state)\n",
    "    action_probs, _ = model(onehot_state)\n",
    "    # select action with the highest probability\n",
    "    action=np.random.choice([1,2],p=np.squeeze(action_probs))   \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfd813",
   "metadata": {},
   "source": [
    "## 2.2. Simulate Games for Player 1\n",
    "Next, we'll define a playing_1() function. The playing_1() function simulates a full game, with Player 1 selecting moves from the policy network in the model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "env=coin_game()\n",
    "def playing_1():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        # estimate action probabilities and future rewards\n",
    "        onehot_state = onehot_encoder(env.state)\n",
    "        action_probs, critic_value = model(onehot_state)\n",
    "        # select action based on policy network\n",
    "        action=np.random.choice([1,2],p=np.squeeze(action_probs))\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                    tf.math.log(action_probs[0, action-1]))      \n",
    "        # Apply the sampled action in our environment\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            rewards_history.append(reward)\n",
    "            break\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(ACplayer(env))\n",
    "            rewards_history.append(reward)               \n",
    "            if done:\n",
    "                break                \n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387db1da",
   "metadata": {},
   "source": [
    "The playing_red() function simulates a full game, and it records all the intermediate steps made by the AC agent. The function returns five values: \n",
    "* a list action_probs_history with the natural logorithm of the recommended probability of the action taken by the agent from the policy network; \n",
    "* a list critic_value_history with the estimated future rewards from the value network; \n",
    "* rewards_history with the rewards to each action taken by the agent in the game;\n",
    "* wroingmoves_history with the rewards to each action associated with wrong moves;\n",
    "* a number episode_reward showing the total rewards to the agent during the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1121b",
   "metadata": {},
   "source": [
    "In reinforcement learning, actions affect not only current period rewards, but also future rewards. This is the credit assignment problem, which is at the heart of every reinforcement learing algorithm. The solution is to give credits to previous moves by using discounted rewards. We therefore use discounted rewards to assign credits properly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rs(r):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        running_add = gamma*running_add + r[i]\n",
    "        discounted_rs[i] = running_add  \n",
    "    return discounted_rs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 1.3. Update Parameters\n",
    "Instead of updating model parameters after one episode, we update after a certain number of episodes to make the model stable. Here we update parameters every ten games, as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "def create_batch(playing_func):\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    for i in range(batch_size):\n",
    "        aps,cvs,rs = playing_func()\n",
    "        # rewards are discounted\n",
    "        returns = discount_rs(rs)\n",
    "        action_probs_history += aps\n",
    "        critic_value_history += cvs\n",
    "        rewards_history += returns       \n",
    "    return action_probs_history,critic_value_history,\\\n",
    "        rewards_history"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cd09207",
   "metadata": {},
   "source": [
    "Note above we discount the rewards associated with wins and losses, but not the rewards associated with wrong moves, due to the credit assignment problem we discussed in the last chapter. We then combined the discounted rewards with punishments associated with wroing moves and put the combined values in the list rewards_history. \n",
    "\n",
    "We can potentially train the model and update the parameters until the average episode reward to the AC agent in the last 100 games reaches a certain value. But if we do so, we can only use the model to design game strategies for the red player, not the yellow player. So we'll wait until later to train the model. \n",
    "\n",
    "However, we can train the model with just one batch of data (i.e., ten games) to make sure that we can indeed use the model to train game strategies for the red player. Run the Python code in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6e8ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.95\n",
    "def train_player(playing_func):\n",
    "    # Train the model for one batch (ten games)\n",
    "    with tf.GradientTape() as tape:\n",
    "        action_probs_history,critic_value_history,\\\n",
    "            rewards_history=create_batch(playing_func)\n",
    "        # Calculating loss values to update our network        \n",
    "        tfdif=tf.convert_to_tensor(rewards_history,\\\n",
    "                                   dtype=tf.float32)-\\\n",
    "        tf.convert_to_tensor(critic_value_history,dtype=tf.float32)\n",
    "        alosses=-tf.multiply(tf.convert_to_tensor(\\\n",
    "          action_probs_history,dtype=tf.float32),tfdif)\n",
    "        closs=loss_func(tf.convert_to_tensor(rewards_history,\\\n",
    "                                             dtype=tf.float32),\\\n",
    "     tf.convert_to_tensor(critic_value_history,dtype=tf.float32))\n",
    "        # Backpropagation\n",
    "        loss_value = tf.reduce_sum(alosses) + closs\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,\\\n",
    "                                  model.trainable_variables)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c598877",
   "metadata": {},
   "source": [
    "Note here we adjust the parameters by the preduct of the gradients and the discounted rewards. This is related to the solution to the rewards maximization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974dc206",
   "metadata": {},
   "source": [
    "# 3. Train Player 2\n",
    "In the last section, you learned how to train the AC agent to move first to play against a rule-based AI. Next, we'll use the same actor-critic model to train the AC agent to move second to play again a rule-based AI. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5fb6d",
   "metadata": {},
   "source": [
    "Next we define the playing_2() function to simulate a full game, with the rule-based AI as the first player, and the actor-critic (AC) agent as the second player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "053a34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_2():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    state, reward, done, _ = env.step(ACplayer(env))\n",
    "    while True:\n",
    "        # estimate action probabilities and future rewards\n",
    "        onehot_state = onehot_encoder(env.state)\n",
    "        action_probs, critic_value = model(onehot_state)\n",
    "        # select action based on policy network\n",
    "        action=np.random.choice([1,2],p=np.squeeze(action_probs))\n",
    "        # record value history\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action-1]))      \n",
    "        # Apply the sampled action in our environment\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            rewards_history.append(-reward)\n",
    "            break\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(ACplayer(env))\n",
    "            rewards_history.append(-reward)                \n",
    "            if done:\n",
    "                break                \n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            rewards_history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2dde9",
   "metadata": {},
   "source": [
    "We'll call the function playing_1() to train Player 1, and playing_2() to train Player 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0070fd",
   "metadata": {},
   "source": [
    "Now that we have a model that can be used to create game strategies for both players, we'll just need to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef0043",
   "metadata": {},
   "source": [
    "# 3. Train the Model for Both Players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0d413",
   "metadata": {},
   "source": [
    "To alternate between training the red player and training the yellow player, we'll create a variable *batches*. It starts with a value of 1. After each batch of training, we add 1 to the value of the variable *batches*. We'll train the red player when the value of *batches* is even and train the yellow player otherwise. \n",
    "\n",
    "## 3.1. Keep Track of Progress\n",
    "\n",
    "Alpha Zero algorithms are extremely costly in terms of computational needs. Therefore, it takes a long time to train the model. To mkae things more complicated, we need to tune the hyper parameters to make the model works (such as the larning rate, the loss function, and so on). Therefore it's important to mesure the progress of the model so that you know whehter things are going in the right dirction or not.\n",
    "\n",
    "To do that, we'll test teh model against the rule-based AI to determine the rpogress. Note that the rule-based AI is not involved in traing hte model directly. It just check how good the model is periodially without itnerfereing with teh model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c62f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def rule_based_AI(state):\n",
    "    if state%3 != 0:\n",
    "        move = state%3\n",
    "    else:\n",
    "        move = random.choice([1,2])\n",
    "    return move\n",
    "\n",
    "def test():\n",
    "    results=[]\n",
    "    for i in range(100):\n",
    "        env = coin_game()\n",
    "        state=env.reset()     \n",
    "        while True:    \n",
    "            action = rule_based_AI(state)  \n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                # record -1 if rule-based AI player won\n",
    "                results.append(-1)\n",
    "                break\n",
    "            # estimate action probabilities and future rewards\n",
    "            onehot_state = onehot_encoder(state)\n",
    "            action_probs, _ = model(onehot_state)\n",
    "            # select action with the highest probability\n",
    "            action=np.argmax(action_probs[0])+1\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                # record 1 if AC agent won\n",
    "                results.append(1)            \n",
    "                break  \n",
    "    return results.count(1)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2fa86",
   "metadata": {},
   "source": [
    "## 3.2. Train the Model till It's Perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954104f",
   "metadata": {},
   "source": [
    "Train the model till it's perfect. Here after every 1000 episodes of training, we test teh model against he AI to see teh rpogress. We'll stop if the model beats the AI 100% of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc850b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 1000, number of wins is 1/100\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 2000, number of wins is 6/100\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 3000, number of wins is 0/100\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 4000, number of wins is 0/100\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 5000, number of wins is 47/100\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 6000, number of wins is 50/100\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "at episode 7000, number of wins is 100/100\n"
     ]
    }
   ],
   "source": [
    "episode_count = 0\n",
    "batches=0\n",
    "# Train the model\n",
    "while True:\n",
    "    if batches%2==0:\n",
    "        train_player(playing_1)\n",
    "    else:\n",
    "        train_player(playing_2)\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    batches += 1\n",
    "    # print out progress\n",
    "    if episode_count % 1000 == 0:\n",
    "        model.save(\"files/ch21/alphazero_coin.h5\")        \n",
    "        wins=test()\n",
    "        print(f\"at episode {episode_count}, number of wins is {wins}/100\")\n",
    "        if wins==100:\n",
    "            break     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2ef29",
   "metadata": {},
   "source": [
    "The model is successfully trained after 7000 episodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda387a5",
   "metadata": {},
   "source": [
    "# 4. Play The Coin Game with the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f4bac",
   "metadata": {},
   "source": [
    "We'll use the trained model to play against the rule-based AI. We'll use stochastic policy first: select the moves according to the probabilities produced by the policy network. Later we'll try the deterministic policy as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c36caa",
   "metadata": {},
   "source": [
    "## 4.1. When the Alpha Zero Agent Moves First\n",
    "The Alpha Zero agent chooses the next action based on the proabbiltiy distrinution recommended by the policy network from the trained model. Below we assume the Alpha Zero agent moves first and it plays against random moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43b875cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = ACplayer(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(1)\n",
    "            break\n",
    "        action=random.choice(env.validinputs) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(-1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9675a0",
   "metadata": {},
   "source": [
    "Note every time time the Alpha Zero agent wins, we add 1 to the list *results*. If the Alpha Zero agent loses, we add -1 to the list *results*.  We can count how many times the Alpha Zero agent has won and lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a26417c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Alpha Zero agent has won 96 games\n",
      "The Alpha Zero agent has lost 4 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that Alpha Zero agent won\n",
    "wins=results.count(1)\n",
    "print(f\"The Alpha Zero agent has won {wins} games\")\n",
    "# Print out the number of games that Alpha Zero agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The Alpha Zero agent has lost {losses} games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301bb85e",
   "metadata": {},
   "source": [
    "The Alpha Zero agent has won 96 out of 100 games. Next, we'll test how the Alpha Zero agent fairs when it moves second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70381d0",
   "metadata": {},
   "source": [
    "## 4.2. When the Alpha Zero Agent Moves Second\n",
    "The Alpha Zero agent chooses the next action based on the proabbiltiy distrinution recommended by the policy network from the trained model. We let the Alpha Zero agent move second while the rule-based AI moves first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d35da9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "for i in range(100):\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = rule_based_AI(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # select action based on policy network\n",
    "        action= ACplayer(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c188a",
   "metadata": {},
   "source": [
    "As before, every time time the Alpha Zero agent wins, we add 1 to the list *results*. If the Alpha Zero agent loses, we add -1 to the list *results*.  We can count how many times the Alpha Zero agent has won and lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f1e4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Alpha Zero agent has won 92 games\n",
      "The Alpha Zero agent has lost 8 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that Alpha Zero agent won\n",
    "wins=results.count(1)\n",
    "print(f\"The Alpha Zero agent has won {wins} games\")\n",
    "# Print out the number of games that Alpha Zero agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The Alpha Zero agent has lost {losses} games\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30831c7",
   "metadata": {},
   "source": [
    "## 4.3. A Deterministic Game Strategy\n",
    "We define a AlphaZeroDeterministic() function that produces the best move based on the trained AC model. The best move is the next move with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6935beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlphaZeroDeterministic(env): \n",
    "    state = env.state\n",
    "    onehot_state = onehot_encoder(state)\n",
    "    action_probs, _ = model(onehot_state)\n",
    "    action_probs, critic_value = model(onehot_state)\n",
    "    idx=np.argmax(np.squeeze(action_probs))\n",
    "    return idx+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0108ab",
   "metadata": {},
   "source": [
    "We play 100 games and let the rule-based AI move first. The Alpha Zero agent moves second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a95c1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "for i in range(100):\n",
    "    state=env.reset()     \n",
    "    while True:    \n",
    "        action = rule_based_AI(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(-1)\n",
    "            break\n",
    "        # select action based on policy network\n",
    "        action=AlphaZeroDeterministic(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            results.append(1)            \n",
    "            break            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b9883",
   "metadata": {},
   "source": [
    "Every time time the Alpha Zero agent wins, we add 1 to the list *results*. If the Alpha Zero agent loses, we add -1 to the list *results*.  We can count how many times the Alpha Zero agent has won and lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f97736e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Alpha Zero agent has won 100 games\n",
      "The Alpha Zero agent has lost 0 games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that Alpha Zero agent won\n",
    "wins=results.count(1)\n",
    "print(f\"The Alpha Zero agent has won {wins} games\")\n",
    "# Print out the number of games that Alpha Zero agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The Alpha Zero agent has lost {losses} games\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180e3db",
   "metadata": {},
   "source": [
    "The Alpha Zero agent has won all 100 games. The Alpha Zero game strategy plays the Coin game perfectly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
