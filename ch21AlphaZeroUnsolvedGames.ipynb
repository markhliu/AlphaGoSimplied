{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 21: AlphaZero in Unsolved Games\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***“Our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.”***\n",
    "\n",
    "-- David Silver et al,  Mastering the game of Go without human knowledge (Nature 2017) \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "What you'll learn in this chapter:\n",
    "\n",
    "* Implementing AlphaZero in Connect Four \n",
    "* Iteratively training AlphaZero in Connect Four through self-play\n",
    "* Testing AlphaZero against earlier versions of itself\n",
    "* Pitting AlphaZero against AlphaGo\n",
    "\n",
    "In the previous two chapters, we implemented the AlphaZero algorithm in the coin game and Tic Tac Toe. The AlphaZero algorithm was trained purely based on deep reinforcement learning, without human expert data, guidance, or domain knowledge beyond game rules. However, we did use rule-based AI to periodically evaluate the AlphaZero agent to gauge its performance and decide whether or not to stop training. Even though rule-based AI was not used in the training process directly, it was used for testing purposes to monitor the performance of the agent to avoid unnecessary training. \n",
    "\n",
    "However, in unsolved games such as Chess or Go, assessing the performance of AlphaZero is more difficult since we don’t have an algorithm that’s more powerful than AlphaZero to use as the benchmark. How should we test the performance of AlphaZero and decide when to stop training in such cases? That’s the question we are going to answer in this chapter.\n",
    "\n",
    "Even though Connect Four is a solved game, implementing a game-solving rule-based algorithm involves too many steps. Therefore, we treat Connect Four as an unsolved game. In this chapter, you’ll train an AlphaZero agent from scratch in Connect Four. To test the performance of AlphaZero and decide when to stop training, we use an earlier version of AlphaZero as the benchmark. When AlphaZero outperforms an\n",
    "earlier version of itself by a certain margin, we’ll stop a training iteration. We then update the parameters in the older version of AlphaZero and restart the training process. We’ll train the AlphaZero model for several iterations to strengthen the AlphaZero agent.\n",
    "\n",
    "After a couple of days of training, we test the trained AlphaZero agent against the AlphaGo agent we developed in Chapter 18. Our newly trained AlphaZero consistently outperforms its predecessor, AlphaGo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "# 1. Steps to Train AlphaZero in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e50de",
   "metadata": {},
   "source": [
    "## 1.1. Steps to Train AlphaZero in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "## 2.2. A Policy Gradient Network in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b019c91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "num_inputs=42\n",
    "num_actions=7\n",
    "# The convolutional input layer\n",
    "conv_inputs=layers.Input(shape=(7,6,1))\n",
    "conv=layers.Conv2D(filters=128, kernel_size=(4,4),padding=\"same\",\n",
    "     input_shape=(7,6,1), activation=\"relu\")(conv_inputs)\n",
    "flat=layers.Flatten()(conv)\n",
    "# The dense input layer\n",
    "inputs = layers.Input(shape=(42,))\n",
    "# Combine the two into a single input layer\n",
    "two_inputs = tf.concat([flat,inputs],axis=1)\n",
    "# hidden layers\n",
    "common = layers.Dense(256, activation=\"relu\")(two_inputs)\n",
    "common = layers.Dense(64, activation=\"relu\")(common)\n",
    "# Policy output network\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "# The final model\n",
    "model = keras.Model(inputs=[inputs, conv_inputs],\\\n",
    "                    outputs=action) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.00025,\n",
    "                                clipnorm=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0a431",
   "metadata": {},
   "source": [
    "# 2. Prepare to Train AlphaZero in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 2.1. Train the Red and Yellow Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b08faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An old policy gradient network that doesn't update \n",
    "old_model = keras.Model(inputs=[inputs, conv_inputs],\\\n",
    "                    outputs=action)\n",
    "\n",
    "from utils.ch20util import alphazero\n",
    "\n",
    "# define an opponent for the policy gradient agent\n",
    "weight=0.05\n",
    "num_rollouts=50\n",
    "def opponent(env):\n",
    "    move=alphazero(env,weight,old_model,\n",
    "                   num_rollouts=num_rollouts) \n",
    "    return move                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "import numpy as np\n",
    "# allow a maximum of 50 steps per game\n",
    "max_steps=50\n",
    "env=conn()\n",
    "\n",
    "def playing_red():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        conv_state = state.reshape(-1,7,6,1)\n",
    "        # Predict action probabilities and future rewards\n",
    "        action_probs = model([state,conv_state])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(reward)\n",
    "                episode_reward += reward\n",
    "                break\n",
    "            else:\n",
    "                state,reward,done,_=env.step(opponent(env))\n",
    "                rewards_history.append(reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,\\\n",
    "        wrongmoves_history,rewards_history,episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b9c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.95\n",
    "def discount_rs(r,wrong):\n",
    "    discounted_rs = np.zeros(len(r))\n",
    "    running_add = 0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        if wrong[i]==0:  \n",
    "            running_add = gamma*running_add + r[i]\n",
    "            discounted_rs[i] = running_add  \n",
    "    return discounted_rs.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053a34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playing_yellow():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    state, reward, done, _ = env.step(opponent(env))\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        conv_state = state.reshape(-1,7,6,1)\n",
    "        # Predict action probabilities and future rewards\n",
    "        action_probs = model([-state,-conv_state])\n",
    "        # select action based on the policy network\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        # record log probabilities\n",
    "        action_probs_history.append(\\\n",
    "                        tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(-reward)\n",
    "                episode_reward += -reward \n",
    "                break\n",
    "            else:\n",
    "                state,reward,done,_=env.step(opponent(env))\n",
    "                rewards_history.append(-reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += -reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,\\\n",
    "        wrongmoves_history,rewards_history,episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 3.2. Update Parameters in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052e7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20     \n",
    "def create_batch_red(batch_size):\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,wms,rs,er= playing_red()\n",
    "        # rewards are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er)  \n",
    "    return action_probs_history,\\\n",
    "        rewards_history,episode_rewards  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a83221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_yellow(batch_size):\n",
    "    action_probs_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    episode_rewards = []\n",
    "    for i in range(batch_size):\n",
    "        aps,wms,rs,er= playing_yellow()\n",
    "        # reward related to legal moves are discounted\n",
    "        returns = discount_rs(rs,wms)\n",
    "        action_probs_history += aps\n",
    "        # punishments for wrong moves are not discounted\n",
    "        wrongmoves_history += wms\n",
    "        # combined discounted rewards with punishments\n",
    "        combined=np.array(returns)+np.array(wms)\n",
    "        # add combined rewards to rewards history\n",
    "        rewards_history += combined.tolist()\n",
    "        episode_rewards.append(er) \n",
    "    return action_probs_history,\\\n",
    "        rewards_history,episode_rewards                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0d413",
   "metadata": {},
   "source": [
    "# 3 Training AlphaZero in Connect Four\n",
    "\n",
    "## 3.1. The Training Loop in the First Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ec3d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "running_rewards=deque(maxlen=1000)\n",
    "episode_count = 0\n",
    "batches=0\n",
    "# Train the model\n",
    "while True:\n",
    "    with tf.GradientTape() as tape:\n",
    "        if batches%2==0:\n",
    "            action_probs_history,rewards_history,\\\n",
    "                episode_rewards=create_batch_red(batch_size)\n",
    "        else:\n",
    "            action_probs_history,rewards_history,\\\n",
    "                episode_rewards=create_batch_yellow(batch_size)             \n",
    "        # Calculating loss values to update our network        \n",
    "        rets=tf.convert_to_tensor(rewards_history,\\\n",
    "                                  dtype=tf.float32)\n",
    "        alosses=-tf.multiply(tf.convert_to_tensor(\\\n",
    "          action_probs_history,dtype=tf.float32),rets)\n",
    "        # Backpropagation\n",
    "        loss_value = tf.reduce_sum(alosses) \n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads,\\\n",
    "                                  model.trainable_variables))\n",
    "    # Log details\n",
    "    episode_count += batch_size\n",
    "    batches += 1\n",
    "    running_rewards+=episode_rewards\n",
    "    running_reward=np.mean(np.array(running_rewards)) \n",
    "    # print out progress\n",
    "    if episode_count % 100 == 0:\n",
    "        template = \"running reward: {:.6f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))   \n",
    "    # Stop if the game is solved\n",
    "    if running_reward>=0.1 and episode_count>1000:  \n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    if episode_count>25000:  \n",
    "        break    \n",
    "model.save(\"files/CONNzero0.h5\")                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e65561",
   "metadata": {},
   "source": [
    "## 3.2 Train for More Iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc850b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "for i in range(4):\n",
    "    weight=0.1+(i+1)*0.2\n",
    "    num_rollouts=50+(i+1)*50\n",
    "    reload=keras.models.load_model(f\"files/CONNzero{i}.h5\")\n",
    "    model.set_weights(reload.get_weights()) \n",
    "    # update weights in the opponent\n",
    "    old_model.set_weights(reload.get_weights()) \n",
    "    running_rewards=deque(maxlen=1000)\n",
    "    episode_count = 0\n",
    "    batches=0\n",
    "    # Train the model\n",
    "    while True:\n",
    "        with tf.GradientTape() as tape:\n",
    "            if batches%2==0:\n",
    "                action_probs_history,rewards_history,\\\n",
    "                    episode_rewards=create_batch_red(batch_size)\n",
    "            else:\n",
    "                action_probs_history,rewards_history,\\\n",
    "                    episode_rewards=create_batch_yellow(batch_size)             \n",
    "            # Calculating loss values to update our network        \n",
    "            rets=tf.convert_to_tensor(rewards_history,\\\n",
    "                                      dtype=tf.float32)\n",
    "            alosses=-tf.multiply(tf.convert_to_tensor(\\\n",
    "              action_probs_history,dtype=tf.float32),rets)\n",
    "            # Backpropagation\n",
    "            loss_value = tf.reduce_sum(alosses) \n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,\\\n",
    "                                      model.trainable_variables))\n",
    "        # Log details\n",
    "        episode_count += batch_size\n",
    "        batches += 1\n",
    "        running_rewards+=episode_rewards\n",
    "        running_reward=np.mean(np.array(running_rewards)) \n",
    "        # print out progress\n",
    "        if episode_count % 100 == 0:\n",
    "            template = \"running reward: {:.6f} at episode {}\"\n",
    "            print(template.format(running_reward, episode_count))   \n",
    "        # Stop if the game is solved\n",
    "        if running_reward>=0.1 and episode_count>1000:  \n",
    "            print(\"Solved at episode {}!\".format(episode_count))\n",
    "            break\n",
    "        if episode_count>25000:  \n",
    "            break    \n",
    "    model.save(f\"files/CONNzero{i+1}.h5\")             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0265838",
   "metadata": {},
   "source": [
    "## 3.3 Tips for Further Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda387a5",
   "metadata": {},
   "source": [
    "# 4. Test AlphaZero in Connect Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea4f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from utils.ch17util import alphago\n",
    "\n",
    "weight=0.75\n",
    "num_rollouts=584\n",
    "\n",
    "# Load the trained fast policy network from Chapter 11\n",
    "fast_net=keras.models.load_model(\"files/policy_conn.h5\")\n",
    "# Load the policy gradient network from Chapter 15\n",
    "PG_net=keras.models.load_model(\"files/PG_conn.h5\")\n",
    "# Load the trained value network from Chapter 15\n",
    "value_net=keras.models.load_model(\"files/value_conn.h5\")\n",
    "\n",
    "# Define alphago_move\n",
    "def alphago_move(env):\n",
    "    move=alphago(env,weight,45,PG_net,value_net,\n",
    "        fast_net, policy_rollout=False,\n",
    "                 num_rollouts=num_rollouts)\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3eca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Define alphago_move\n",
    "from utils.ch20util import alphazero\n",
    "\n",
    "old_model=keras.models.load_model(\"files/CONNzero4.h5\")\n",
    "def alphazero_move(env):\n",
    "    move=alphazero(env,weight,old_model,\n",
    "                   num_rollouts=num_rollouts) \n",
    "    return move  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13cfa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "env=conn()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset()  \n",
    "    if i%2==0:\n",
    "        action=alphago_move(env)  \n",
    "        state,reward,done,_=env.step(action)        \n",
    "    while True: \n",
    "        # AlphaZero moves\n",
    "        action=alphazero_move(env) \n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            results.append(abs(reward))\n",
    "            break      \n",
    "        # AlphaGo moves\n",
    "        action=alphago_move(env)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            results.append(-abs(reward))\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f951400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AlphaZero agent has won 79 games.\n",
      "The AlphaZero agent has lost 21 games.\n",
      "The game was tied 0 times.\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games that AlphaZero won\n",
    "wins=results.count(1)\n",
    "print(f\"The AlphaZero agent has won {wins} games.\")\n",
    "# Print out the number of games that AlphaZero lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The AlphaZero agent has lost {losses} games.\")\n",
    "# Print out the number of tie games\n",
    "ties=results.count(0)\n",
    "print(f\"The game was tied {ties} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0ae79a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
