{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 20: AlphaGo in Connect Four: Self Play\n",
    "\n",
    "The AlphaGo algorithm combines deep reinforcement learning (namely, the actor-critic method) with traditional rule-based AI (namely, the Monte Carlo Tree Search) to generate intelligent game strategies in Go. In the last two chapters, we have applied this technique to the Coin game and Tic Tac Toe and created perfect or nearly perfect game strategies. \n",
    "\n",
    "In this chapter, you'll apply the same techniques to Connect Four. You'll use self play to make your actor-critic model much stronger. A stronger actor-critic model will lead to a better policy network, which in turn leads to better rollout policies in MCTS, hence better game strategies. \n",
    "\n",
    "By now, you have probably figured out that in reinforcement learning, the stronger the opponent, the stronger the trained agent. Since we have rule-based agents who play games perfectly in both the Coin game and Tic Tac Toe, we were able to train perfect reinforcement learning agents (hence AlphaGo agents) in these two games. In Connect Four, in contrast, building a perfect agent using rule-based AI is too costly. The trained reinforcement learning agent in Connect Four, therefore, is not as strong. To make the RL agent stronger, we let it play against a slightly stronger version of itself through self-play. After many rounds of training, the RL agent becomes much stronger than we started, and that's the idea behind self play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 20}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 20 in a subfolder /files/ch20. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch20\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. Policy-Based MCTS for Connect Four\n",
    "Instead of choosing moves randomly each step, we'll use the trained policy network from Chapter 17 to guide the moves in each step. Intelligent moves lead to more accurate game outcomes, which in turn lead to more accurate position evaluations from game rollouts. \n",
    "\n",
    "In this section, we'll create a policy-based MCTS algorithm in Connect Four. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfde6",
   "metadata": {},
   "source": [
    "## 1.1. Best Moves from the Trained Actor-Critic Model\n",
    "We'll use the trained actor-critic model from Chapter 17 to select moves in game rollouts in MCTS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7961b4",
   "metadata": {},
   "source": [
    "In the local module policy_mcts_conn, we define a AC_conn_stochastic() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52448c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stochastic moves based on the trained models\n",
    "def AC_conn_stochastic(env,model): \n",
    "    state = env.state.reshape(-1,42)\n",
    "    if env.turn==\"red\":\n",
    "        action_probs, critic_value = model(state)\n",
    "    else:\n",
    "        action_probs, critic_value = model(-state)\n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return np.random.choice(sorted(env.validinputs),p=ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e747d",
   "metadata": {},
   "source": [
    "In Chapter 17, we trained one single actor-critic model for both the red and yelllow players. The AC_conn_stochastic() function selects the best move based on the policy network from the trained model. Note we are using the stochastic policy here, meaning that we select the moves randomly based on the probability distribution from the policy network. A determininstic policy will select the move with the highest probability in the distribution instead. Stochastic policy usually leads to better simulation outcomes. It provides exploration naturally. With a deterministic policy, we use only exploitation but not exploration and it's possible that the model gets stuck in the suboptimal strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d70a2",
   "metadata": {},
   "source": [
    "## 1.2. Roll Out A Connect Four Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecf068",
   "metadata": {},
   "source": [
    "Now that we know how to select best moves based on the trained actor-critic model, we'll define a simulate_policy_conn() function in the local module policy_mcts_conn. The function rolls out a game from a certain starting position all the way to the end of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy_conn(env,model,counts,wins,losses):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    # roll out the game till the terminal state\n",
    "    while True:   \n",
    "        move=AC_conn_stochastic(env_copy,model)\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==\"red\") or \\\n",
    "                (reward==-1 and env.turn==\"yellow\"):\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==\"red\") or \\\n",
    "                (reward==1 and env.turn==\"yellow\"):\n",
    "                losses[actions[0]] += 1                \n",
    "            break\n",
    "    return counts,wins,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7136ae",
   "metadata": {},
   "source": [
    "The simulate_policy_conn() function rolls out a Connect Four game and updates the number of game counts and the number of wins and losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a960",
   "metadata": {},
   "source": [
    "We also define a best_move() function in the file policy_mcts_conn.py, which selects the best move based on the nunbers of wins and losses associated with each next move. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4b0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move(counts,wins,losses):\n",
    "    # See which action is most promising\n",
    "    scores={}\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "    best_move=max(scores,key=scores.get)  \n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d714446",
   "metadata": {},
   "source": [
    "The score is defined as the proportions of wins minus the proportions of losses associated with each next move. The function selects the move with the highest score as the best next move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5d06",
   "metadata": {},
   "source": [
    "## 1.3. Policy MCTS in Connect Four\n",
    "Finally, in the local module policy_mcts_conn, we define a policy_mcts_conn() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e682f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mcts_conn(env,model,num_rollouts=50):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0  \n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        counts,wins,losses=simulate_policy_conn(\\\n",
    "                           env,model,counts,wins,losses)\n",
    "    # See which action is most promising\n",
    "    best_next_move=best_move(counts,wins,losses)  \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91376624",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 50. If there is only one legal move left, we skip searching and select the only move available. Otherwise, we create three dicitonaries counts, wins, and losses to record the outcomes from simulated games. Once the simulation is complete, we select the best next move based on the simulation results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db802f09",
   "metadata": {},
   "source": [
    "## 1.4. Test the Policy MCTS in Connect Four\n",
    "Next, we test the policy MCTS agent against the actor-critic agent we developed in Chapter 17. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6b1ad",
   "metadata": {},
   "source": [
    "We use the stochastic strategy and let the policy MCTS agent play against the actor-critic agent for 20 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60eaadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from utils.policy_mcts_conn import policy_mcts_conn\n",
    "from utils.policy_mcts_conn import AC_conn_stochastic\n",
    "\n",
    "\n",
    "model=keras.models.load_model(\"files/ch17/ac_conn.h5\")\n",
    "num_rollouts=200\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset() \n",
    "results=[]\n",
    "for i in range(20):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=AC_conn_stochastic(env,model)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action=policy_mcts_conn(env,model,num_rollouts=num_rollouts)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = AC_conn_stochastic(env,model)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5377a",
   "metadata": {},
   "source": [
    "Half the time, the policy MCTS agent moves first and the other half the actor-critic agent moves first so that no player has an advantage. We record a result of 1 if the policy MCTS agent wins and a result of -1 if the policy MCTS agent loses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a4a6a",
   "metadata": {},
   "source": [
    "We now count how many times the policy MCTS agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb7a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy MCTS agent has won 11 games\n",
      "the policy MCTS agent has lost 9 games\n",
      "there are 0 tied games\n"
     ]
    }
   ],
   "source": [
    "# count how many times the policy MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the policy MCTS agent has won {wins} games\")\n",
    "# count how many times the policy MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the policy MCTS agent has lost {losses} games\")   \n",
    "# count how many tie games\n",
    "ties=results.count(0)\n",
    "print(f\"there are {ties} tied games\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb8050",
   "metadata": {},
   "source": [
    "The above results show that the policy MCTS agent is slightly better than the actor-critic agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "# 2. A Mix MCTS Algorithm in Connect Four\n",
    "While using the policy network to select moves in rollouts is generally better than the UCT formula that we studied in Chapter 9, each method has its own advantages. In particular, the UCT formula allows exploration to avoid repeated rollouts. Therefore, if we combine the policy network with the UCT formula when selecting moves in rollouts, it leads to even better game strategies in Connect Four. \n",
    "\n",
    "## 2.1. A Mixed Formula to Select Moves\n",
    "\n",
    "Recall in Chapter 9, the Upper Confidence Bounds (UCB) formula we used to select moves is as follows:\n",
    "$$UCB=v_i+C\\times \\sqrt{\\frac {logN}{n_i}}$$\n",
    "\n",
    "In the above formula, the value $v_i$ is the estimated value of choosing the next move i. C is a constant that adjusts how much exploration one wants in move selection. N is the total number of times the parent node has been visited, whereas $n_i$ is the number of times that move i has been selected.\n",
    "\n",
    "In policy MCTS, next move is selected based on the recommendation from the policy network in the trained actor-critic model. \n",
    "\n",
    "To combine the policy network with UCT, we'll select the next move based on this formula\n",
    "$$MIX=v_i+C\\times \\sqrt{\\frac {logN}{n_i}}+weight\\times p_i$$\n",
    "\n",
    "where $p_i$ is the probability of selecting move i recommended by the policy network, and $weight$ is a positive constant on how much weight you want to put on the policy network instead of the UCT formula. We can start by setting $weight=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83d76e",
   "metadata": {},
   "source": [
    "## 2.2. Best Moves Based on the Mixed Formula\n",
    "We create a local module for the mix MCTS algorithm. Download the file mix_conn_mcts.py from the book's GitHub repository and save it in /Desktop/ai/utils/ on your computer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b136b48",
   "metadata": {},
   "source": [
    "In the local module mix_conn_mcts, we define a uct_plus_policy() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bfb1d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uct_plus_policy(env_copy,path,paths,temperature,model):\n",
    "    if len(env_copy.validinputs)==1:\n",
    "        return env_copy.validinputs[0]    \n",
    "    # use uct to select move\n",
    "    parent=[]\n",
    "    pathvs=[]\n",
    "    for v in env_copy.validinputs:\n",
    "        pathv=path+str(v)\n",
    "        pathvs.append(pathv)\n",
    "        for p in paths:\n",
    "            if p[0]==pathv:\n",
    "                parent.append(p)\n",
    "    # calculate uct score for each action\n",
    "    ucta=uctb={}\n",
    "    for pathv in pathvs:\n",
    "        history=[p for p in parent if p[0]==pathv]\n",
    "        if len(history)==0:\n",
    "            ucta[pathv]=0\n",
    "            uctb[pathv]=float(\"inf\")\n",
    "        else:\n",
    "            ucta[pathv]=sum([p[1] for p in history])/len(history) \n",
    "            uctb[pathv]=temperature*sqrt(\\\n",
    "                                 log(len(parent))/len(history)) \n",
    "    \n",
    "    ua={int(k[-1]):v for k,v in ucta.items()}\n",
    "    ub={int(k[-1]):v for k,v in uctb.items()}\n",
    "    # probability from the policy network in actor-critic\n",
    "    state = env_copy.state.reshape(-1,42)\n",
    "    if env_copy.turn==\"red\":\n",
    "        action_probs, _= model(state)\n",
    "    else:\n",
    "        action_probs, _= model(-state)     \n",
    "    uctscores={}\n",
    "    # combine UCT with policy network\n",
    "    for a in sorted(env_copy.validinputs):\n",
    "        uctscores[a]=ua.get(a,0)+ub.get(a,0)+\\\n",
    "            np.squeeze(action_probs)[a-1]\n",
    "    return max(uctscores,key=uctscores.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea134c93",
   "metadata": {},
   "source": [
    "The above function uct_plus_policy() determines which move to select at each step when rolling out games. It has two parts: the first part is based on the UCT formula $v_i+C\\times \\sqrt{\\frac {logN}{n_i}}$, and the second part is the probability from the policy network in the trained actor-critic model $weight\\times p_i$. We set the weight on the policy network to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f5dec",
   "metadata": {},
   "source": [
    "## 2.3. Roll Out A Connect Four Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26dc1aa",
   "metadata": {},
   "source": [
    "Now that we know how to select best moves based on the mixed formual, we'll define a mix_simulate_conn() function in the local module mix_conn_mcts. The function rolls out a game from a certain starting position all the way to the end of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28cca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_simulate_conn(env,paths,counts,wins,losses,\\\n",
    "                     temperature,model):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    path=\"\"\n",
    "    while True:\n",
    "        utc_move=uct_plus_policy(env_copy,path,paths,\\\n",
    "                                 temperature,model)\n",
    "        move=deepcopy(utc_move)\n",
    "        actions.append(move)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        path += str(move)\n",
    "        if done:\n",
    "            result=0\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==\"red\") or \\\n",
    "                (reward==-1 and env.turn==\"yellow\"):\n",
    "                result=1\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==\"red\") or \\\n",
    "                (reward==1 and env.turn==\"yellow\"):\n",
    "                losses[actions[0]] += 1  \n",
    "                result=-1\n",
    "            break\n",
    "    return result,path,counts,wins,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69a846",
   "metadata": {},
   "source": [
    "The function rolls out a Connect Four game and updates the number of game counts and the number of wins and losses. After each game, we update the results using the backpropagate() function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b78c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagate\n",
    "def backpropagate(path,result,paths):\n",
    "    while path != \"\":\n",
    "        paths.append((path,result))\n",
    "        path=path[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f49efc5",
   "metadata": {},
   "source": [
    "We also define a best_move() function in the file mix_conn_mcts.py, which selects the best move based on the nunbers of wins and losses associated with each next move. The function is the same as that defined in the file policy_mcts_conn.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742c448",
   "metadata": {},
   "source": [
    "## 2.4. A Mix MCTS Algorithm in Connect Four\n",
    "Finally, in the local module mix_conn_mcts, we define a mix_mcts_conn() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c60a97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_mcts_conn(env,model,num_rollouts=50,temperature=1.4):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0\n",
    "    paths=[]    \n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        result,path,counts,wins,losses=mix_simulate_conn(\\\n",
    "             env,paths,counts,wins,losses,temperature,model)      \n",
    "        # backpropagate\n",
    "        backpropagate(path,result,paths)\n",
    "    # See which action is most promising\n",
    "    best_next_move=best_move(counts,wins,losses) \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fdc83",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 50. If there is only one legal move left, we skip searching and select the only move available. Otherwise, we create three dicitonaries counts, wins, and losses to record the outcomes from simulated games. Once the simulation is complete, we select the best next move based on the simulation results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e0986",
   "metadata": {},
   "source": [
    "## 2.5. Test the Mix MCTS in Connect Four\n",
    "We will let the mix MCTS agent play against the UCT MCTS agent for 100 games and see which agent is stronger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006470cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mix_conn_mcts import mix_mcts_conn\n",
    "\n",
    "num_rollouts=200\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset() \n",
    "results=[]\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=AC_conn_stochastic(env,model)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action=mix_mcts_conn(env,model,num_rollouts=num_rollouts)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = AC_conn_stochastic(env,model)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47310c",
   "metadata": {},
   "source": [
    "Half the time, the mix MCTS agent moves first and the other half the actor-critic agent moves firsts so that no player has the first-mover's advantage. We record a result of 1 if the mix MCTS agent wins and a result of -1 if the actor-critic agent wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab103f9",
   "metadata": {},
   "source": [
    "We now count how many times the mix MCTS agent has won and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mix MCTS agent has won 12 games\n",
      "the mix MCTS agent has lost 7 games\n",
      "the game has tied 1 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the mix MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the mix MCTS agent has won {wins} games\")\n",
    "# count how many times the mix MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the mix MCTS agent has lost {losses} games\")  \n",
    "# count how many tie games\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f454f",
   "metadata": {},
   "source": [
    "The above results show that the mix MCTS agent is slightly better than the actor-critic agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35951ed",
   "metadata": {},
   "source": [
    "# 3. The Idea of Self Play\n",
    "We implement self play in this section. Specifically, we build a stonger version of a RL agnet by enhancing it through a mixed MCTS algorithm that we discussed in Chapter 19. \n",
    "\n",
    "we let a RL agent play against a slightly stronger version of itself through self-play. The stronger version is the mix MCTS agent with 50 rollouts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ac2d0",
   "metadata": {},
   "source": [
    "To implement self play, we follow the steps we used in Chapter 17 when training the actor-critc agent. The difference here is that in Chapter 17, the opponent is the look-three-steps-ahead rule-based AI agent. In self play, we'll change the opponent to the mix MCTS agent who selects moves by using the UCT formula and the probability recommended by its own policy network. In a way, the opponent is a moving target since as the actor-critic agent becomes stronger, the opponent also becomes stronger. \n",
    "\n",
    "Specifically, we'll create a local module selfplay in the local package to train the selfplay agent. Download the file selfplay.py from the book's GitHub page and save it in the folder /Desktop/ai/utils/ on your computer. Below, we'll highlight the changes we have made compared to what we have done in Chapter 17. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e312e9d5",
   "metadata": {},
   "source": [
    "## 3.1. Load the Trained Model\n",
    "We'll not create a model from scratch. Instead, we'll use the model we have trained in Chapter 7 and further train it using self play. Therefore, in the local module selfplay.py, we load up the model that we have saved in Chapter 17 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "383393e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "import numpy as np\n",
    "from utils.conn_policy_mcts import policy_mcts_conn\n",
    "from tensorflow import keras\n",
    "from utils.mix_policy_mcts42 import mix_mcts_conn\n",
    "\n",
    "num_inputs = 42\n",
    "num_actions = 7\n",
    "model=keras.models.load_model(\"files/ch17/ac_conn.h5\")   \n",
    "env=conn()\n",
    "num_rollouts=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1def0",
   "metadata": {},
   "source": [
    "The saved model ac_conn.h5 is loaded up and is now the model we are training. We set the number of rollouts in the mix MCTS algorithm to 50 to save time: if this number is too low, we cannot further improve the model; if the number is too large, it takes too long to train the model. \n",
    "\n",
    "Below, we specify the optimizer and the loss function we use to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68df75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "loss_func = keras.losses.MeanAbsoluteError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7398c385",
   "metadata": {},
   "source": [
    "The optimizer is Adam with a learning rate of 0.0005. The loss function is the mean absolute error loss function, which punishes outliner less compared to other loss functions such as Huber or mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6d496",
   "metadata": {},
   "source": [
    "## 3.2. Train Players in Connect Four\n",
    "We'll let the actor-critic agent play against the mix MCTS algorithm we developed in the las section. In the local module selfplay.py, we'll define a playing_red() function. The playing_red() function simulates a full game, with the mix MCTS agent as the second player, and the actor-critic (AC) agent as the first player, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dd9f55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps=50\n",
    "def playing_red():\n",
    "    # create lists to record game history\n",
    "    action_probs_history = []\n",
    "    critic_value_history = []\n",
    "    wrongmoves_history = []\n",
    "    rewards_history = []\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        state = state.reshape(-1,42,)\n",
    "        action_probs, critic_value = model(state)\n",
    "        critic_value_history.append(critic_value[0, 0])\n",
    "        action=np.random.choice(num_actions,\\\n",
    "                                p=np.squeeze(action_probs))\n",
    "        action_probs_history.append(\\\n",
    "                    tf.math.log(action_probs[0, action]))\n",
    "        # punish the agent if there is an illegal move\n",
    "        if action+1 not in env.validinputs:\n",
    "            rewards_history.append(0)\n",
    "            wrongmoves_history.append(-1)\n",
    "            #episode_reward += -1 \n",
    "        # otherwise, place the move on the game board\n",
    "        else:              \n",
    "        # Apply the sampled action in our environment\n",
    "            state, reward, done, _ = env.step(action+1)\n",
    "            if done:\n",
    "                wrongmoves_history.append(0)\n",
    "                rewards_history.append(reward)\n",
    "                episode_reward += reward \n",
    "                break\n",
    "            else:\n",
    "                state, reward, done, _ = env.step(\\\n",
    "          mix_mcts_conn(env,model,num_rollouts=num_rollouts))\n",
    "                rewards_history.append(reward)\n",
    "                wrongmoves_history.append(0)\n",
    "                episode_reward += reward                 \n",
    "                if done:\n",
    "                    break                \n",
    "    return action_probs_history,critic_value_history, \\\n",
    "            wrongmoves_history,rewards_history, episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387db1da",
   "metadata": {},
   "source": [
    "We define another function playing_yellow() function similarly. The playing_yellow() function simulates a full game, with the mix MCTS agent as the first player, and the actor-critic (AC) agent as the second player. As we did in Chapter 17, we use *-state* instead of *state* when we feed the game board to the model. Instead of using 1 and -1 to denote a win by the red and yellow player, respectively, we'll use a reward of 1 to denote the current player has won and a reward of -1 to denote that the current player has lost. We accomplish this by use *-reward* instead of *reward* in the above code cell. Therefore, after the yellow player makes a move, the reward is now 1 if the yellow player wins and -1 if the yellow player loses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156ac04",
   "metadata": {},
   "source": [
    "## 3.3. Keep Track of Progress\n",
    "Self plays are extremely costly in terms of computational needs. Therefore, it takes a long time to train the model. To make things more complicated, we need to tune the hyper parameters to make the model works (such as the larning rate, the loss function, and so on). Hence it's important to mesure the progress of the model so that you know things are going in the right dirction for you.\n",
    "\n",
    "To do that, we'll test the model against the rule-based AI on the side to determine the progress. Note that the rule-based AI is not involved in training hte model directly. It just checks how good the model is periodially without interfereing with teh model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39fdf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.conn_think3 import red_think3\n",
    "from utils.conn_policy_mcts import DL_stochastic\n",
    "\n",
    "def one_game():\n",
    "    state=env.reset()   \n",
    "    while True:  \n",
    "        action = red_think3(env) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:       \n",
    "            return reward \n",
    "        action = DL_stochastic(env,model) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            return reward\n",
    "\n",
    "from collections import deque\n",
    "testresults=deque(maxlen=10000)\n",
    "def test():\n",
    "    for i in range(100):\n",
    "        testresults.append(-one_game())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cd09207",
   "metadata": {},
   "source": [
    "Periodically, we test the model against the look-three-steps-ahead AI on the side for 100 games to check the progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fef0043",
   "metadata": {},
   "source": [
    "## 3.4. Train the Self-Play Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0d413",
   "metadata": {},
   "source": [
    "To alternate between training the red player and training the yellow player, we'll create a variable *batches*. It starts with a value of 1. After each batch of training, we add 1 to the value of the variable *batches*. We'll train the red player when the value of *batches* is even and train the yellow player otherwise. \n",
    "\n",
    "We train the model for 300 episodes of games. For that purpose, we define the selfplay_conn() function in the local module selfplay.py, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fc850b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfplay_conn():\n",
    "    episode_count = 0\n",
    "    batches=0    \n",
    "    while episode_count<=300:\n",
    "        if batches%2==0:\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probs_history,critic_value_history,\\\n",
    "    rewards_history,episode_rewards=create_batch_red(batch_size)                      \n",
    "        else:\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probs_history,critic_value_history,\\\n",
    "rewards_history,episode_rewards=create_batch_yellow(batch_size)                              \n",
    "            # Calculating loss values to update our network        \n",
    "            tfdif=tf.convert_to_tensor(rewards_history,\\\n",
    "                                       dtype=tf.float32)-\\\n",
    "    tf.convert_to_tensor(critic_value_history,dtype=tf.float32)\n",
    "            alosses=-tf.multiply(tf.convert_to_tensor(\\\n",
    "                  action_probs_history, dtype=tf.float32),tfdif)\n",
    "            closs=loss_func(tf.convert_to_tensor(rewards_history,\\\n",
    "                                             dtype=tf.float32),\\\n",
    "     tf.convert_to_tensor(critic_value_history,dtype=tf.float32))\n",
    "            # Backpropagation\n",
    "            loss_value = tf.reduce_sum(alosses) + closs\n",
    "        grads = tape.gradient(loss_value,\\\n",
    "                              model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads,\\\n",
    "                                      model.trainable_variables))    \n",
    "        # Log details\n",
    "        episode_count += batch_size\n",
    "        batches += 1\n",
    "        if episode_count % 20 == 0:\n",
    "            model.save(f\"files/ch20/selfplay_conn.h5\") \n",
    "            # check progress\n",
    "            test()\n",
    "            avg=np.array(testresults).mean()\n",
    "            print(f\"score {avg:.6f} at episode {episode_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0f718",
   "metadata": {},
   "source": [
    "Now we can call the selfplay_conn() function from the local module to train the model through self play, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb3336b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.selfplay import selfplay_conn\n",
    "\n",
    "selfplay_conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2ef29",
   "metadata": {},
   "source": [
    "It takes about several hours to train the model. Once done, the model is saved as selfplay_conn.h5 on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbc623",
   "metadata": {},
   "source": [
    "# 4. Test the Self-Play Trained Model\n",
    "In this section, we test the self-play trained model and compare it to the model that we trained in Chapter 17. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edf0110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model=keras.models.load_model(\"files/ch17/ac_conn.h5\")\n",
    "new_model=keras.models.load_model(\"files/ch20/selfplay_conn.h5\")\n",
    "\n",
    "def DL(env,model): \n",
    "    state = env.state.reshape(-1,42)\n",
    "    if env.turn==\"red\":\n",
    "        action_probs, critic_value = model(state)\n",
    "    else:\n",
    "        action_probs, critic_value = model(-state)\n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return np.random.choice(sorted(env.validinputs),p=ps)\n",
    "\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    env=conn()\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=DL(env,old_model)\n",
    "        state, reward, done, info = env.step(action)        \n",
    "    while True:    \n",
    "        action=DL(env,new_model)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # if the new model wins, record 1\n",
    "            if reward!=0:\n",
    "                results.append(1)\n",
    "            else:\n",
    "                results.append(0)\n",
    "            break\n",
    "        action=DL(env,old_model) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45b8697",
   "metadata": {},
   "source": [
    "We create an empty list results. We play 100 games and in 50 of them, the new model moves first; in the other 50 games, the old model moves first. If the new model wins, we add an outcome 1 to results. If the old model wins, we add an outcome of -1 to results. If the game is tied, we add a 0 to results.\n",
    "\n",
    "We can now count how many games the new model has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a186977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new model has won 55 games\n",
      "The new model has lost 42 games\n",
      "There are 3 tied games\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of games the new model won\n",
    "wins=results.count(1)\n",
    "print(f\"The new model has won {wins} games\")\n",
    "# Print out the number of games the new model lost\n",
    "losses=results.count(-1)\n",
    "print(f\"The new model has lost {losses} games\")              \n",
    "# Print out the number of tie games\n",
    "ties=results.count(0)\n",
    "print(f\"There are {ties} tied games\")     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f666b",
   "metadata": {},
   "source": [
    "The results show that the new model has won 55 games, lost 42 games, and the remaining 3 games are tied. We can see a significant improvement in the model due to self play. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d80ece",
   "metadata": {},
   "source": [
    "# The AlphaGo Agent in Connect Four\n",
    "To conclude this chapter, we'll create an AlphaGo agent in Connect Four. The agent will use mix MCTS algorithm to come up with moves when playing a game. We'll have two versions of the AlphaGo agent: a fast version in which the agent rolls out 50 games before making a move, and a strong version in which the agent rolls out 200 games before making a move. The fast version makes a move quickly but slightly weaker. The strong version takes longer to make a move but makes more intelligent moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0f14dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.load_model(\"files/ch20/slefplay_conn.h5\")\n",
    "from utils.mix_conn_mcts import mix_mcts_conn\n",
    "def AlphaGo_conn_fast(env,model):\n",
    "    return mix_mcts_conn(env,model,num_rollouts=50)  \n",
    "\n",
    "def AlphaGo_conn_strong(env,model):\n",
    "    return mix_mcts_conn(env,model,num_rollouts=200)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f52e69",
   "metadata": {},
   "source": [
    "Below, I play a full game with the fast version of the AlphaGo agent, and I timed how long it takes the agent to make a move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4c6bfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\.conda\\envs\\deepq\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "it took AlphaGo 3.649463176727295 seconds\n",
      "AlphaGo drops a disc in column 4\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "enter a move:4\n",
      "it took AlphaGo 1.8945810794830322 seconds\n",
      "AlphaGo drops a disc in column 3\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]]\n",
      "enter a move:4\n",
      "it took AlphaGo 1.1149749755859375 seconds\n",
      "AlphaGo drops a disc in column 2\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  1  1  1  0  0  0]]\n",
      "enter a move:5\n",
      "it took AlphaGo 0.5531809329986572 seconds\n",
      "AlphaGo drops a disc in column 1\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 1  1  1  1 -1  0  0]]\n",
      "the AlphaGo agent has won!\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "env=conn()\n",
    "state=env.reset() \n",
    "while True:\n",
    "    start=time()\n",
    "    action=AlphaGo_conn_fast(env,model)  \n",
    "    print(f\"it took AlphaGo {time()-start} seconds\")\n",
    "    print(f\"AlphaGo drops a disc in column {action}\")\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{env.state.T[::-1]}\")\n",
    "    if done:\n",
    "        print(\"the AlphaGo agent has won!\")    \n",
    "        break  \n",
    "    action = int(input(\"enter a move:\"))   \n",
    "    state, reward, done, info = env.step(action)  \n",
    "    if done:\n",
    "        # result is 1 if the MCTS agent wins\n",
    "        if reward!=0:\n",
    "            print(\"the human player has won!\") \n",
    "        else:\n",
    "            print(\"tie game\")    \n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735a5bf",
   "metadata": {},
   "source": [
    "The agent is fast but powerful. It takes just a couple of seconds for the AlphaGo agent to make a move, but it wins by creating a double attack horizontally. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
