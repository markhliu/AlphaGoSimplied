{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 8: Monte Carlo Tree Search\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "*“Our program AlphaGo combined these deep neural networks with a state-of-the-art tree search. In October 2015, AlphaGo became the first program to defeat a professional human player.”*\n",
    "\n",
    "-- David Silver\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "What you'll learn in this chapter:\n",
    "\n",
    "* Monte Carlo Tree Search (MCTS) and Upper Confidence Bounds for Trees (UCT)  \n",
    "* Implementing a naive MCTS algorithm in the Coin game\n",
    "* Breaking down the four steps in MCTS: select, expand, simulate, and backpropagate\n",
    "* Designing a generic MCTS game strategy that can apply to the Coin game, Tic Tac Toe, and Connect Four\n",
    "\n",
    "\n",
    "\n",
    "So far we have covered one type of tree search: MiniMax tree search. In\n",
    "simple games such as Last Coin Standing or Tic Tac Toe, MiniMax agents\n",
    "solve the game and provide game strategies that are as good as any other intelligent\n",
    "game strategies. In complicated games such as Connect Four or Chess, the number of\n",
    "possibilities is too large and it’s infeasible for the MiniMax agent to solve the game\n",
    "in a short amount of time. We therefore use depth pruning and alpha-beta pruning,\n",
    "combined with powerful position evaluation functions, to help MiniMax agents come\n",
    "up with intelligent moves in the allotted time. With such an approach, Deep Blue\n",
    "beat the world Chess champion Gary Kasparov in 1997.\n",
    "\n",
    "While in games such as Chess, position evaluation functions are relatively accurate,\n",
    "in certain games such as Go, evaluating positions is more challenging. For example,\n",
    "in Chess, if white has an extra rook than black, it’s difficult for the black to win or\n",
    "tie the game. We are fairly certain that white will win without following a game tree\n",
    "all the way to the end. In Go, on the other hand, guessing who will eventually win\n",
    "in the mid-game based on board positions is not as simple. Counting the number of\n",
    "stones for each side can provide a clue, but this can change in an instant if one side\n",
    "captures a large number of the opponent’s stones.\n",
    "\n",
    "Therefore, in the game of Go, researchers usually use another type of tree search:\n",
    "Monte Carlo Tree Search (MCTS). In depth-pruned MiniMax tree search, the agent\n",
    "searches all possible outcomes to a fixed number of moves ahead, this is sometimes\n",
    "called a breadth-first approach. In contrast, in MCTS, the agent simulates games all\n",
    "the way to the terminal state to see the game outcome. It doesn’t cover all scenarios.\n",
    "MCTS, therefore, is a depth-first approach.\n",
    "\n",
    "The idea behind MCTS is to roll out random games starting from the current game\n",
    "state and see what the average game outcome is. If you roll out, say 1000, games from\n",
    "this point, and Player 1 wins 99 percent of the time, you know that the current game\n",
    "state must favor Player 1 over Player 2. To select the best next move, the MCTS\n",
    "algorithm uses the Upper Confidence Bounds for Trees (UCT) method. We’ll break\n",
    "down the process into four steps: select, expand, simulate, and backpropagate. You’ll\n",
    "implement an MCTS algorithm that can be applied to the three games in this book:\n",
    "the coin game, Tic Tac Toe, and Connect Four.\n",
    "\n",
    "As the opening quote of this chapter states, DeepMind combines deep learning with\n",
    "MCTS when designing game strategies for AlphaGo, which led to the algorithm’s\n",
    "great success. The “state-of-the-art tree search” David Silver mentions is an improved\n",
    "version of MCTS. After learning the basics of MCTS, you’ll be ready to integrate\n",
    "MCTS with other algorithms to create superhuman agents in various games,\n",
    "as you’ll learn to do for the rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. What Is Monte Carlo Tree Search?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. A Thought Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1337aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the current state is 21\n",
      "Player 1 has chosen action=1\n",
      "the current state is 20\n"
     ]
    }
   ],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "\n",
    "# Initiate the game environment\n",
    "env = coin_game()\n",
    "state=env.reset()   \n",
    "print(f\"the current state is {state}\")\n",
    "# Player 1 takes one coin from the pile\n",
    "player1_move=1\n",
    "print(f\"Player 1 has chosen action={player1_move}\") \n",
    "state, reward, done, info = env.step(player1_move)\n",
    "print(f\"the current state is {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a76d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dictionary counts has values {1: 0, 2: 0}\n",
      "the dictionary wins has values {1: 0, 2: 0}\n",
      "the dictionary losses has values {1: 0, 2: 0}\n"
     ]
    }
   ],
   "source": [
    "counts={}\n",
    "wins={}\n",
    "losses={}\n",
    "for move in env.validinputs:\n",
    "    counts[move]=0\n",
    "    wins[move]=0\n",
    "    losses[move]=0\n",
    "print(f\"the dictionary counts has values {counts}\")   \n",
    "print(f\"the dictionary wins has values {wins}\")    \n",
    "print(f\"the dictionary losses has values {losses}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0b8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# simulate 10,000 games\n",
    "for _ in range(100000):\n",
    "    # create a deep copy of the game environment\n",
    "    env_copy=deepcopy(env)\n",
    "    # record moves\n",
    "    actions=[]\n",
    "    # play a full game\n",
    "    while True:\n",
    "        # randomly select a next move\n",
    "        move=env.sample()\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            # see whehter the enxt move is 1 or 2\n",
    "            next_move=deepcopy(actions[0])\n",
    "            # update total number of simulated games\n",
    "            counts[actions[0]] += 1\n",
    "            # update total number of wins\n",
    "            if (reward==1 and env.turn==1) or \\\n",
    "                (reward==-1 and env.turn==2):\n",
    "                wins[actions[0]] += 1\n",
    "            # update total number of losses                \n",
    "            if (reward==-1 and env.turn==1) or \\\n",
    "                (reward==1 and env.turn==2):\n",
    "                losses[actions[0]] += 1                \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47d6a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dictionary counts has values {1: 49986, 2: 50014}\n",
      "the dictionary wins has values {1: 25022, 2: 25191}\n",
      "the dictionary losses has values {1: 24964, 2: 24823}\n"
     ]
    }
   ],
   "source": [
    "print(f\"the dictionary counts has values {counts}\")   \n",
    "print(f\"the dictionary wins has values {wins}\")    \n",
    "print(f\"the dictionary losses has values {losses}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5dfac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.0011603248909694715, 2: 0.007357939776862479}\n",
      "the best move is 2\n"
     ]
    }
   ],
   "source": [
    "# See which action is most promising\n",
    "scores={}\n",
    "for k,v in counts.items():\n",
    "    scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "print(scores)\n",
    "best_move=max(scores,key=scores.get)  \n",
    "print(f\"the best move is {best_move}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 1.2. A Naive MCTS Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43004c01",
   "metadata": {},
   "source": [
    "```python\n",
    "def simulate_a_game(env,counts,wins,losses):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    # play a full game\n",
    "    while True:\n",
    "        #randomly select a next move\n",
    "        move=random.choice(env_copy.validinputs)\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==1) or \\\n",
    "                (reward==-1 and env.turn==2):\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==1) or \\\n",
    "                (reward==1 and env.turn==2):\n",
    "                losses[actions[0]] += 1                \n",
    "            break\n",
    "    return counts, wins, losses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7961b4",
   "metadata": {},
   "source": [
    "In the file *ch08util.py*, we also define a *best_move()* function as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44003de3",
   "metadata": {},
   "source": [
    "```python\n",
    "def best_move(counts,wins,losses):\n",
    "    # See which action is most promising\n",
    "    scores={}\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "    return max(scores,key=scores.get)  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e851bb",
   "metadata": {},
   "source": [
    "Finally, we define the *naive_mcts()* function in *ch08util.py* as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a7b73",
   "metadata": {},
   "source": [
    "```python\n",
    "def naive_mcts(env, num_rollouts=10000):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0\n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        counts,wins,losses=simulate_a_game(env,counts,\\\n",
    "                                           wins, losses)\n",
    "    return best_move(counts,wins,losses) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "# 2. A Naive MCTS Player in the Coin Game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ef98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch08util import naive_mcts\n",
    "\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    env=coin_game()\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=env.sample()\n",
    "        state, reward, done, info=env.step(action)\n",
    "    while True:\n",
    "        action=naive_mcts(env,num_rollouts=1000)  \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            results.append(1) \n",
    "            break  \n",
    "        action=env.sample()   \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the MCTS agent loses\n",
    "            results.append(-1) \n",
    "            break              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MCTS agent has won 96 games\n",
      "the MCTS agent has lost 4 games\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MCTS agent has lost {losses} games\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59e2a6",
   "metadata": {},
   "source": [
    "# 3. Upper Confidence Bounds for Trees (UCT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfaa96",
   "metadata": {},
   "source": [
    "## 3.1. The UCT Formala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f693d",
   "metadata": {},
   "source": [
    "## 3.2. An MCTS Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3964a1f",
   "metadata": {},
   "source": [
    "```python\n",
    "from math import sqrt, log\n",
    "\n",
    "def select(env,counts,wins,losses,temperature):\n",
    "    # calculate the uct score for all next moves\n",
    "    scores={}\n",
    "    # the ones not visited get the priority\n",
    "    for k in env.validinputs:\n",
    "        if counts[k]==0:\n",
    "            return k\n",
    "    # total number of simulations conducted\n",
    "    N=sum([v for k,v in counts.items()])\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            # vi for each next move\n",
    "            vi=(wins.get(k,0)-losses.get(k,0))/v\n",
    "            exploration=temperature*sqrt(log(N)/counts[k])\n",
    "            scores[k]=vi+exploration\n",
    "    # Select the next move with the highest UCT score\n",
    "    return max(scores,key=scores.get)   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f097f7",
   "metadata": {},
   "source": [
    "```python\n",
    "def expand(env,move):\n",
    "    env_copy=deepcopy(env)\n",
    "    state,reward,done,info=env_copy.step(move)\n",
    "    return env_copy, done, reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1d54a",
   "metadata": {},
   "source": [
    "```python\n",
    "def simulate(env_copy,done,reward):\n",
    "    # if the game has already ended\n",
    "    if done==True:\n",
    "        return reward\n",
    "    while True:\n",
    "        move=env_copy.sample()\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done==True:\n",
    "            return reward\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2061f67",
   "metadata": {},
   "source": [
    "```python\n",
    "def backpropagate(env,move,reward,counts,wins,losses):\n",
    "    # add 1 to the total game counts\n",
    "    counts[move]=counts.get(move,0)+1\n",
    "    # if the current player wins\n",
    "    if reward==1 and (env.turn==1 or \\\n",
    "        env.turn==\"X\" or env.turn==\"red\"):\n",
    "        wins[move]=wins.get(move,0)+1\n",
    "    if reward==-1 and (env.turn==2 or \\\n",
    "        env.turn==\"O\" or env.turn==\"yellow\"):\n",
    "        wins[move]=wins.get(move,0)+1        \n",
    "    if reward==-1 and (env.turn==1 or \\\n",
    "        env.turn==\"X\" or env.turn==\"red\"):\n",
    "        losses[move]=losses.get(move,0)+1\n",
    "    if reward==1 and (env.turn==2 or \\\n",
    "        env.turn==\"O\" or env.turn==\"yellow\"):\n",
    "        losses[move]=losses.get(move,0)+1       \n",
    "    return counts,wins,losses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c7c11",
   "metadata": {},
   "source": [
    "```python\n",
    "def next_move(counts,wins,losses):\n",
    "    # See which action is most promising\n",
    "    scores={}\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "    return max(scores,key=scores.get)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e4b864",
   "metadata": {},
   "source": [
    "Finally, we define the *mcts()* function in the local module:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229de34",
   "metadata": {},
   "source": [
    "```python\n",
    "def mcts(env,num_rollouts=100,temperature=1.4):\n",
    "    # if there is only one valid move left, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # create three dictionaries counts, wins, losses\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0\n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        # selection\n",
    "        move=select(env,counts,wins,losses,temperature)\n",
    "        # expansion\n",
    "        env_copy, done, reward=expand(env,move)\n",
    "        # simulation\n",
    "        reward=simulate(env_copy,done,reward)      \n",
    "        # backpropagate\n",
    "        counts,wins,losses=backpropagate(\\\n",
    "            env,move,reward,counts,wins,losses)\n",
    "    # make the move\n",
    "    return next_move(counts,wins,losses)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9084e2e",
   "metadata": {},
   "source": [
    "# 4. Test the MCTS Agent in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f8f81",
   "metadata": {},
   "source": [
    "## 4.1. Manually Play against the MCTS Agent in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77c6b520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen 5\n",
      "Current state is \n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "What's your move?1\n",
      "Player O has chosen 1\n",
      "Current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  0]]\n",
      "Player X has chosen 3\n",
      "Current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  1]]\n",
      "What's your move?7\n",
      "Player O has chosen 7\n",
      "Current state is \n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  1]]\n",
      "Player X has chosen 4\n",
      "Current state is \n",
      "[[-1  0  0]\n",
      " [ 1  1  0]\n",
      " [-1  0  1]]\n",
      "What's your move?6\n",
      "Player O has chosen 6\n",
      "Current state is \n",
      "[[-1  0  0]\n",
      " [ 1  1 -1]\n",
      " [-1  0  1]]\n",
      "Player X has chosen 2\n",
      "Current state is \n",
      "[[-1  0  0]\n",
      " [ 1  1 -1]\n",
      " [-1  1  1]]\n",
      "What's your move?8\n",
      "Player O has chosen 8\n",
      "Current state is \n",
      "[[-1 -1  0]\n",
      " [ 1  1 -1]\n",
      " [-1  1  1]]\n",
      "Player X has chosen 9\n",
      "Current state is \n",
      "[[-1 -1  1]\n",
      " [ 1  1 -1]\n",
      " [-1  1  1]]\n",
      "Game over, it's a tie!\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "from utils.ch08util import mcts\n",
    "\n",
    "env=ttt()\n",
    "state=env.reset() \n",
    "while True:\n",
    "    action=mcts(env,num_rollouts=10000)\n",
    "    print(f\"Player {env.turn} has chosen {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player {env.turn} has won!\") \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")    \n",
    "        break  \n",
    "    action=int(input(\"What's your move?\"))\n",
    "    print(f\"Player {env.turn} has chosen {action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        print(f\"Player {env.turn} has won!\")  \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e0986",
   "metadata": {},
   "source": [
    "## 4.2. Effectiveness of the Tic Tac Toe MCTS Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c8e35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=env.sample()\n",
    "        state, reward, done, info=env.step(action)\n",
    "    while True:\n",
    "        action=mcts(env,num_rollouts=10000)  \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action=env.sample()   \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the random-move agent wins\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ee606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MCTS agent has won 96 games\n",
      "the MCTS agent has lost 1 games\n",
      "the game is tied 3 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MCTS agent has lost {losses} games\")  \n",
    "# count how many tie games\n",
    "losses=results.count(0)\n",
    "print(f\"the game is tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c826b4",
   "metadata": {},
   "source": [
    "# 5. An MCTS Agent in Connect Four\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69390c",
   "metadata": {},
   "source": [
    "## 5.1. A Manual Game against the Connect Four MCTS Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a93930e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player red has chosen 4\n",
      "Current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "What's your move?4\n",
      "Player yellow has chosen 4\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0  1  0  0  0]]\n",
      "Player red has chosen 3\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]]\n",
      "What's your move?4\n",
      "Player yellow has chosen 4\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  0  0  0]]\n",
      "Player red has chosen 5\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  1  1  1  0  0]]\n",
      "What's your move?5\n",
      "Player yellow has chosen 5\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0]\n",
      " [ 0  0  1  1  1  0  0]]\n",
      "Player red has chosen 2\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0 -1  0  0  0]\n",
      " [ 0  0  0 -1 -1  0  0]\n",
      " [ 0  1  1  1  1  0  0]]\n",
      "Player red has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset() \n",
    "while True:\n",
    "    action=mcts(env,num_rollouts=5000)\n",
    "    print(f\"Player {env.turn} has chosen {action}\")    \n",
    "    state, reward, done, info=env.step(action)\n",
    "    print(f\"Current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        print(\"Player red has won!\") \n",
    "        break\n",
    "    action=int(input(\"What's your move?\"))\n",
    "    print(f\"Player {env.turn} has chosen {action}\")    \n",
    "    state, reward, done, info=env.step(action)\n",
    "    print(f\"Current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward!=0:\n",
    "            print(\"Player yellow has won!\") \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")    \n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c97da9",
   "metadata": {},
   "source": [
    "## 5.2. Effectiveness of the Connect Four MCTS Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c70da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=env.sample()\n",
    "        state, reward, done, info=env.step(action)\n",
    "    while True:\n",
    "        action=mcts(env,num_rollouts=5000)  \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            results.append(abs(reward)) \n",
    "            break  \n",
    "        action=env.sample()  \n",
    "        state, reward, done, info=env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the MCTS agent loses\n",
    "            results.append(-abs(reward)) \n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572f5877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MCTS agent has won 100 games\n",
      "the MCTS agent has lost 0 games\n",
      "the game is tied 0 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MCTS agent has lost {losses} games\")  \n",
    "# count how many tie games\n",
    "losses=results.count(0)\n",
    "print(f\"the game is tied {losses} times\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
