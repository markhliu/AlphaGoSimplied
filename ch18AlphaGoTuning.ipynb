{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de3a7f85",
   "metadata": {},
   "source": [
    "# Chaper 18: Hyperparameter Tuning in AlphaGo\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***“Currently, most of the job of a deep-learning engineer consists of munging data with Python scripts and then tuning the architecture and hyperparameters of a deep network at length to get a working model—or even to get a state-of-the-art model, if the engineer is that ambitious.”***\n",
    "\n",
    "-- Francois Chollet, creator of the Keras deep-learning library \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In Chapters 16 and 17, we applied a simplified version of the AlphaGo algorithm to the Coin game and Tic Tac Toe. This integration of Monte Carlo Tree Search (MCTS) with deep reinforcement learning enabled the AlphaGo-based agent to effectively solve both games. Notably, in the Coin game, the AlphaGo agent consistently won when playing second, even against opponents employing flawless\n",
    "strategies. In Tic Tac Toe, it consistently achieved draws against the MiniMax agent, which made perfect moves in each game.\n",
    "\n",
    "The focus of this chapter is on adapting the AlphaGo algorithm for Connect Four. Unlike in the previous games, creating expert-level moves in Connect Four is a more complex task. Consequently, the AlphaGo agent, trained within a constrained time frame and with limited computational resources, won’t achieve perfection. Nevertheless, it impressively outperforms the agent that plans three moves ahead. This achievement underscores a key takeaway: the adaptability of the AlphaGo algorithm to various games. Given sufficient training and computational power, the AlphaGo algorithm has the potential to reach superhuman performance levels, a feat demonstrated by the DeepMind team in 2016.\n",
    "\n",
    "In the coin game and Tic Tac Toe, fine-tuning hyperparameters is less critical: the majority of hyperparameter configurations result in the development of optimal game strategies. However, in the case of Connect Four, where the AlphaGo agent does not fully solve the game, it becomes crucial to identify the optimal combination of hyperparameters that yields the most effective game strategy.\n",
    "\n",
    "Grid search is a common approach for hyperparameter tuning. This process involves experimenting with different permutations of hyperparameters in the model to determine empirically which combination offers the best performance. However, this technique can be quite resource-intensive and time-consuming, particularly when dealing with a large number of hyperparameters.\n",
    "\n",
    "In this chapter, to streamline the process and save time, we’ll limit ourselves to testing only a few values per hyperparameter. The optimal parameters identified here should be regarded as a basic benchmark and a lower bound for the model’s capabilities. Specifically, you’ll explore 18 different hyperparameter combinations using grid search to determine which leads to the strongest game strategy.\n",
    "We focus on four key hyperparameters for the AlphaGo agent: weight (ranging from 0 to 1), which balances the impact of priors (suggested probabilities by the policy gradient agent) and rollout outcomes in node selection; policy_rollout (either True or False), determines whether to use the policy network to roll out games; depth, defining the maximum search steps ahead in a rollout; and num_rollouts, the number of rollouts performed in MCTS for each move in actual gameplay. \n",
    "\n",
    "We also impose a one-second time limit per move. This constraint effectively reduces the number of hyperparameters to three, as we will determine the maximum num_rollouts that can fit within this limit for each combination of weight, policy_rollout, and depth. The most effective combination identified is weight=0.75, policy_rollout=False, depth=45. The maximum number of rollouts the AlphaGo agent\n",
    "can perform within a second is num_rollouts=584. This optimized AlphaGo agent can defeat an AI that plans three steps ahead. If we relax the one-second-per-move time constraint and increase the number of rollouts to 5000, the AlphaGo agent becomes even stronger: it surpasses a rule-based AI that looks four steps ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d70a2",
   "metadata": {},
   "source": [
    "# 1. Test the AlphaGo Agent in Connect Four"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "## 2.1. The Opponent in Connect Four Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab56e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch06util import MiniMax_conn\n",
    "\n",
    "def rule_based_AI(env,depth=3):\n",
    "    move = MiniMax_conn(env,depth=depth)\n",
    "    return move "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39fab8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the trained fast policy network from Chapter 11\n",
    "fast_net=keras.models.load_model(\"files/policy_conn.h5\")\n",
    "# Load the policy gradient network from Chapter 15\n",
    "PG_net=keras.models.load_model(\"files/PG_conn.h5\")\n",
    "# Load the trained value network from Chapter 15\n",
    "value_net=keras.models.load_model(\"files/value_conn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef56cd1",
   "metadata": {},
   "source": [
    "## 1.2. AlphaGo vs Rule-Based AI in Connect Four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f806366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "from utils.ch17util import alphago\n",
    "\n",
    "weight=0.75\n",
    "depth=20\n",
    "# initiate game environment\n",
    "env=conn()\n",
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # AlphaGo moves first\n",
    "        action=alphago(env,weight,depth,PG_net,value_net,\n",
    "                fast_net, policy_rollout=True,num_rollouts=100)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            print(\"AlphaGo wins!\")\n",
    "            break     \n",
    "        # move recommended by rule-based AI\n",
    "        action=rule_based_AI(env)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"Rule-based AI wins!\")\n",
    "            break                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13cfa062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "The game is tied!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "Rule-based AI wins!\n"
     ]
    }
   ],
   "source": [
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # Rule-based AI moves first \n",
    "        action=rule_based_AI(env)  \n",
    "        state,reward,done,_=env.step(action)        \n",
    "        if done:\n",
    "            print(\"Rule-based AI wins!\")\n",
    "            break     \n",
    "        # AlphaGo moves second\n",
    "        action=alphago(env,weight,depth,PG_net,value_net,\n",
    "                fast_net, policy_rollout=True,num_rollouts=100)\n",
    "        state,reward,done,_=env.step(action)   \n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"AlphaGo wins!\")\n",
    "            break                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53dc8fc",
   "metadata": {},
   "source": [
    "# 2. Hypterparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebec0020",
   "metadata": {},
   "source": [
    "## 2.1 Real-World Constaints to Narrow Down Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8974da15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 18 combinations of hyperparameters\n"
     ]
    }
   ],
   "source": [
    "grid=[]\n",
    "weights=[0.25, 0.5, 0.75]\n",
    "policy_rollouts=[True, False]\n",
    "depths=[25, 35, 45]\n",
    "for x in weights:\n",
    "    for y in policy_rollouts:\n",
    "        for z in depths:\n",
    "            parameters={\"weight\":x,\"policy_rollout\":y,\"depth\":z}\n",
    "            grid.append(parameters)\n",
    "print(f\"there are {len(grid)} combinations of hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5221aed",
   "metadata": {},
   "source": [
    "## 2.2 The Maximum Number of Rollouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "397463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the priors from the PG policy network\n",
    "state=env.reset()\n",
    "state=env.state.reshape(-1,42)\n",
    "conv_state=state.reshape(-1,7,6,1)\n",
    "priors=PG_net([state,conv_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8705930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new list of hyperparameters\n",
    "new_grid=[]\n",
    "\n",
    "import time\n",
    "from utils.ch17util import (select, expand,\n",
    "            simulate, backpropagate)\n",
    "# iterate through different combinations of hyperparameters\n",
    "for parameters in grid:\n",
    "    # reset the Connect Four game\n",
    "    state=env.reset()  \n",
    "    # create a dictionary results\n",
    "    results={}\n",
    "    for move in env.validinputs:\n",
    "        results[move]=[]    \n",
    "    # extract hyperparameters\n",
    "    weight=parameters[\"weight\"]\n",
    "    policy_rollout=parameters[\"policy_rollout\"]    \n",
    "    depth=parameters[\"depth\"]    \n",
    "    # start counting\n",
    "    start=time.time()\n",
    "    n=0\n",
    "    # roll out games\n",
    "    while True:\n",
    "        n += 1\n",
    "        # select\n",
    "        move=select(priors,env,results,weight)\n",
    "        # expand\n",
    "        env_copy, done, reward=expand(env,move)\n",
    "        # simulate\n",
    "        reward=simulate(env_copy,done,reward,depth,value_net,\n",
    "                     fast_net, policy_rollout)\n",
    "        # backpropagate\n",
    "        results=backpropagate(env,move,reward,results)\n",
    "        # stop rollouts if it lasts more than a second\n",
    "        if time.time()-start>=1:\n",
    "            parameters[\"num_rollouts\"]=n\n",
    "            new_grid.append(parameters)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe54f212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'depth': 25, 'num_rollouts': 31, 'policy_rollout': True, 'weight': 0.25},\n",
      " {'depth': 35, 'num_rollouts': 32, 'policy_rollout': True, 'weight': 0.25},\n",
      " {'depth': 45, 'num_rollouts': 34, 'policy_rollout': True, 'weight': 0.25},\n",
      " {'depth': 25, 'num_rollouts': 107, 'policy_rollout': False, 'weight': 0.25},\n",
      " {'depth': 35, 'num_rollouts': 464, 'policy_rollout': False, 'weight': 0.25},\n",
      " {'depth': 45, 'num_rollouts': 575, 'policy_rollout': False, 'weight': 0.25},\n",
      " {'depth': 25, 'num_rollouts': 29, 'policy_rollout': True, 'weight': 0.5},\n",
      " {'depth': 35, 'num_rollouts': 32, 'policy_rollout': True, 'weight': 0.5},\n",
      " {'depth': 45, 'num_rollouts': 34, 'policy_rollout': True, 'weight': 0.5},\n",
      " {'depth': 25, 'num_rollouts': 128, 'policy_rollout': False, 'weight': 0.5},\n",
      " {'depth': 35, 'num_rollouts': 387, 'policy_rollout': False, 'weight': 0.5},\n",
      " {'depth': 45, 'num_rollouts': 577, 'policy_rollout': False, 'weight': 0.5},\n",
      " {'depth': 25, 'num_rollouts': 27, 'policy_rollout': True, 'weight': 0.75},\n",
      " {'depth': 35, 'num_rollouts': 32, 'policy_rollout': True, 'weight': 0.75},\n",
      " {'depth': 45, 'num_rollouts': 37, 'policy_rollout': True, 'weight': 0.75},\n",
      " {'depth': 25, 'num_rollouts': 165, 'policy_rollout': False, 'weight': 0.75},\n",
      " {'depth': 35, 'num_rollouts': 405, 'policy_rollout': False, 'weight': 0.75},\n",
      " {'depth': 45, 'num_rollouts': 584, 'policy_rollout': False, 'weight': 0.75}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(new_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ff35f",
   "metadata": {},
   "source": [
    "# 3. Search for the Best Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4608140",
   "metadata": {},
   "source": [
    "## 3.1 Stategies Compete with Each Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392f359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_game(si,sj):\n",
    "    # parameters in strategy i\n",
    "    weight_i=si[\"weight\"]\n",
    "    policy_rollout_i=si[\"policy_rollout\"]    \n",
    "    depth_i=si[\"depth\"]    \n",
    "    num_rollouts_i=si[\"num_rollouts\"] \n",
    "    # parameters in strategy j   \n",
    "    weight_j=sj[\"weight\"]\n",
    "    policy_rollout_j=sj[\"policy_rollout\"]    \n",
    "    depth_j=sj[\"depth\"]    \n",
    "    num_rollouts_j=sj[\"num_rollouts\"] \n",
    "    # reset the game\n",
    "    state=env.reset() \n",
    "    # play a full game\n",
    "    while True: \n",
    "        # move recommended by strategy si\n",
    "        action=alphago(env,weight_i,depth_i,\n",
    "                       PG_net,value_net,fast_net,\n",
    "                       policy_rollout_i,\n",
    "                       num_rollouts_i) \n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            result=abs(reward)\n",
    "            break      \n",
    "        # move recommended by strategy sj\n",
    "        action=alphago(env,weight_j,depth_j,\n",
    "                       PG_net,value_net,fast_net,\n",
    "                       policy_rollout_j,\n",
    "                       num_rollouts_j) \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            result=-abs(reward)\n",
    "            break  \n",
    "    return result     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b922562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to record game outcome\n",
    "results=[]\n",
    "# play ten games for each permutation\n",
    "for _ in range(10):\n",
    "    for i in new_grid:\n",
    "        for j in new_grid:\n",
    "            if i!=j:\n",
    "                result=one_game(i,j)\n",
    "                # record players and outcome\n",
    "                results.append((i,j,result))\n",
    "                print(i,j,result)\n",
    "# save the record to the computer\n",
    "import pickle\n",
    "with open(\"files/outcome.p\",\"wb\") as fb:\n",
    "    pickle.dump(results, fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e247afe",
   "metadata": {},
   "source": [
    "## 3.2 Find Out The Best Game Strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a5476a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the grid search results from the file\n",
    "import pickle\n",
    "with open(\"files/outcome.p\",\"rb\") as fb:\n",
    "    outcome=pickle.load(fb) \n",
    "\n",
    "# Create a dictionary to count results\n",
    "rewards={}\n",
    "for i, x in enumerate(new_grid):\n",
    "    rewards[i]=[]\n",
    "# iterate through all grid search results\n",
    "for x in outcome:\n",
    "    si, sj, r = x\n",
    "    # count the game outcome for the red player\n",
    "    rewards[new_grid.index(si)].append(r)\n",
    "    # count the game outcome for the yellow player\n",
    "    rewards[new_grid.index(sj)].append(-r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "760763c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -0.1864406779661017,\n",
      " 1: -0.2994350282485876,\n",
      " 2: -0.2542372881355932,\n",
      " 3: -0.11864406779661017,\n",
      " 4: 0.096045197740113,\n",
      " 5: 0.1694915254237288,\n",
      " 6: -0.2138728323699422,\n",
      " 7: -0.19254658385093168,\n",
      " 8: -0.22784810126582278,\n",
      " 9: 0.07586206896551724,\n",
      " 10: 0.2357142857142857,\n",
      " 11: 0.3023255813953488,\n",
      " 12: -0.11627906976744186,\n",
      " 13: -0.078125,\n",
      " 14: -0.015748031496062992,\n",
      " 15: 0.3333333333333333,\n",
      " 16: 0.42857142857142855,\n",
      " 17: 0.4523809523809524}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to count average score\n",
    "scores={}\n",
    "# iterate through keys and values in rewards\n",
    "for k, v in rewards.items():\n",
    "    # calculate average score for each game strategy\n",
    "    score=sum(v)/len(v)\n",
    "    # record in the dictionary scores\n",
    "    scores[k]=score\n",
    "# print out the saverage scores\n",
    "from pprint import pprint\n",
    "pprint(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68fbdc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "{'depth': 45, 'num_rollouts': 584, 'policy_rollout': False, 'weight': 0.75}\n"
     ]
    }
   ],
   "source": [
    "# the index of the best game strategy\n",
    "best_idx=max(scores, key=scores.get)\n",
    "print(best_idx)\n",
    "best_parameters=new_grid[best_idx]\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006698d1",
   "metadata": {},
   "source": [
    "# 4. Test the Best AlphaGo Agent in Connect Four\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f222e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo wins!\n",
      "The game is tied!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "from utils.conn_simple_env import conn\n",
    "from utils.ch17util import alphago\n",
    "\n",
    "weight_i=best_parameters[\"weight\"]\n",
    "policy_rollout_i=best_parameters[\"policy_rollout\"]    \n",
    "depth_i=best_parameters[\"depth\"]    \n",
    "num_rollouts_i=best_parameters[\"num_rollouts\"] \n",
    "# initiate game environment\n",
    "env=conn()\n",
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # AlphaGo moves first\n",
    "        action=alphago(env,weight_i,depth_i,PG_net,value_net,\n",
    "                fast_net, policy_rollout_i,num_rollouts_i)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            print(\"AlphaGo wins!\")\n",
    "            break     \n",
    "        # move recommended by rule-based AI\n",
    "        action=rule_based_AI(env)  \n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"Rule-based AI wins!\")\n",
    "            break                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a4dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    while True: \n",
    "        # Rule-based AI moves first \n",
    "        action=rule_based_AI(env)  \n",
    "        state,reward,done,_=env.step(action)        \n",
    "        if done:\n",
    "            print(\"Rule-based AI wins!\")\n",
    "            break     \n",
    "        # AlphaGo moves second\n",
    "        action=alphago(env,weight_i,depth_i,PG_net,value_net,\n",
    "                fast_net, policy_rollout_i,num_rollouts_i)\n",
    "        state,reward,done,_=env.step(action)   \n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"AlphaGo wins!\")\n",
    "            break                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e54b41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "Rule-based AI wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "Rule-based AI wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n",
      "AlphaGo wins!\n"
     ]
    }
   ],
   "source": [
    "# test ten games\n",
    "for i in range(10):\n",
    "    state=env.reset()  \n",
    "    if i%2==0:\n",
    "        # Ruled-based AI moves\n",
    "        action=rule_based_AI(env, depth=4)\n",
    "        state,reward,done,_=env.step(action)          \n",
    "    while True:     \n",
    "        # AlphaGo moves, setting num_rollouts=5000 \n",
    "        action=alphago(env,weight_i,depth_i,PG_net,value_net,\n",
    "                fast_net, policy_rollout_i,num_rollouts=5000)\n",
    "        state,reward,done,_=env.step(action)\n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"AlphaGo wins!\")\n",
    "            break  \n",
    "        # Rule-based AI moves\n",
    "        action=rule_based_AI(env, depth=4)\n",
    "        state,reward,done,_=env.step(action)     \n",
    "        if done:\n",
    "            if reward==0:\n",
    "                print(\"The game is tied!\")\n",
    "            else:\n",
    "                print(\"Rule-based AI wins!\")\n",
    "            break             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b765d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
