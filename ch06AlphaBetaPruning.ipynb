{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 6: Alpha-Beta Pruning\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "*“Art is the elimination of the unnecessary.”*\n",
    "\n",
    "-- Pablo Picasso\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "What you'll learn in this chapter:\n",
    "\n",
    "* The logic behind alpha-beta pruning\n",
    "* Implementing alpha-beta pruning in Tic Tac Toe and Connect Four\n",
    "* Calculating time saved by the alpha-beta pruning agent\n",
    "* Verifying that alpha-beta pruning won’t affect game outcomes\n",
    "\n",
    "As you have seen in Chapter 5, depth pruning makes the MiniMax algorithm possible in complicated games such as Connect Four, Chess, and Go. In this chapter, you'll use another method to improve the MiniMax algorithm and make it more efficient. Specifically, alpha beta pruning allows us to skip certain branches that cannot possibly influence the final game outcome. Doing so significantly reduces the amount of time for the MiniMax agent to come up with a move.\n",
    "\n",
    "To implement alpha-beta pruning in a game, we keep track of two numbers: alpha and beta, the best outcomes so far for Players 1 and 2, respectively. Whenever we have $alpha>-beta$, or equivalently $beta>-alpha$, the MiniMax algorithm stop searching a branch. \n",
    "\n",
    "We implement alpha-beta pruning in both Tic Tac Toe and Connect Four in this chapter. We show that the outcomes are the same with and without alpha-beta pruning. We also show that alpha-beta pruning saves significant amount of time for the player to find the best moves. For example, in Tic Tac Toe, the amount of time for the MiniMax agent to come up with the first move decreases from 34 seconds without alpha-beta pruning to 1.06 seconds with alpha-beta pruning, a 97% reduction in the amount of time the MiniMax agent needs to come up with a move. In Connect Four, we find that on average, the time spent on a move has reduced from 0.15 seconds to 0.05 seconds after we added in alpha-beta pruning when we limit the depth to three. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. What is Alpha Beta Pruning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe6000",
   "metadata": {},
   "source": [
    "# 2. Alpha-Beta Pruning in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c0ce",
   "metadata": {},
   "source": [
    "## 2.1. The maximized_payoff_ttt() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b94f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximized_payoff_ttt(env,reward,done,alpha,beta):\n",
    "    # if game ended after previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # set initial alpha and beta to -2\n",
    "    if alpha==None:\n",
    "        alpha=-2\n",
    "    if beta==None:\n",
    "        beta=-2\n",
    "    if env.turn==\"X\":\n",
    "        best_payoff = alpha\n",
    "    if env.turn==\"O\":\n",
    "        best_payoff = beta         \n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=maximized_payoff_ttt(env_copy,\\\n",
    "                                     reward,done,alpha,beta)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff > best_payoff:        \n",
    "            best_payoff = my_payoff\n",
    "            if env.turn==\"X\":\n",
    "                alpha=best_payoff\n",
    "            if env.turn==\"O\":\n",
    "                beta=best_payoff \n",
    "        # skip the rest of the branch        \n",
    "        if alpha>=-beta:\n",
    "            break        \n",
    "    return best_payoff         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cead4",
   "metadata": {},
   "source": [
    "## 2.2. The MiniMax_ab() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ef98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniMax_ab(env):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=maximized_payoff_ttt(env_copy,\\\n",
    "                                     reward,done,-2,-2)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return env.sample()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d4e3",
   "metadata": {},
   "source": [
    "## 2.3. Time Saved by Alpha-Beta Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20703e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=1\n",
      "It took the agent 1.0604541301727295 seconds\n",
      "Current state is \n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "Player O, what's your move?\n",
      "9\n",
      "Player O has chosen action=9\n",
      "Current state is \n",
      "[[ 0  0 -1]\n",
      " [ 0  0  0]\n",
      " [ 1  0  0]]\n",
      "Player X has chosen action=7\n",
      "It took the agent 0.06767606735229492 seconds\n",
      "Current state is \n",
      "[[ 1  0 -1]\n",
      " [ 0  0  0]\n",
      " [ 1  0  0]]\n",
      "Player O, what's your move?\n",
      "4\n",
      "Player O has chosen action=4\n",
      "Current state is \n",
      "[[ 1  0 -1]\n",
      " [-1  0  0]\n",
      " [ 1  0  0]]\n",
      "Player X has chosen action=3\n",
      "It took the agent 0.006998538970947266 seconds\n",
      "Current state is \n",
      "[[ 1  0 -1]\n",
      " [-1  0  0]\n",
      " [ 1  0  1]]\n",
      "Player O, what's your move?\n",
      "5\n",
      "Player O has chosen action=5\n",
      "Current state is \n",
      "[[ 1  0 -1]\n",
      " [-1 -1  0]\n",
      " [ 1  0  1]]\n",
      "Player X has chosen action=2\n",
      "It took the agent 0.0 seconds\n",
      "Current state is \n",
      "[[ 1  0 -1]\n",
      " [-1 -1  0]\n",
      " [ 1  1  1]]\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch06util import MiniMax_ab\n",
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action=MiniMax_ab(env)\n",
    "    end=time.time()\n",
    "    print(f\"Player X has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"Current state is \\n{state.reshape(3,3)[::-1]}\")\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a7d1a",
   "metadata": {},
   "source": [
    "It took only 1.06 seconds for the MiniMax agent to make the first move, instead of 34 seconds when alpha-beta pruning is not used. That's a huge improvement on the efficiency of the algorithm without affecting the effectiveness of the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df6cd7",
   "metadata": {},
   "source": [
    "# 3. Test MiniMax with Alpha-Beta Pruning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3d78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import MiniMax_X,MiniMax_O\n",
    "from utils.ch02util import one_ttt_game \n",
    "\n",
    "results=[]\n",
    "for i in range(10):\n",
    "    # MiniMax with pruning moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=one_ttt_game(MiniMax_ab,MiniMax_O)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # MiniMax with pruning moves second if i is an odd number\n",
    "    else:\n",
    "        result=one_ttt_game(MiniMax_X,MiniMax_ab)\n",
    "        # record negative game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "787481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMax with pruning won 0 games\n",
      "MiniMax with pruning lost 0 games\n",
      "the game was tied 10 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times MiniMax with pruning won\n",
    "wins=results.count(1)\n",
    "print(f\"MiniMax with pruning won {wins} games\")\n",
    "# count how many times MiniMax with pruning lost\n",
    "losses=results.count(-1)\n",
    "print(f\"MiniMax with pruning lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game was tied {ties} times\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "# 4. Alpha-Beta Pruning in Connect Four\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff7f0",
   "metadata": {},
   "source": [
    "## 4.1. Add Alpha-Beta Pruning in Connect Four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eecbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_payoff_conn(env,reward,done,depth,alpha,beta):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # If the maximum depth is reached, assume tie game\n",
    "    if depth==0:\n",
    "        return 0    \n",
    "    if alpha==None:\n",
    "        alpha=-2\n",
    "    if beta==None:\n",
    "        beta=-2\n",
    "    if env.turn==\"red\":\n",
    "        best_payoff = alpha\n",
    "    if env.turn==\"yellow\":\n",
    "        best_payoff = beta         \n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=max_payoff_conn(env_copy,\\\n",
    "                                reward,done,depth-1,alpha,beta)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff > best_payoff:        \n",
    "            best_payoff = my_payoff\n",
    "            if env.turn==\"red\":\n",
    "                alpha=best_payoff\n",
    "            if env.turn==\"yellow\":\n",
    "                beta=best_payoff   \n",
    "        # Skip the rest of the branch\n",
    "        if alpha>=-beta:\n",
    "            break        \n",
    "    return best_payoff        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1d6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniMax_conn(env,depth=3):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=max_payoff_conn(env_copy,\\\n",
    "                            reward,done,depth,-2,-2)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return env.sample()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf67f75",
   "metadata": {},
   "source": [
    "## 4.2. Time Saved due to Alpha-Beta Pruning in Connect Four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0982762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The red player has chosen action=6\n",
      "It took the agent 0.034066200256347656 seconds\n",
      "Current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "Player yellow, what's your move?\n",
      "1\n",
      "Player yellow has chosen action=1\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  1  0]]\n",
      "The red player has chosen action=1\n",
      "It took the agent 0.03681349754333496 seconds\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  1  0]]\n",
      "Player yellow, what's your move?\n",
      "2\n",
      "Player yellow has chosen action=2\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1 -1  0  0  0  1  0]]\n",
      "The red player has chosen action=5\n",
      "It took the agent 0.047269582748413086 seconds\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1 -1  0  0  1  1  0]]\n",
      "Player yellow, what's your move?\n",
      "1\n",
      "Player yellow has chosen action=1\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1 -1  0  0  1  1  0]]\n",
      "The red player has chosen action=4\n",
      "It took the agent 0.049561500549316406 seconds\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1 -1  0  1  1  1  0]]\n",
      "Player yellow, what's your move?\n",
      "3\n",
      "Player yellow has chosen action=3\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1 -1 -1  1  1  1  0]]\n",
      "The red player has chosen action=7\n",
      "It took the agent 0.05922341346740723 seconds\n",
      "Current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [-1  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0]\n",
      " [-1 -1 -1  1  1  1  1]]\n",
      "The red player has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch06util import MiniMax_conn\n",
    "from utils.conn_env import conn\n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action=MiniMax_conn(env,depth=3)\n",
    "    end=time.time()\n",
    "    print(f\"The red player has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"Current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"The red player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action=input(\"Player yellow, what's your move?\\n\")\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"Current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==-1:\n",
    "            print(f\"The yellow player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64596e",
   "metadata": {},
   "source": [
    "## 4.3. Effectiveness of Alpha-Beta Pruning in Connect Four\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97c5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import MiniMax_depth\n",
    "from utils.ch03util import one_conn_game \n",
    "\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    # MiniMax with pruning moves first if i is even \n",
    "    if i%2==0:\n",
    "        result=one_conn_game(MiniMax_conn,MiniMax_depth)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # MiniMax with pruning moves second if i is odd \n",
    "    else:\n",
    "        result=one_conn_game(MiniMax_depth,MiniMax_conn)\n",
    "        # record negative game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d97135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMax with alpha-beta pruning won 41 games\n",
      "MiniMax with alpha-beta pruning lost 39 games\n",
      "the game was tied 20 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times MiniMax with alpha-beta pruning won\n",
    "wins=results.count(1)\n",
    "print(f\"MiniMax with alpha-beta pruning won {wins} games\")\n",
    "# count how many times MiniMax with pruning lost\n",
    "losses=results.count(-1)\n",
    "print(f\"MiniMax with alpha-beta pruning lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game was tied {ties} times\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec2255",
   "metadata": {},
   "source": [
    "The above results show that the MiniMax agent with alpha-beta pruning has won 41 times and lost 39 times. This shows that the MiniMax agent with alpha-beta pruning is as intelligent as the agent without alpha-beta pruning. Note that since the outcomes are random, you may get results showing that the MiniMax agent with alpha-beta pruning has lost more often than it has won. If that happens, run the above two cells again and see if the results change. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
