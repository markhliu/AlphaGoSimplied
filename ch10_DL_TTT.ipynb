{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 10: Introduction to Deep Learning\n",
    "\n",
    "Starting from this chapter, you'll learn a new AI paradigm: machine learning. Instead of hard-coding in the rules, machine learning takes in input-output pairs and figures out the relation between the inputs (which we call features) and outputs (the labels). One field of machine learning, deep learning, has attracted much attention in recent years. The algorithm used by AlphaGo is based on deep reinforcement learning, which is a combimation of deep learning and reinforcement learning.\n",
    "\n",
    "Deep learning are based on artificial neural networks. A neural network consists of an input layer, some hidden layers, and an output layer. In this chapter, we'll learn to use deep neural networks to design game strategies for Tic Tac Toe. The neural network we create includes both dense layers and convolutional layers. While dense layers treat the input as a one-dimensional vector, convolutional layers can process two dimensional inputs such as images or game boards. As a result, convolutional layers can extract spatial features in images and game boards. This, in turn, has greatly improved the power of deep neural networks (DNNs). You'll learn to treat the Tic Tac Toe game board as a two-dimensional image and extract spatial features from the image and associate these features with game outcome and design intelligent game strategies.  \n",
    "\n",
    "You'll use similated games as input data to feed into a deep neural network with a convolutional layer and some dense layers. After the model is trained, you'll use it to play games. At each step of the game, you'll look at all possible next moves. The trained model predicts the probability of winning the game with each hypothetical move. You'll pick the move with the highest probability of winning the game for the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 10}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 10 in a subfolder /files/ch10. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch10\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. What Are Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "This section discusses the basic structure of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. Elements of A Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "A neural network consists of one input layer, one output layer, and a number of hidden layers. In general, each layer in a neural network has one or more neurons. Neural networks with two or more hidden layers are usually called deep neural networks. \n",
    "\n",
    "There are differnet types of layers in a neural network. The most common type of layer is the dense layer, in which each neuron is fully connected to the neurons in the next layer. \n",
    "\n",
    "The convolutional layers, in contrast, treats the input as a two-dimensional image and extract patterns from the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de48446",
   "metadata": {},
   "source": [
    "## 1.2. Activation Functions\n",
    "In artificial neural networks, activation functions transform inputs into outputs. As\n",
    "the name suggests, the activation functions activate the neuron when the input reaches a certain threshold. Simply put, activation functions are on-off switches in artificial neural networks. These on-off switches play an important role in making artificial neural networks powerful. The activation functions allow a network to learn more complex patterns in the data. Without activation functions, neural networks can only learn linear relationships in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6f7d9",
   "metadata": {},
   "source": [
    "Activation functions help us create a nonlinear relationship between the inputs and outputs. Without them, we can only approximate linear relations. No matter how many hidden layers we add to the neural network, we cannot achieve a nonlinear relationship. Without activation functions, the neural network cannot learn a nonlinear relationship: the linear transformation of a linear relationship is still linear.\n",
    "\n",
    "ReLU is short for rectified linear unit activation function. It returns the original value if it’s positive, and 0 otherwise. It has the mathematical formula of \n",
    "$$ReLU(x)\\\n",
    "=\\{\\genfrac{}{}{0}{}{x\\ if \\ x>0}{0\\ if \\ x\\leq 0}$$\n",
    "\n",
    "It’s widely used in many neural networks, and you’ll see it in this book more often\n",
    "than any other type of activation function.\n",
    "In essence, the ReLU activation function activates the neuron when the value of x\n",
    "reaches the threshold value of zero. When the value of x is below zero, the neuron is\n",
    "switched off. This simply on-off switch is able to create a nonlinear relation between\n",
    "inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeb40b6",
   "metadata": {},
   "source": [
    "Another commonly used activation function is the sigmoid function. It’s widely used in many machine learning models. In particular, it’s a must-have in any binary classification problem.\n",
    "The sigmoid function has the form\n",
    "$$y=\\frac {1} {1+e^{-x}} $$\n",
    "The sigmoid function has an S-shaped curve. It has this nice property: for any value\n",
    "of input x between −∞ and∞, the output value y is always between 0 and 1. Because\n",
    "of this property, we use the sigmoid activation function to model the probability of\n",
    "an outcome, which also falls between 0 and 1 (0 means there is no chance of the\n",
    "outcome occurring, while 1 the outcome occurring with 100% certainty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd9dd0",
   "metadata": {},
   "source": [
    "The third most-used activation function in this book is the softmax function. It’s a\n",
    "must-have in any multi-category classification problem.\n",
    "The softmax function has the form\n",
    "$$y(x)=\\frac {e^{x}} {\\sum_{k=1}^{K}e^{x_k}}$$\n",
    "where $x=[x_1,x_2,...,x_K]$ and $y=[y_1,y_2,...,y_K]$ are K-element lists. The i-th element of $y$ is \n",
    "$$y_i(x)=\\frac {e^{x_i}} {\\sum_{k=1}^{K}e^{x_k}}$$ \n",
    "The softmax function has a nice property: each element in the output vector $y$ is always between 0 and 1. Further, elements in the output vector $y$ sum up to 1. Because of this property, we use the softmax activation function to model the probability of a multiple outcome event. Therefore, the activation function in the output layer is always the softmax function when we model multi-class classification problems.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc801428",
   "metadata": {},
   "source": [
    "Finally, we'll also use the tanh activation function when we train the actor-critic models later in this book. The Tanh activation function is similar to the sigmoid activation function in the sense that it also s-shaped. However, the output from the tanh activation function is in the range of -1 to 1 instead of from 0 to 1. The tanh activation function has the form\n",
    "$$y=\\frac {2} {1+e^{-2x}} -1$$ In multiplayer games, the game outcome for a player is a number between -1 and 1: -1 means the player has lost the game, 1 means the palyer has won teh game, and a 0 means the game has tied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db08fb0",
   "metadata": {},
   "source": [
    "## 1.3. Loss Functions\n",
    "The loss function in ML is the objective function in the mathematical optimization process. Intuitively, the loss function measures the forecasting error of the machine learning algorithm. \n",
    "By minimizing the loss function, the machine learning model finds parameter values that\n",
    "lead to the best predictions. \n",
    "\n",
    "The most commonly used loss function is mean squared error (MSE). MSE is defined as $$MSE= \\frac{1}{N} \\sum_{i=1} ^{N} (Y_n-\\hat{Y}_n)^2$$\n",
    "\n",
    "where $Y_n$ is the actual value of the target variable (i.e., the label) and $\\hat{Y}_n$ is the predicted value of the target variable. \n",
    "To calculate MSE, we look at the forecasting error: the difference between the model’s predictions and the actual values. We then square the forecasting error for each observation, and average it across all observations. In short, it is the average squared forecasting error in each observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e556587",
   "metadata": {},
   "source": [
    "In binary classification problems, the preferred loss function is the binary cross-entropy function, which measures the average difference between the predicted probabilities and the actual labels (1 or 0). If a model makes a perfect prediction and assigns a 100% probability to all observations labeled 1 and a 0% probability to all observations labeled 0, the binary cross-entropy loss function will have a value of 0. \n",
    "\n",
    "Mathematically, the binary class-entropy loss function is defined as \n",
    "$$BinaryCrossEntropy= \\sum_{n=1} ^{N} -[Y_n\\times log(\\hat{Y}_n) + (1-Y_n)\\times log(1-\\hat{Y}_n)]$$\n",
    "where $\\hat{Y}_n$ is the estimated probability of observation n being class 1, and $Y_n$ is the actual label of observation n (which is either 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bcc97",
   "metadata": {},
   "source": [
    "The preferred loss function to use in multi-category classifications is the categorical-crossentropy loss function. It measures the average difference between the predicted distribution and the actual distribution. \n",
    "\n",
    "Mathematically, the categorical class-entropy loss function is defined as \n",
    "$$Categorical\\ Cross\\ Entropy= \\sum_{n=1} ^{N}\\sum_{k=1} ^{K} -y_{n,k}\\times log(\\hat{y}_{n,k})$$\n",
    "where $\\hat{y}_{n,k}$ is the estimated probability of observation n being class k, and $y_{n,k}$ is the actual label of observation n belonging to category k (which is either 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcbdf00",
   "metadata": {},
   "source": [
    "# 2.  Deep Learning Game Strategies in Tic Tac Toe\n",
    "In this chapter, you'll use a deep neural network to train intelligent game strategies for Tic Tac Toe. In particular, you’ll use the convolutional neural network to train the game strategy. By treating the game board as a two-dimensional image instead of a one-dimensional vector, you’ll greatly improve the intelligence of your game strategies.\n",
    "\n",
    "You’ll learn how to prepare data to train the model, how to interpret the prediction from the model, how to use the prediction to play games, and how to check the efficacy of your game strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0b2e3",
   "metadata": {},
   "source": [
    "## 1.1. A Summary of the Game Strategy for Tic Tac Toe\n",
    "Here is a summary of what we’ll do to train the game strategy:\n",
    "\n",
    "1.\tWe’ll let two computer players play a game with random moves, and record the whole game history. The game history will contain all the game board positions from the very first move to the very last move.\n",
    "2.\tWe then associate each board position with a game outcome (a win, a tie, or a loss). We'll use the game board position as features X, and the outcome as labels y. We'll treat this as a multi-category classification problem since there are three possible outcomes associated with each board posotion: a win, a tie, or a loss.\n",
    "3.\tWe’ll simulate 100,000 games. By using the histories of the games and the corresponding outcomes as Xs and ys, we feed the data into a Deep Neural Network. After the training is done, we have a trained model.\n",
    "4.\tWe can now use the trained model to play a game. At each move of the game, we look at all possible next moves, and feed the hypothetical game board into the pretained model. The model will tell you the probabilities of a win, a loss, and a tie.\n",
    "5.\tYou select the move that the model predicts with the highest chance of winning for the current player."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c5b19",
   "metadata": {},
   "source": [
    "## 1.2. Create Training Data for Tic Tac Toe \n",
    "You’ll learn how to generate data to train the DNN. The logic is as follows: you’ll generate 100,000 games in which both players use random moves. You’ll then record the board positions of all intermediate steps and the eventual outcomes of each board position (a win, a loss, or a tie). \n",
    "\n",
    "First, let's simulate one game. The code in the cell below accomplishes that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2f8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 0, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0]]),\n",
      " array([[ 1,  0,  0],\n",
      "       [ 0, -1,  0],\n",
      "       [ 0,  0,  0]]),\n",
      " array([[ 1,  0,  0],\n",
      "       [ 0, -1,  0],\n",
      "       [ 0,  0,  1]]),\n",
      " array([[ 1,  0,  0],\n",
      "       [-1, -1,  0],\n",
      "       [ 0,  0,  1]]),\n",
      " array([[ 1,  0,  0],\n",
      "       [-1, -1,  0],\n",
      "       [ 0,  1,  1]]),\n",
      " array([[ 1, -1,  0],\n",
      "       [-1, -1,  0],\n",
      "       [ 0,  1,  1]]),\n",
      " array([[ 1, -1,  1],\n",
      "       [-1, -1,  0],\n",
      "       [ 0,  1,  1]]),\n",
      " array([[ 1, -1,  1],\n",
      "       [-1, -1, -1],\n",
      "       [ 0,  1,  1]])]\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "\n",
    "# Define the one_game() function\n",
    "def one_game():\n",
    "    history = []\n",
    "    state=env.reset()   \n",
    "    while True:   \n",
    "        action = random.choice(env.validinputs)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        history.append(np.array(state).reshape(3,3))\n",
    "        if done:\n",
    "            break\n",
    "    return history, reward\n",
    "\n",
    "# Simulate one game and print out results\n",
    "history, outcome = one_game()\n",
    "pprint(history)\n",
    "pprint(outcome)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db511f44",
   "metadata": {},
   "source": [
    "Note here we convert the game board to a 3 by 3 array so it's easy for you to see the positions of the game pieces. \n",
    "\n",
    "Now let's simulate 100,000 games and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the game 100000 times and record all games\n",
    "results = []        \n",
    "for x in range(100000):\n",
    "    history, outcome = one_game()\n",
    "    # Associate each board with the game outcome\n",
    "    for board in history:\n",
    "        results.append((outcome, board))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb7be7",
   "metadata": {},
   "source": [
    "Now let's save the data on your computer for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88782a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-1, array([[0, 1, 0],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0]])),\n",
      " (-1, array([[-1,  1,  0],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0,  0]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [ 0, -1,  0],\n",
      "       [ 0,  0,  0]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [ 0, -1,  1],\n",
      "       [ 0,  0,  0]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [-1, -1,  1],\n",
      "       [ 0,  0,  0]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [-1, -1,  1],\n",
      "       [ 0,  1,  0]])),\n",
      " (-1, array([[-1,  1,  1],\n",
      "       [-1, -1,  1],\n",
      "       [ 0,  1, -1]])),\n",
      " (1, array([[0, 0, 1],\n",
      "       [0, 0, 0],\n",
      "       [0, 0, 0]])),\n",
      " (1, array([[ 0,  0,  1],\n",
      "       [ 0,  0,  0],\n",
      "       [ 0,  0, -1]]))]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# save the simulation data on your computer\n",
    "with open('files/ch10/games_ttt100K.p', 'wb') as fp:\n",
    "    pickle.dump(results,fp)\n",
    "# read the data and print out the first 10 observations       \n",
    "with open('files/ch10/games_ttt100K.p', 'rb') as fp:\n",
    "    games = pickle.load(fp)\n",
    "pprint(games[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d59dd",
   "metadata": {},
   "source": [
    "The first eight observations are from teh first game in which player O won by occupying cells 1, 5, and 9. Therefore you see -1 as the first element of the first eight observations. The data are stored correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f5bef",
   "metadata": {},
   "source": [
    "We have the data we need. You’ll learn how to train the model next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e27a43",
   "metadata": {},
   "source": [
    "# 3. Create A Convolutional Neural Network\n",
    "\n",
    "We'll use Keras to create a deep neural network to train game strategies in Tic Tac Toe. In particular, the network will include some dense layers and a convolutional layer. Since there are three possible game outcomes (a win, a loss, and a tie), we'll treat the learning process as a multi-category classification probelm. Therefore, we'll have three neurons in the output layer. We'll use softmax as out activation function in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6e4fd",
   "metadata": {},
   "source": [
    "## 3.1. Convolutional Layers\n",
    "Convolutional layers use filters (also called kernels) to find patterns on the input data. A convolutional layer can automatically detect a large number of patterns and associate certain patterns with the target label. This is useful in both image classifications and game strategy developments in machine learning.\n",
    "\n",
    "In particular, we'll use the Tic Tac Toe game board, something everyone knows, as our example in this chapter. Game boards have far fewer pixels than images and we can focus on certain patterns that we know are associated with game outcomes (vertical, horizontal, or diagonal lines in Tic Tac Toe and Connect Four games, for example). Therefore, we’ll use game boards to explain how CNNs work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc632f",
   "metadata": {},
   "source": [
    "Let’s say that the input data is the Tic Tac Toe game board. For simplicity let’s assume the board looks like the picture below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c9cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = np.array([[1,1,1],\n",
    "                   [0,-1,-1],\n",
    "                   [0,0,0]]).reshape(-1,3,3,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be719c32",
   "metadata": {},
   "source": [
    "In teh anove game board, Player X occupies cells 1, 2, and 3, while Player O occupies cells 5 and 6. We represent the board with a 3 by 3 matrix: the first row has three ones since they are occupied by Xs. We use reshape(-1,3,3,1) to reshape the matrix to a four dimensional array: the first dimension represents how many images we have; the second and third dimensions are the width and height of the image. The last dimension is the color channel. For a color picture, there are three channels (RGB, i.e., red, green, and blue), but here we put the number of channels as one for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec592e5",
   "metadata": {},
   "source": [
    "Below, we'll create a horizontal filter with a size of 3 by 3. The middle row has values 1, while the other two rows have 0s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62e1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a horizontal filter\n",
    "h_filter = np.array([[0,0,0], \n",
    "                   [1,1,1],\n",
    "                   [0,0,0]]).reshape(3,3,1,1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45c808",
   "metadata": {},
   "source": [
    "A horizontal filter highlights the horizontal features in the image and blurs the rest. We’ll apply the 3 by 3 horizontal filter on the Tic Tac Toe game board as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a8a38b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  2]\n",
      " [-1 -2 -2]\n",
      " [ 0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Apply the filter on the game board\n",
    "outputs=tf.nn.conv2d(inputs,h_filter,strides=1,padding=\"SAME\")\n",
    "# Convert output to numpy array and print it output\n",
    "print(outputs.numpy().reshape(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66218000",
   "metadata": {},
   "source": [
    "In the output, the values are large in the first row. The values are much lower in the other two rows. So the horizontal filter has correctly detected the horizontal pattern in the first row of the game board.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd8eed1",
   "metadata": {},
   "source": [
    "## 3.2. Create A Model to Train the Tic Tac Toe Game Strategy\n",
    "We use Keras to create teh following deep neural network to train teh game strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1579eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=128, \n",
    "kernel_size=(3,3),padding=\"same\",activation=\"relu\",\n",
    "                 input_shape=(3,3,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam', \n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f26be1",
   "metadata": {},
   "source": [
    "We first use a convoluttional layer with 128 filters. The kernel size is 3 by 3. We tehn flatten the output form the convolutional layer to a vector and feed it to two hidden dense layers with 64 neurons each. The output layer has three neurons, representing there possible game outcomes: a win, a tie, or a loss. The softmax activation ensures that the proabilities add up to 100%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441286c7",
   "metadata": {},
   "source": [
    "## 3.3. Train the Deep Neural Network\n",
    "We'll train the deep neural network we just created in the last section. We first preprocess the data so that we can feed them into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9bc75d",
   "metadata": {},
   "source": [
    "The outcome data is a variable with three possible values: -1, 0, and 1. We'll convert them into one-hot variables so that the deep neural network can process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e2ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "labels=[0,1,-1]\n",
    "one_hot=tf.keras.utils.to_categorical(labels,3)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43472ce9",
   "metadata": {},
   "source": [
    "In the example above, we have three labels: 0, 1, and -1. They represent a tie, a win for Player X, and a loss for Player X (i.e., a win for Player O).\n",
    "\n",
    "We can use the *to_categorical()* method in TensorFlow to change them into one-hot variables. The second argument in the *to_categorical()* method, 3, indicates the depth of the one-hot variable. This means each one-hot variable will be a vector with a length of 3, with value 1 in one position and 0\n",
    "in all others.\n",
    "\n",
    "A tie, which has an initial label of 0, now becomes a one-hot label: a 3-value\n",
    "vector [1, 0, 0]. The first value (i.e., index 0) is turned on as 1, and the other two values are turned off as 0. Similarly, a win for Player X, which has a label of 1 originally, now becomes a one-hot label of [0, 1, 0]. The second value (i.e., index 1) is turned on as 1, and the rest are turned off as 0. By the same logic, a loss for Player X, with an original value of -1, is now represented by\n",
    "[0, 0, 1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965b0cf8",
   "metadata": {},
   "source": [
    "Next, we load up the simulated game data and convert them into Xs and ys so that we can feed them into the deep neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31cfc31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('files/ch10/games_ttt100K.p','rb') as fp:\n",
    "    tttgames=pickle.load(fp)\n",
    "\n",
    "boards = []\n",
    "outcomes = []\n",
    "for game in tttgames:\n",
    "    boards.append(game[1])\n",
    "    outcomes.append(game[0])\n",
    "\n",
    "X = np.array(boards).reshape((-1, 3, 3, 1))\n",
    "# one_hot encoder, three outcomes: -1, 0, and 1\n",
    "y = tf.keras.utils.to_categorical(outcomes, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea807c",
   "metadata": {},
   "source": [
    "Finally, we train the model for 100 epochs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fc541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for 100 epochs\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "model.save('files/ch10/trained_ttt100K.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4185711",
   "metadata": {},
   "source": [
    "It takes several hours to train the model since we have close to a million observations. The trained model is saved on your computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14805aac",
   "metadata": {},
   "source": [
    "## 4. Use the Trained Model to Play Tic Tac Toe\n",
    "Next, we’ll use the strategy to play a game. \n",
    "\n",
    "The player X will use the best move from the trained model. Player O will randomly select a move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765a00ae",
   "metadata": {},
   "source": [
    "### 4.1. Best Moves Based on the Trained Model\n",
    "First, we'll define a *best_move_X()* function for player X. The function will go over each move hypothetically, and use the trained deep neural network to predict the probability of player X winning the game. The function returns the move with the highest chance of winning.\n",
    "\n",
    "We define a best_move_X() function for the computer to find best moves. \n",
    "What the computer does is as follows:\n",
    "1.\tLook at the current board.\n",
    "2.\tLook at all possible next moves, and add each move to the current board to form a hypothetical board\n",
    "3.\tUse the pretained model to predict the chance of winning with the hypothetical board\n",
    "4.\tChoose the move that produces the highest chance of winning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efd4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_X(env):\n",
    "    # if there is only one valid move, take it\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1, 3,3,1)\n",
    "        prediction=reload.predict(state, verbose=0)\n",
    "        # output is prob(X wins) - prob(O wins)\n",
    "        win_lose_dif=prediction[0][1]-prediction[0][2]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c112990",
   "metadata": {},
   "source": [
    "Similarly, we'll define a *best_move_O()* function for player O. The function will go over each move hypothetically, and use the trained deep neural network to predict the probability of player O winning the game. The function returns the move with the highest chance of winning for Player O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c142967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move_O(env):\n",
    "    # Set the initial value of bestoutcome        \n",
    "    bestoutcome = -2;\n",
    "    bestmove=None    \n",
    "    #go through all possible moves hypothetically \n",
    "    for move in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        state=state.reshape(-1,3,3,1)\n",
    "        prediction=reload.predict(state, verbose=0)\n",
    "        # output is prob(O wins) - prob(X wins)\n",
    "        win_lose_dif=prediction[0][2]-prediction[0][1]\n",
    "        if win_lose_dif>bestoutcome:\n",
    "            # Update the bestoutcome\n",
    "            bestoutcome = win_lose_dif\n",
    "            # Update the best move\n",
    "            bestmove = move\n",
    "    return bestmove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fe8eb",
   "metadata": {},
   "source": [
    "Now let's use the best move functions to choose moves for player X and play a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f51838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=5\n",
      "the current state is \n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Player O has chosen action=2\n",
      "the current state is \n",
      "[[ 0 -1  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Player X has chosen action=1\n",
      "the current state is \n",
      "[[ 1 -1  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Player O has chosen action=4\n",
      "the current state is \n",
      "[[ 1 -1  0]\n",
      " [-1  1  0]\n",
      " [ 0  0  0]]\n",
      "Player X has chosen action=9\n",
      "the current state is \n",
      "[[ 1 -1  0]\n",
      " [-1  1  0]\n",
      " [ 0  0  1]]\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "file='files/ch10/trained_ttt100K.h5'\n",
    "reload=tf.keras.models.load_model(file)\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "while True:\n",
    "    # Use the best_move() function to select the next move\n",
    "    action = best_move_X(env)\n",
    "    print(f\"Player X has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\") \n",
    "        else:\n",
    "            print(f\"It's a tie!\") \n",
    "        break   \n",
    "    action = random.choice(env.validinputs)\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfd00ef",
   "metadata": {},
   "source": [
    "The DNN model has won the game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ece520",
   "metadata": {},
   "source": [
    "Next, we’ll test how often the DNN trained game strategy wins against a player who makes random moves. \n",
    "The following script does that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f926d",
   "metadata": {},
   "source": [
    "## 4.2. Against Random Players\n",
    "We'll see how the deep learning game strategy fairs against a random player. We simulate 100 games. If the deep learning agent wins, we record an outcome of 1. Otherwise, we record an outcome of -1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d392cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        if env.turn==\"X\":\n",
    "            action = best_move_X(env) \n",
    "        else:\n",
    "            action = best_move_O(env)    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the DL agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = random.choice(env.validinputs)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the DL agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b8f32",
   "metadata": {},
   "source": [
    "Among 50 games, the deep learning agent moves. In the remaining 50 games, the random-move agent goes first. This way, no player has a first-mover's advantage. We first create an empty list *results*. Whenever the deep learning agent wins, we append a value of 1 to the list. Otherwise we add an element of -1 to the list.\n",
    "\n",
    "Next, we count how many times the deep learning agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f806366f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the deep learning agent has won 92 games\n",
      "the deep learning agent has lost 3 games\n",
      "the game has tied 5 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the deep learning agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the deep learning agent has lost {losses} games\")         \n",
    "# count how many times the game ties\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf606284",
   "metadata": {},
   "source": [
    "The deep learning agent wins 92 out of 100 games and loses 3 games. The remaining 5 games are tied. So the deep learning game strategy works really well!\n",
    "\n",
    "Note that we simulated 100,000 games to train the model. You can potentially simulate even more games, say, 1 million games and train the model. The trained model will be even more powerful. However, it takes much longer to train as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2649a",
   "metadata": {},
   "source": [
    "## Against Think-Two-Steps-Ahead AI\n",
    "Next, we see how the deep learning agent fairs against the think-two-steps-ahead AI agent that we developed in Chapter 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13cfa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import AI_think2\n",
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=AI_think2(env)\n",
    "        if action is None:\n",
    "            action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        if env.turn==\"X\":\n",
    "            action = best_move_X(env) \n",
    "        else:\n",
    "            action = best_move_O(env)    \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the DL agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = AI_think2(env)  \n",
    "        if action is None:\n",
    "            action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the DL agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47606028",
   "metadata": {},
   "source": [
    "In Tic Tac Toe, Player X has a huge first-mover’s advantage. Therefore, we test 100 games and in 50 of them, we let the think-two-steps-ahead agent go first. In the other 50 games, the deep learning agent moves first. We record game outcomes in a list results. If the deep learning agent wins, we record an outcome of 1 in the list results. If the deep learning agent loses, we record an outcome of -1. If the game is tied, we record an outcome of 0.\n",
    "\n",
    "Next, we check how many times the deep learning agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e519e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the deep learning agent has won 36 games\n",
      "the deep learning agent has lost 12 games\n",
      "the game has tied 52 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the deep learning agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the deep learning agent has lost {losses} games\")         \n",
    "# count how many times the game ties\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02231a3b",
   "metadata": {},
   "source": [
    "Whenever it’s Player X’s turn, the deep learning agent uses the best_move_X() function to select a move. Whenever it’s Player O’s turn, the deep learning agent uses the best_move_O() function to select a move. The opponent of the deep learning agent is the think-two-steps-ahead agent. Results show that the deep learning agent has won 36 games and lost 12 games out of 100 games. The remaining 52 games are tied. So the deep learning game strategy works really well and seems to be better than a think-two-steps-ahead agent. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
