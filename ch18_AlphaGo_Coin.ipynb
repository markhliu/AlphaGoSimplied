{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 18: Implement AlphaGo in the Coin Game\n",
    "\n",
    "AlphaGo combined deep reinforcement learning (namely, the actor-critic method) with traditional rule-based AI (namely, the Monte Carlo Tree Search) to generate intelligent game strategies in Go. \n",
    "\n",
    "Now that you learned how both MCTS and the actor-critic method work, you'll learn how to combine these two methods to create powerful game strategies in simple games. We'll start with the Coin game in this chapter. In the next two chapters, we'll apply the AlphaGo approach to Tic Tac Toe and Connect Four. \n",
    "\n",
    "In traditional MCTS, you evaluate the position of each game board by simulating many games from that point on, all the way to the terminal state in each game. You then look at the averge outcome and use that to evaluate positions. The games are rolled out by choosing random moves. \n",
    "\n",
    "To combine MCTS with the actor-critic method, we'll roll out games not by picking random moves. Instead, games are rolled out based on the trained policy network from the actor-critic method that we discussed in Chapters 15 to 17. The improved roll-out policy leads to better position evaluations. That's the main insight from AlphaGo. \n",
    "\n",
    "The policy-MCTS agent is better than the actor-critic agent as well. The reason is for a stochastic policy, there is a msall chance of error. Say there are 4 coins left on the table, and the stochatic policy may recommend to take 1 coin with 99.9% prob and to take 2 coins with 0.1% prob. Even though the policy is highly effecive: it leads to wins 99.9% of the time. But if you roll out many games using the same 99.9%/0.1% policy and take the average outcome, the mistake can be further reduced. This is the insight form alphago as well.\n",
    "\n",
    "So the policy MCTS is better than both MCTS and the policy network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 18}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 18 in a subfolder /files/ch18. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch18\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. A Traditional UCT MCTS Agent for the Coin Game\n",
    "For comparison, we'll create a MCTS agent based on the UCT rules for the Coin game. The UCT rules are teh same as those we discuseed earlier in the book in Chapter 9 for Tic Tac Toe and Connect Four."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfde6",
   "metadata": {},
   "source": [
    "## 1.1. Select Moves Based on UCT Rule "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7961b4",
   "metadata": {},
   "source": [
    "To save space, we'll create a local module ch18util to store all functions related to the UCT MCTS agent as well as the policy MCTS agent. Download the file ch18util.py from the book's GitHub repository and place it in /Desktop/ai/utils/ on your computer. \n",
    "\n",
    "In the local module ch18util, we define a uct_select() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52448c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from math import log, sqrt\n",
    "\n",
    "def uct_select(env_copy,path,paths,temperature):\n",
    "    # use uct to select move\n",
    "    parent=[]\n",
    "    pathvs=[]\n",
    "    for v in env_copy.validinputs:\n",
    "        pathv=path+str(v)\n",
    "        pathvs.append(pathv)\n",
    "        for p in paths:\n",
    "            if p[0]==pathv:\n",
    "                parent.append(p)\n",
    "    # calculate uct score for each action\n",
    "    uct={}\n",
    "    for pathv in pathvs:\n",
    "        history=[p for p in parent if p[0]==pathv]\n",
    "        if len(history)==0:\n",
    "            uct[pathv]=float(\"inf\")\n",
    "        else:\n",
    "            uct[pathv]=sum([p[1] for p in history])/len(history)+\\\n",
    "                temperature*sqrt(log(len(parent))/len(history))    \n",
    "    move=max(uct,key=uct.get)\n",
    "    move=int(move[-1])\n",
    "    return move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797053e1",
   "metadata": {},
   "source": [
    "The agent uses the UCT rule to select moves. The function returns the selected move. We'll simulate games based on the above move selections next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d70a2",
   "metadata": {},
   "source": [
    "## 1.2. Simulate A Game Based on UCT Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecf068",
   "metadata": {},
   "source": [
    "Now that we know how to select moves based on the UCT rule, we'll define a uct_simulate_coin() function in the local module ch18util. The function simulates a game from a certain starting position all the way to the end of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uct_simulate_coin(env,paths,counts,wins,losses,temperature):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    path=\"\"\n",
    "    # play a full game\n",
    "    while True:\n",
    "        utc_move=uct_select(env_copy,path,paths,temperature)\n",
    "        move=deepcopy(utc_move)\n",
    "        actions.append(move)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        path += str(move)\n",
    "        if done:\n",
    "            result=0\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==1) or \\\n",
    "                (reward==-1 and env.turn==2):\n",
    "                result=1\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==1) or \\\n",
    "                (reward==1 and env.turn==2):\n",
    "                result=-1\n",
    "                losses[actions[0]] += 1                \n",
    "            break\n",
    "    return result,path,counts,wins,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7136ae",
   "metadata": {},
   "source": [
    "This function simulates a coin game and updates the number of game counts, number of wins and losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072b01c",
   "metadata": {},
   "source": [
    "After each game, we'll update teh statistics using the backpropagate function defined in the local module ch18util, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e5be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagate\n",
    "def backpropagate(path,result,paths):\n",
    "    while path != \"\":\n",
    "        paths.append((path,result))\n",
    "        path=path[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a960",
   "metadata": {},
   "source": [
    "We also define a best_move() function in ch18util.py, to select the best move based on the number of games, the number of wins and losses, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4b0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move(counts,wins,losses):\n",
    "    # See which action is most promising\n",
    "    scores={}\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "    best_move=max(scores,key=scores.get)  \n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb4bc12",
   "metadata": {},
   "source": [
    "The function looks at all valid next moves and calculates a score for each move: the score is the difference in the percentage of wins versus losses. The function selects the best move as the next move with the highest score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5d06",
   "metadata": {},
   "source": [
    "## 1.3. A UCT-Based MCTS Algorithm\n",
    "Furhter, we define a uct_mcts_coin() in the local module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e682f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uct_mcts_coin(env, num_rollouts=100, temperature=1.4):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0\n",
    "    paths=[]    \n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        result,path,counts,wins,losses=uct_simulate_coin(\\\n",
    "         env,paths,counts,wins,losses,temperature)      \n",
    "        # backpropagate\n",
    "        backpropagate(path,result,paths)\n",
    "    # See which action is most promising\n",
    "    best_next_move=best_move(counts,wins,losses) \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91376624",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 100. You can change it to a different number when calling the function. We create three dictionaries, counts, wins, and losses, to record the outcomes from simulated games. Once all roll outs are complete, we select the best next move based on the simulation results.\n",
    "\n",
    "The default value of temperature, which governs exploration versus exploitation, is set to 1.4. You can also change that based on the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "# 2. Test UCT MCTS in the Coin Game\n",
    "We'll test the UCT MCTS agent against random moves. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e0986",
   "metadata": {},
   "source": [
    "\n",
    "We will play 100 games and see how many times it wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8e35e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\.conda\\envs\\deepq\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from utils.coin_simple_env import coin_game\n",
    "from utils.ch18util import uct_mcts_coin\n",
    "import random\n",
    "\n",
    "env=coin_game()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=random.choice(env.validinputs)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action = uct_mcts_coin(env,num_rollouts=100) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            results.append(1)    \n",
    "            break  \n",
    "        action = random.choice(env.validinputs)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the MCTS agent loses\n",
    "            results.append(-1)   \n",
    "            break   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47310c",
   "metadata": {},
   "source": [
    "Half the time, the MCTS agent moves first so that no player has an advantage. We record a result of 1 if the MCTS agent wins and a result of -1 if the MCTS agent loses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab103f9",
   "metadata": {},
   "source": [
    "We now count how many times the MCTS agent with pruning has won and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the MCTS agent has won 87 games\n",
      "the MCTS agent has lost 13 games\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the MCTS agent has lost {losses} games\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f454f",
   "metadata": {},
   "source": [
    "The above results show that the MCTS agent beats the random moves 87 out of 100 games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a934805a",
   "metadata": {},
   "source": [
    "# 3. Policy-Based MCTS in the Coin Game\n",
    "Instead of choosing moves randomly each step, we'll use the trained policy network from Chapter 15 to guide the moves in each step. Intelligent moves lead to more accurate game outcomes, which in turn lead to more accurate position evaluations from game roll outs. \n",
    "\n",
    "In this section, we'll create a policy-based MCTS algorithm in the Coin game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a949cf6",
   "metadata": {},
   "source": [
    "## 3.1. Best Moves Based on the Policy Network\n",
    "We'll use the trained actor-critic model from Chapter 15 to select moves in the Coin game simulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207914a0",
   "metadata": {},
   "source": [
    "In the local module ch18util, we define a DL_stochastic() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26dacc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(state):\n",
    "    onehot=np.zeros((1,22))\n",
    "    onehot[0,state]=1\n",
    "    return onehot\n",
    "# Load the trained models from Chapter 15\n",
    "model=keras.models.load_model(\"files/ch15/ac_coin.h5\")\n",
    "# Define stochastic moves based on the trained models\n",
    "def DL_stochastic(env): \n",
    "    state = env.state\n",
    "    onehot_state = onehot_encoder(state)\n",
    "    action_probs, _ = model(onehot_state)\n",
    "    action_probs, critic_value = model(onehot_state)\n",
    "    return np.random.choice([1,2], p=np.squeeze(action_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e747d",
   "metadata": {},
   "source": [
    "In Chapter 15, we trained a deep reinforcement model with two networks: a value network (critic) and a plicy network (actor). The DL_stochastic() function selects the best move based on the policy network from the trained model. Note we are using the stochastic policy here, meaning we select the moves randomly based on the probability distribution from the policy network.\n",
    "\n",
    "A determininstic policy will select the move with the highest probability in the distribution instead. Stochastic policy usually leads to better simulation outcomes. We define a deterministic strategy in the local module ch18util as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b029e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DL_deterministic(env): \n",
    "    state = env.state\n",
    "    onehot_state = onehot_encoder(state)\n",
    "    action_probs, _ = model(onehot_state)\n",
    "    action_probs, critic_value = model(onehot_state)\n",
    "    return np.argmax(np.squeeze(action_probs))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c98ca8",
   "metadata": {},
   "source": [
    "Later we'll use both the stochastic and deterministic versions of the strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83eda2",
   "metadata": {},
   "source": [
    "## 3.2. Simulate A Game Based on the Policy Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ec7f1",
   "metadata": {},
   "source": [
    "Now that we know how to select best moves based on the trained actor-critic model, we'll define a simulate_policy_coin() function in the local module ch18util. The function simulates a game from a certain starting position all the way to the end of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50cb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy_coin(env,counts,wins,losses):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    # roll out the game till the terminal state\n",
    "    while True:   \n",
    "        move=DL_stochastic(env_copy)\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==1) or \\\n",
    "                (reward==-1 and env.turn==2):\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==1) or \\\n",
    "                (reward==1 and env.turn==2):\n",
    "                losses[actions[0]] += 1                \n",
    "            break\n",
    "    return counts,wins,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6851c3",
   "metadata": {},
   "source": [
    "This function simulates a coin game and updates the number of game counts, number of wins and losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb19026",
   "metadata": {},
   "source": [
    "## 3.3. A Policy-Based MCTS Algorithm\n",
    "Furhter, we define a policy_mcts_coin() in the local module as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77e9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mcts_coin(env, num_rollouts=100):\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0  \n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        counts,wins,losses=simulate_policy_coin(\\\n",
    "                           env,counts,wins,losses)\n",
    "    # See which action is most promising\n",
    "    best_next_move=best_move(counts,wins,losses)  \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9a2e2",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 100. You can change the a different number when calling the function. We create three dicitonaries counts, wins, and losses to record the outcomes from simulated games. Once all game roll outs are complete, we select the best next move based on the simulation results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4535cf7",
   "metadata": {},
   "source": [
    "# 4. The Effectiveness of the Policy MCTS Agent\n",
    "We'll compare the policy-based MCTS algorithm with the UCT MCTS algorithm in the coin game. We'll show that the former is much stronger than the latter, with a fixed number of rollouts in both algroithms. We'll also show that the policy-based MCTS algorithm is better than the actor-critic agent as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cf3939",
   "metadata": {},
   "source": [
    "## 4.1. Policy MCTS versus UCT MCTS\n",
    "We will let the two MCTS agents play 100 games against each other and see how many times each algorithm wins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3fa6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch18util import policy_mcts_coin\n",
    "\n",
    "env=coin_game()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=uct_mcts_coin(env,num_rollouts=100)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action = policy_mcts_coin(env,num_rollouts=100) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the policy MCTS agent wins\n",
    "            results.append(1)    \n",
    "            break  \n",
    "        action = uct_mcts_coin(env,num_rollouts=100)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the policy MCTS agent loses\n",
    "            results.append(-1)   \n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960409e0",
   "metadata": {},
   "source": [
    "Half the time, the UCT MCTS agent moves first and the other half the policy MCTS agent moves first so that no player has an advantage. We record a result of 1 if the policy MCTS agent wins and a result of -1 if the UCT MCTS agent wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8687c783",
   "metadata": {},
   "source": [
    "We now count how many times the policy MCTS agent has won and how mnay times the UCT MCTS agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6e7fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy MCTS agent has won 100 games\n",
      "the policy MCTS agent has lost 0 games\n"
     ]
    }
   ],
   "source": [
    "wins=results.count(1)\n",
    "print(f\"the policy MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the policy MCTS agent has lost {losses} games\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36d941",
   "metadata": {},
   "source": [
    "The above results show that the policy MCTS agent has won all 100 games. This indicates that the policy MCTS algorithm is much better than the UCT MCTS algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6b1ad",
   "metadata": {},
   "source": [
    "## 4.2. Policy MCTS versus the Actor-Critic Agent\n",
    "We'll compare the policy-based MCTS algorithm with the moves recommended by the actor-critic model. We use the stochastic strategy and let the policy MCTS agent play against the actor-critic agent for 100 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60eaadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch18util import  DL_stochastic\n",
    "\n",
    "env=coin_game()\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action= DL_stochastic(env)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action = policy_mcts_coin(env,num_rollouts=100) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the policy MCTS agent wins\n",
    "            results.append(1)    \n",
    "            break  \n",
    "        action =  DL_stochastic(env)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the policy MCTS agent loses\n",
    "            results.append(-1)   \n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5377a",
   "metadata": {},
   "source": [
    "Half the time, the policy MCTS agent moves first and the other half the actor-critic agent moves first so that no player has an advantage. We record a result of 1 if the policy MCTS agent wins and a result of -1 if the policy MCTS agent loses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a4a6a",
   "metadata": {},
   "source": [
    "We now count how many times the policy MCTS agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cb7a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy MCTS agent has won 51 games\n",
      "the policy MCTS agent has lost 49 games\n"
     ]
    }
   ],
   "source": [
    "# count how many times the MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the policy MCTS agent has won {wins} games\")\n",
    "# count how many times the MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the policy MCTS agent has lost {losses} games\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb8050",
   "metadata": {},
   "source": [
    "The above results show that the policy MCTS agent is slightly better than the actor-critic agent. \n",
    "\n",
    "Two points worth mentioning. First, we already know that the actor-critic agent in the Coin game plays perfect games. Why did it lose to the policy MCTS agent above? The reason is for a stochastic policy, there is a msall chance of error. Say there are 4 coins left on the table, and the stochatic policy may recommend to take 1 coin with 99.9% prob and to take 2 coins with 0.1% prob. Even though the policy is highly effecive: it leads to wins 99.9% of the time. But if you roll out many games using the same 99.9%/0.1% policy and take the average outcome, the mistake can be further reduced. This is the insight form alphago as well.\n",
    "\n",
    "The second point worht mentioning is that in most games, the actor-critc method doesn't provide a perfect game strategy. It only provides a relatively good strategy. In such cases, combining the actor-critic method with MCTS provides great value, as we'll see in the next two chatpers when we deal with Tic Tac Toe and Connect Four games. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
