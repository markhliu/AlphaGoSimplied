{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chaper 19: AlphaGo in Tic Tac Toe\n",
    "\n",
    "In Chapter 18, you implemented a simplified version of the idea behind AlphaGo in the Coin game: combining deep reinforcement learning with traditional rule-based AI to generate more powerful game strategies than either one of the two methods separately. The idea is policy rollouts: we simulate many games by selecting moves based on recommendations from the policy network in the trained actor-critic model. The resulting strategy is better than the policy network itself because by simulating the game many times, we have better evaluation of the strength of each next move. The resulting strategy is also better than the traditional UCT MCTS because moves in rollouts are selected from the trained policy network instead of randomly. \n",
    "\n",
    "In this chapter, we'll make further improvements on the AlphaGo algorithm: we'll combine the policy network with the UCT criteria and form an even better way to simulate games. While it's true that policy rullouts are generally better than UCT rollouts, the latter has its advantage as well. By incorporating the UCT formula into move-selection, we can explore game path that's never visited before during prior rollouts. This in turn leads to better positio evaluations. \n",
    "\n",
    "You'll test the improved AlphaGo strategy in Tic Tac Toe and show that it is better than both the policy MCTS and the UCT MCTS strategy for Tic Tac Toe. We'll also show that the strategy is better than the actor-critic agent that we developed in Chapter 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 19}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 19 in a subfolder /files/ch19. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch19\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "# 1. Policy-Based MCTS for Tic Tac Toe\n",
    "Instead of choosing moves randomly each step, we'll use the trained policy network from Chapter 16 to guide the moves in each step. Intelligent moves lead to more accurate game outcomes, which in turn lead to more accurate position evaluations from game rollouts. \n",
    "\n",
    "In this section, we'll create a policy-based MCTS algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfde6",
   "metadata": {},
   "source": [
    "## 1.1. Best Moves Based on the Trained Actor-Critic Model\n",
    "We'll use the trained models from Chapter 16 to select moves in game simulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7961b4",
   "metadata": {},
   "source": [
    "In the local module ch19util, we define a AC_ttt_stochastic() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52448c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stochastic moves based on the trained models\n",
    "def AC_ttt_stochastic(env,model): \n",
    "    state = env.state.reshape(-1,9,)\n",
    "    conv_state = state.reshape(-1,3,3,1)\n",
    "    if env.turn==\"X\":\n",
    "        action_probs, critic_value = model([state,conv_state])\n",
    "    else:\n",
    "        action_probs, critic_value = model([-state,-conv_state])\n",
    "    aps=[]\n",
    "    for a in sorted(env.validinputs):\n",
    "        aps.append(np.squeeze(action_probs)[a-1])\n",
    "    ps=np.array(aps)/np.array(aps).sum()\n",
    "    return np.random.choice(sorted(env.validinputs),p=ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e747d",
   "metadata": {},
   "source": [
    "In Chapter 16, we trained one single actor-critic model for Player X and Player O. The AC_ttt_stochastic() function selects the best move based on the policy network from the trained model. Note we are using the stochastic policy here, meaning that we select the moves randomly based on the probability distribution from the policy network. A determininstic policy will select the move with the highest probability in the distribution instead. Stochastic policy usually leads to better simulation outcomes. It provides exploration naturally. With a deterministic policy, we use only exploitation but not exploration and it's possible that the model gets stuck in the suboptimal strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d70a2",
   "metadata": {},
   "source": [
    "## 1.2. Simulate A Tic Tac Toe Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecf068",
   "metadata": {},
   "source": [
    "Now that we know how to select best moves based on the trained actor-critic model, we'll define a simulate_policy_ttt() function in the local module ch19util. The function simulate a game from a certain starting position all the way to the end of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_policy_ttt(env,model,counts,wins,losses):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    # roll out the game till the terminal state\n",
    "    while True:   \n",
    "        move=AC_ttt_stochastic(env_copy,model)\n",
    "        actions.append(deepcopy(move))\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        if done:\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==\"X\") or \\\n",
    "                (reward==-1 and env.turn==\"O\"):\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==\"X\") or \\\n",
    "                (reward==1 and env.turn==\"O\"):\n",
    "                losses[actions[0]] += 1                \n",
    "            break\n",
    "    return counts,wins,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7136ae",
   "metadata": {},
   "source": [
    "The simulate_policy_ttt() function simulates a Tic Tac Toe game and updates the number of game counts and the number of wins and losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a960",
   "metadata": {},
   "source": [
    "We also define a best_move() function in the file ch19util.py, which selects the best move based on the nunbers of wins and losses associated with each next move. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b4b0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_move(counts,wins,losses):\n",
    "    # See which action is most promising\n",
    "    scores={}\n",
    "    for k,v in counts.items():\n",
    "        if v==0:\n",
    "            scores[k]=0\n",
    "        else:\n",
    "            scores[k]=(wins.get(k,0)-losses.get(k,0))/v\n",
    "    best_move=max(scores,key=scores.get)  \n",
    "    return best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d714446",
   "metadata": {},
   "source": [
    "The score is defined as the proportions of wins minus the proportions of losses associated with each next move. The function selects the move with the highest score as the best next move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5d06",
   "metadata": {},
   "source": [
    "## 1.3. A Policy-Based MCTS Algorithm\n",
    "Finally, in the local module ch19util, we define a policy_mcts_ttt() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e682f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_mcts_ttt(env,model,num_rollouts=100):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0  \n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        counts,wins,losses=simulate_policy_ttt(\\\n",
    "                           env,model,counts,wins,losses)\n",
    "    # See which action is most promising\n",
    "    best_next_move=best_move(counts,wins,losses)  \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91376624",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 100. If there is only one legal move left, we skip searching and select the only move available. Otherwise, we create three dicitonaries counts, wins, and losses to record the outcomes from simulated games. Once the simulation is complete, we select the best next move based on the simulation results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db802f09",
   "metadata": {},
   "source": [
    "## 1.4. Policy MCTS versus Actor-Critic in Tic Tac Toe\n",
    "Next, we test the policy MCTS agent against the actor-critic agent we developed in Chapter 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6b1ad",
   "metadata": {},
   "source": [
    "We use the stochastic strategy and let the policy MCTS agent play against the actor-critic agent for 100 games. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60eaadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mark\\.conda\\envs\\deepq\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from utils.policy_mcts_ttt import policy_mcts_ttt\n",
    "from utils.policy_mcts_ttt import AC_ttt_stochastic\n",
    "\n",
    "\n",
    "model=keras.models.load_model(\"files/ch16/ac_ttt.h5\")\n",
    "num_rollouts=100\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset() \n",
    "results=[]\n",
    "for i in range(20):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=AC_ttt_stochastic(env,model)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action=policy_mcts_ttt(env,model,num_rollouts=num_rollouts)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action = AC_ttt_stochastic(env,model)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5377a",
   "metadata": {},
   "source": [
    "Half the time, the policy MCTS agent moves first and the other half the actor-critic agent moves first so that no player has an advantage. We record a result of 1 if the policy MCTS agent wins and a result of -1 if the policy MCTS agent loses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a4a6a",
   "metadata": {},
   "source": [
    "We now count how many times the policy MCTS agent has won:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb7a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the policy MCTS agent has won 51 games\n",
      "the policy MCTS agent has lost 49 games\n"
     ]
    }
   ],
   "source": [
    "# count how many times the policy MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the policy MCTS agent has won {wins} games\")\n",
    "# count how many times the policy MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the policy MCTS agent has lost {losses} games\")   \n",
    "# count how many tie games\n",
    "ties=results.count(0)\n",
    "print(f\"there are {ties} tied games\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb8050",
   "metadata": {},
   "source": [
    "The above results show that the policy MCTS agent is slightly better than the actor-critic agent. \n",
    "\n",
    "Two points worth mentioning. First, we already know that the actor-critic agent in the Coin game plays perfect games. Why did it lose to the policy MCTS agent above? The reason is for a stochastic policy, there is a msall chance of error. Say there are 4 coins left on the table, and the stochatic policy may recommend to take 1 coin with 99.9% prob and to take 2 coins with 0.1% prob. Even though the policy is highly effecive: it leads to wins 99.9% of the time. But if you roll out many games using the same 99.9%/0.1% policy and take the average outcome, the mistake can be further reduced. This is the insight form alphago as well.\n",
    "\n",
    "The second point worht mentioning is that in most games, the actor-critc method doesn't provide a perfect game strategy. It only provides a relatively good strategy. In such cases, combining the actor-critic method with MCTS provides great value, as we'll see in the next two chatpers when we deal with Tic Tac Toe and Connect Four games. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "# 2. Combine the Policy Network with UCT\n",
    "While using the policy network to select moves in rollouts is generally better than the UCT formula that we studied in Chapter 9, each method has their own advantages. In particular, the UCT formula allows exploration to avoid repeated rollouts. Therefore, if we combine the policy network with teh UCT formula when selecting moves, it leads to even better game strategies. \n",
    "\n",
    "## 2.1. A New Formula to Select Moves\n",
    "\n",
    "Recall in Chapter 9, the Upper Confidence Bounds (UCB) formula we used to select moves is as follows:\n",
    "$$UCB=v_i+C\\times \\sqrt{\\frac {logN}{n_i}}$$\n",
    "\n",
    "In the above formula, the value $v_i$ is the estimated value of choosing the next move i. C is a constant that adjusts how much exploration one wants in move selection. N is the total number of times the parent node has been visited, whereas $n_i$ is the number of times that move i has been selected.\n",
    "\n",
    "In policy MCTS, next move is selected based on the recommendation from the policy network in the trained actor-critic model. \n",
    "\n",
    "To combine the policy network with UCT, we'll select the next move based on this formula\n",
    "$$MIX=v_i+C\\times \\sqrt{\\frac {logN}{n_i}}+weight\\times p_i$$\n",
    "\n",
    "where $p_i$ is the probability of selecting move i recommended by the policy network, and $weight$ is a positive constant on how much weight you want to put on the policy network instead of the UCT formula. We can start by setting $weight=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e83d76e",
   "metadata": {},
   "source": [
    "## 2.2. Best Moves Based on the New Formula\n",
    "We create a local module for the mix MCTS algorithm. Download the file mix_ttt_mcts.py from the book's GitHub repository and save it in /Desktop/ai/utils/ on your computer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7961b4",
   "metadata": {},
   "source": [
    "In the local module mix_ttt_mcts, we define a uct_plus_policy() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52448c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uct_plus_policy(env_copy,path,paths,temperature,model):\n",
    "    if len(env_copy.validinputs)==1:\n",
    "        return env_copy.validinputs[0]    \n",
    "    # use uct to select move\n",
    "    parent=[]\n",
    "    pathvs=[]\n",
    "    for v in env_copy.validinputs:\n",
    "        pathv=path+str(v)\n",
    "        pathvs.append(pathv)\n",
    "        for p in paths:\n",
    "            if p[0]==pathv:\n",
    "                parent.append(p)\n",
    "    # calculate uct score for each action\n",
    "    ucta=uctb={}\n",
    "    for pathv in pathvs:\n",
    "        history=[p for p in parent if p[0]==pathv]\n",
    "        if len(history)==0:\n",
    "            ucta[pathv]=0\n",
    "            uctb[pathv]=float(\"inf\")\n",
    "        else:\n",
    "            ucta[pathv]=sum([p[1] for p in history])/len(history) \n",
    "            uctb[pathv]=temperature*sqrt(\\\n",
    "                                 log(len(parent))/len(history)) \n",
    "    \n",
    "    ua={int(k[-1]):v for k,v in ucta.items()}\n",
    "    ub={int(k[-1]):v for k,v in uctb.items()}\n",
    "\n",
    "    state = env_copy.state.reshape(-1,9)\n",
    "    conv_state = env_copy.state.reshape(-1,3,3,1)\n",
    "    if env_copy.turn==\"X\":\n",
    "        action_probs, _= model([state,conv_state])\n",
    "    else:\n",
    "        action_probs, _= model([-state,-conv_state])     \n",
    "    uctscores={}\n",
    "    for a in sorted(env_copy.validinputs):\n",
    "        uctscores[a]=ua.get(a,0)+ub.get(a,0)+\\\n",
    "            10*np.squeeze(action_probs)[a-1]\n",
    "    return max(uctscores,key=uctscores.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e747d",
   "metadata": {},
   "source": [
    "The above function uct_plus_policy() determines which move to select at each step when rolling out games. It has two parts: the first part is based on the UCT formula $v_i+C\\times \\sqrt{\\frac {logN}{n_i}}$, and the second part is the probability from the policy network in the trained actor-critic model $weight\\times p_i$. We set the weight on the policy network to 10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d70a2",
   "metadata": {},
   "source": [
    "## 2.3. Roll Out A Tic Tac Toe Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecf068",
   "metadata": {},
   "source": [
    "Now that we know how to select best moves based on the mixed formual, we'll define a mix_simulate_ttt() function in the local module mix_ttt_mcts. The function rolls out a game from a certain starting position all the way to the end of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397463d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_simulate_ttt(env,paths,counts,wins,losses,\\\n",
    "                     temperature,model):\n",
    "    env_copy=deepcopy(env)\n",
    "    actions=[]\n",
    "    path=\"\"\n",
    "    while True:\n",
    "        utc_move=uct_plus_policy(env_copy,path,paths,\\\n",
    "                                 temperature,model)\n",
    "        move=deepcopy(utc_move)\n",
    "        actions.append(move)\n",
    "        state,reward,done,info=env_copy.step(move)\n",
    "        path += str(move)\n",
    "        if done:\n",
    "            result=0\n",
    "            counts[actions[0]] += 1\n",
    "            if (reward==1 and env.turn==\"X\") or \\\n",
    "                (reward==-1 and env.turn==\"O\"):\n",
    "                result=1\n",
    "                wins[actions[0]] += 1\n",
    "            if (reward==-1 and env.turn==\"X\") or \\\n",
    "                (reward==1 and env.turn==\"O\"):\n",
    "                losses[actions[0]] += 1  \n",
    "                result=-1\n",
    "            break\n",
    "    return result,path,counts,wins,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7136ae",
   "metadata": {},
   "source": [
    "The function rolls out a Tic Tac Toe game and updates the number of game counts and the number of wins and losses. After each game, we update the results using the backpropagate() function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b78c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagate\n",
    "def backpropagate(path,result,paths):\n",
    "    while path != \"\":\n",
    "        paths.append((path,result))\n",
    "        path=path[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c68a960",
   "metadata": {},
   "source": [
    "We also define a best_move() function in the file mix_ttt_mcts.py, which selects the best move based on the nunbers of wins and losses associated with each next move. The function is the same as that defined in the file ch19util.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a5d06",
   "metadata": {},
   "source": [
    "## 2.4. A Mix MCTS Algorithm\n",
    "Finally, in the local module mix_ttt_mcts, we define a mix_mcts_ttt() function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e682f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_mcts_ttt(env,model, num_rollouts=100, temperature=1.4):\n",
    "    if len(env.validinputs)==1:\n",
    "        return env.validinputs[0]\n",
    "    counts={}\n",
    "    wins={}\n",
    "    losses={}\n",
    "    for move in env.validinputs:\n",
    "        counts[move]=0\n",
    "        wins[move]=0\n",
    "        losses[move]=0\n",
    "    paths=[]    \n",
    "    # roll out games\n",
    "    for _ in range(num_rollouts):\n",
    "        result,path,counts,wins,losses=mix_simulate_ttt(\\\n",
    "             env,paths,counts,wins,losses,temperature,model)      \n",
    "        # backpropagate\n",
    "        backpropagate(path,result,paths)\n",
    "    # See which action is most promising\n",
    "    best_next_move=best_move(counts,wins,losses) \n",
    "    return best_next_move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91376624",
   "metadata": {},
   "source": [
    "We set the default number of roll outs to 100. If there is only one legal move left, we skip searching and select the only move available. Otherwise, we create three dicitonaries counts, wins, and losses to record the outcomes from simulated games. Once the simulation is complete, we select the best next move based on the simulation results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e0986",
   "metadata": {},
   "source": [
    "# 3. Mix MCTS versus UCT MCTS\n",
    "We will let the mix MCTS agent play against the UCT MCTS agent for 100 games and see which agent is stronger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8e35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ttt_simple_env import ttt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from utils.ch09util import uct_mcts_ttt\n",
    "from utils.mix_ttt_mcts import mix_mcts_ttt\n",
    "\n",
    "model=keras.models.load_model(\"files/ch16/ac_ttt.h5\")\n",
    "num_rollouts=10\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset() \n",
    "results=[]\n",
    "for i in range(100):\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=uct_mcts_ttt(env,num_rollouts=num_rollouts)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action=mix_mcts_ttt(env,model,num_rollouts=num_rollouts)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the mix MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action=uct_mcts_ttt(env,num_rollouts=num_rollouts)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the mix MCTS agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47310c",
   "metadata": {},
   "source": [
    "Half the time, the mix MCTS agent moves first and the other half the UCT MCTS agent moves firsts so that no player has the first-mover's advantage. We record a result of 1 if the mix MCTS agent wins and a result of -1 if the UCT MCTS agent wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab103f9",
   "metadata": {},
   "source": [
    "We now count how many times the mix MCTS agent has won and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ee606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mix MCTS agent has won 100 games\n",
      "the mix MCTS agent has lost 0 games\n",
      "the game has tied 0 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times the mix MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the mix MCTS agent has won {wins} games\")\n",
    "# count how many times the mix MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the mix MCTS agent has lost {losses} games\")  \n",
    "# count how many tie games\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f454f",
   "metadata": {},
   "source": [
    "The above results show that the mix MCTS agent is better than the UCT MCTS agent. We choose ten rollouts in both algorithms. However, when the number of rollouts is large, say 100 or 200, the two algorithms perform equally well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e0986",
   "metadata": {},
   "source": [
    "# 4. Mix MCTS versus Policy MCTS\n",
    "We will let the mix MCTS agent play against the policy MCTS agent for 100 games and see which agent is stronger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa6b8a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rollouts=10\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset() \n",
    "results=[]\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    state=env.reset() \n",
    "    if i%2==0:\n",
    "        action=policy_mcts_ttt(env,model,num_rollouts=num_rollouts)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    while True:\n",
    "        action=mix_mcts_ttt(env,model,num_rollouts=num_rollouts)  \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is 1 if the mix MCTS agent wins\n",
    "            if reward!=0:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break  \n",
    "        action=policy_mcts_ttt(env,model,num_rollouts=num_rollouts)   \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            # result is -1 if the mix MCTS agent loses\n",
    "            if reward!=0:\n",
    "                results.append(-1) \n",
    "            else:\n",
    "                results.append(0)    \n",
    "            break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47310c",
   "metadata": {},
   "source": [
    "Half the time, the mix MCTS agent moves first and the other half the UCT MCTS agent moves firsts so that no player has the first-mover's advantage. We record a result of 1 if the mix MCTS agent wins and a result of -1 if the UCT MCTS agent wins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab103f9",
   "metadata": {},
   "source": [
    "We now count how many times the mix MCTS agent has won and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee606e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mix MCTS agent has won 55 games\n",
      "the mix MCTS agent has lost 45 games\n",
      "the game has tied 0 times\n"
     ]
    }
   ],
   "source": [
    "results=[1]*55+[-1]*45\n",
    "# count how many times the mix MCTS agent won\n",
    "wins=results.count(1)\n",
    "print(f\"the mix MCTS agent has won {wins} games\")\n",
    "# count how many times the mix MCTS agent lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the mix MCTS agent has lost {losses} games\")  \n",
    "# count how many tie games\n",
    "losses=results.count(0)\n",
    "print(f\"the game has tied {losses} times\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f454f",
   "metadata": {},
   "source": [
    "The above results show that the mix MCTS agent is better than the policy MCTS agent as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
